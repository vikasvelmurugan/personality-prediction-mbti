{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2ff1e4",
   "metadata": {},
   "source": [
    "Complete Explanation on what Myers-Briggs Type Indicator:\n",
    "https://www.youtube.com/watch?v=QPtrDt_VybY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201396c",
   "metadata": {},
   "source": [
    "Dataset links :\n",
    "* https://www.kaggle.com/datasets/datasnaek/mbti-type\n",
    "* https://www.kaggle.com/datasets/kaggle/meta-kaggle?select=ForumMessages.csv \n",
    "Just download the forum messages.csv file from the second link (approx 720 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b06b5",
   "metadata": {},
   "source": [
    "'I': \"Introversion\",\n",
    "'E': \"Extriversion\",\n",
    "'N':'Intuition',\n",
    "\"S\":\"Sensing\",\n",
    "\"T\":\"Thinking\",\n",
    "\"F\":\"Feeling\",\n",
    "\"J\": \"Judging\",\n",
    "\"P\": \"Perceiving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316c50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511125de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c7ce83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961e1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data=pd.read_csv('forum_topic_messages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb227e0",
   "metadata": {},
   "source": [
    "* <b> Data Preparation and Cleaning </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de77c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAITS OF A PERSON ACCORDING TO OUR DATASET\n",
    "# FOR EXAMPLE:\n",
    "# INFJ - Person has 4 traits-> Introversion, Intuition, Feeling, Judging\n",
    "\n",
    "mbti={\n",
    "    'I': \"Introversion\",\n",
    "    'E': \"Extriversion\",\n",
    "    'N':'Intuition',\n",
    "    \"S\":\"Sensing\",\n",
    "    \"T\":\"Thinking\",\n",
    "    \"F\":\"Feeling\",\n",
    "    \"J\": \"Judging\",\n",
    "    \"P\": \"Perceiving\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeee92b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ForumTopicId</th>\n",
       "      <th>PostUserId</th>\n",
       "      <th>PostDate</th>\n",
       "      <th>ReplyToForumMessageId</th>\n",
       "      <th>Message</th>\n",
       "      <th>Medal</th>\n",
       "      <th>MedalAwardDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>478</td>\n",
       "      <td>04/28/2010 23:13:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;div&gt;In response to a comment on the No Free H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>606</td>\n",
       "      <td>04/29/2010 15:48:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi, I'm interested in participating in the con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>478</td>\n",
       "      <td>04/29/2010 15:48:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tanya,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Good to hear from yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>368</td>\n",
       "      <td>04/29/2010 15:48:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi Tanya, &lt;br&gt;&lt;br&gt;Kaggle will maintain a ratin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>478</td>\n",
       "      <td>05/02/2010 14:37:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Now that we have a handful of algorithms that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  ForumTopicId  PostUserId             PostDate  ReplyToForumMessageId  \\\n",
       "0   1             1         478  04/28/2010 23:13:08                    NaN   \n",
       "1   2             2         606  04/29/2010 15:48:46                    NaN   \n",
       "2   3             2         478  04/29/2010 15:48:46                    NaN   \n",
       "3   4             2         368  04/29/2010 15:48:46                    NaN   \n",
       "4  14             7         478  05/02/2010 14:37:35                    NaN   \n",
       "\n",
       "                                             Message  Medal MedalAwardDate  \n",
       "0  <div>In response to a comment on the No Free H...    NaN            NaN  \n",
       "1  Hi, I'm interested in participating in the con...    NaN            NaN  \n",
       "2  Tanya,<div><br></div><div>Good to hear from yo...    NaN            NaN  \n",
       "3  Hi Tanya, <br><br>Kaggle will maintain a ratin...    NaN            NaN  \n",
       "4  Now that we have a handful of algorithms that ...    NaN            NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9190cdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f55026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2077184, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ffa7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFP    1832\n",
      "INFJ    1470\n",
      "INTP    1304\n",
      "INTJ    1091\n",
      "ENTP     685\n",
      "ENFP     675\n",
      "ISTP     337\n",
      "ISFP     271\n",
      "ENTJ     231\n",
      "ISTJ     205\n",
      "ENFJ     190\n",
      "ISFJ     166\n",
      "ESTP      89\n",
      "ESFP      48\n",
      "ESFJ      42\n",
      "ESTJ      39\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Looking at the personality distribution in the training data\n",
    "type_count=train_data['type'].value_counts()\n",
    "print(type_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201394d",
   "metadata": {},
   "source": [
    "* <B> Looking for missing values in the data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67153dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the train data:\n",
      "type     0\n",
      "posts    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in the train data:\")\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78367c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the forum data:\n",
      "Id                             0\n",
      "ForumTopicId                   0\n",
      "PostUserId                     0\n",
      "PostDate                       0\n",
      "ReplyToForumMessageId    1057676\n",
      "Message                     7150\n",
      "Medal                    1178831\n",
      "MedalAwardDate           1169140\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in the forum data:\")\n",
    "print(forum_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1239835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5675139997227016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1178831/2077184"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e3cc6",
   "metadata": {},
   "source": [
    "* We can see that more than 50% values are missing in medal column. But, if we think logically there is a chance of post not winning a medal. So it makes sense for understanding that not every post can win a medal so we can replace those values with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e91b02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    831979\n",
       "2.0     38883\n",
       "1.0     27491\n",
       "Name: Medal, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data['Medal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f640ed",
   "metadata": {},
   "source": [
    "* Just think about this. Ideally, it should be harder to win 3 medals than 1 but our dataset has more posts having three medals. \n",
    "* This is the dataset we have and we need to proceed with this but if this was a real world problem in your organisation then you can go to the client and check if there are any issues with the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5fbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data['Medal']=forum_data['Medal'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da87d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in forum data: \n",
      "Id                             0\n",
      "ForumTopicId                   0\n",
      "PostUserId                     0\n",
      "PostDate                       0\n",
      "ReplyToForumMessageId    1057676\n",
      "Message                        0\n",
      "Medal                    1178831\n",
      "MedalAwardDate           1169140\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replacing null in Message with blank space as the amount of data missing is less\n",
    "\n",
    "forum_data['Message']=forum_data['Message'].fillna('')\n",
    "print(\"Missing values in forum data: \")\n",
    "print(forum_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76622ab9",
   "metadata": {},
   "source": [
    "* ReplyToForumMessageId is an ID column and most of the values are missing so better to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62f44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data.drop(['ReplyToForumMessageId'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15bf12",
   "metadata": {},
   "source": [
    "* Now, we will group the user by PostUserID since a particular user could have posted more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b7aa7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_g=forum_data.groupby('PostUserId')['Message'].agg(lambda col: ' '.join(col)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cae7383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62          1\n",
      "6608670     1\n",
      "6608665     1\n",
      "6608589     1\n",
      "6608524     1\n",
      "           ..\n",
      "2481048     1\n",
      "2481047     1\n",
      "2480956     1\n",
      "2480902     1\n",
      "17044377    1\n",
      "Name: PostUserId, Length: 336000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(forum_data_g['PostUserId'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23a37c",
   "metadata": {},
   "source": [
    "* <b> Data Cleaning </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395ebd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dd03517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text=BeautifulSoup(text, 'lxml').text\n",
    "    #removing html and seperators\n",
    "    text=re.sub(r'\\|\\|\\|',r' ',text)\n",
    "    text=re.sub(r'http\\S+', r' ', text)\n",
    "    #removing punctuations\n",
    "    text=text.replace('.', ' ')\n",
    "    translator=str.maketrans('','',string.punctuation)\n",
    "    text=text.translate(translator)\n",
    "    #removing numbers\n",
    "    text=''.join(i for i in text if not i.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d79d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikas Velmurugan\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data['clean_posts']=train_data['posts'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c01e4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im finding the lack of me in these posts very alarming  Sex can be boring if its in the same position often  For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary  There isnt enough    Giving new meaning to Game theory  Hello ENTP Grin  Thats all it takes  Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins  This  Lack of Balance and Hand Eye Coordination  Real IQ test I score   Internet IQ tests are funny  I score s or higher   Now like the former responses of this thread I will mention that I dont believe in the IQ test  Before you banish    You know youre an ENTP when you vanish from a site for a year and a half return and find people are still commenting on your posts and liking your ideasthoughts  You know youre an ENTP when you        I over think things sometimes  I go by the old Sherlock Holmes quote   Perhaps when a man has special knowledge and special powers like my  own it rather encourages him to seek a complex    cheshirewolf tumblr com  So is I D   post Not really Ive never thought of EI or JP as real functions   I judge myself on what I use  I use Ne and Ti as my dominates  Fe for emotions and rarely Si  I also use Ni due to me strength    You know though  That was ingenious  After saying it I really want to try it and see what happens with me playing a first person shooter in the back while we drive around  I want to see the look on    out of all of them the rock paper one is the best  It makes me lol   You guys are lucky D Im really high up on the tumblr system  So did you hear about that new first person shooter game Ive been rocking the hell out of the soundtrack on my auto sound equipment that will shake the heavens  We managed to put a couple PSs in    No The way he connected things was very Ne  Ne dominates are just as aware of their environments as Se dominates   Example Shawn Spencer or Patrick Jane Both ENTPs  Well charlie I will be the first to admit I do get jealous like you do  I chalk it up to my w heart mixed with my dominate w  s and s both like to be noticed  s like to be known not the same    D Ill upload the same clip with the mic away from my mouth  Than you wont hear anything   Ninja Assassin style but with splatter  Tik Tok is a really great song  As long as you can mental block out the singer  I love the beat it makes me bounce  drop io vswck  D Mic really close to my mouth and smokin aces assassins ball playing in the background  Sociable  extrovert Im an extrovert and Im not sociable   Sherlock in the movie was an ENTP  Normally hes played as a EXTJ  In the books hes an ESTJ   As I said  The movie looked good except for it being called sherlock holmes    Oh I never had fear of kissing a guy  I will kiss an animal too  So there was nothing to vanish  Just personal taste and me not liking it   The guy I kissed didnt know me  It was one of those    Sounds pretty much like my area and what Im going through right now trying to figure out which way I want to take my life  I want to do so many things  The biggest problem is that I know if I dont    D I was operating under the impression that you were female  I never looked at your boxy  Okay I help out my gay friends all the time and one of them has developed a little crush on me  I get red    TT You just described me  and Im living the worst nightmare  Im trapped in one place with one one around  Only dull woods  If I was a serial killer this would be the perfect place but sadly Im    TBH and biased sounds like a shadowed INFP  I think maybe he was hurt and turned ESTJ  I can tell because he has some of the typical INFP traits left over  Checks list Im sorry  It seems that you have came at a bad time  Weve already reached our quota of INFJs  However being youre female and I like females I will make you a deal  I will kick one    Im ANTP Leaning toward E  Im easy for both ENTPs and INTPs to identify with   I also imagine ENTPs interrogations would go a little bit like Jacks from  except more mechanical  Rigging up shock treatment equipment in an abandoned building out of an old car batty jumper    It was a compliment  Trust me  Im just as psychopathic D except I have emoticons  Theyre just weird ones  Like laughing when I get hurt or at people running themselves over with their lawn mower        No  Its like a theme for where I live and that is why I know it by heart      and I usual dont leave until the thing ends  But in the mean time  In between times  You work your thing  Ill work mine D  D Im the MBP Pleasure to meet you  Damn need to trust my instincts more I would have been closer I was going to say INFP  EXFP Leaning toward S with the way she responded   D My friends even my gay and lesbian ones always come to me for advice  I bow to my entp masters ENTPs are so great  If it wasnt for ENTPs I wouldnt have been able to build what Im building  Duck Duck  Duck  Shotgun What Me I never do that     Because its hard to be sad about losing someone you like when you knew you were right and give yourself a big pat on the back because youre awesome and always correct  Oh you dont have to tell me that most of them are stupid  I know this  That is why I play with them and it makes me laugh  D As Im going to take Neuropsychology and I have a few psychologist    D Im a Nightowl  I wake up between pm and stay awake till am  Personal opinion backed by theory would suggest that INTPs are the most socially difficult  While INTJs can be socially indifferent but they will also use social situations if the the need arises     Personal stocks that I have on my desktop that Ive downloaded from random stock sites and stock photobuckets  Ill tell you when I open photoshop    Glad you like it static  D Thanks       Made for a friend  Several hours of work  I constructed every line by     Static    Ill have to get to your avatar later if one of my fellow teammates doesnt  Psychologist dont keep me around long enough to diagnosis me  I like to toy with them  What I have diagnosis myself with and had a few psychologist friends  a few other friends tell me I have is   '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['clean_posts'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae5606b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikas Velmurugan\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "forum_data_g['clean_messages']=forum_data_g['Message'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5148b896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Tanya Kaggle will maintain a rating system  If you win but youre ineligible for prize money you will still get a strong rating Anthony Here are some papers that analyze Eurovision voting patterns  You might find some of them helpful  Gatherer Comparison of Eurovision Song Contest Simulation with Actual Results Reveals Shifting Patterns of Collusive Voting Alliances    Eurovision Song Contest Is Voting Political or Cultural Ginburgh and Noury   Suleman Efstathiou and Johnson   Eurovision Song Contest as a ‘Friendship’ Network\\r\\nDekker  \\n More research    enjoyLove thy Neighbor Love thy Kin Voting Biases in the Eurovision Song Contest  culture and religion Explaining the bias in Eurovision song contest voting  Hybrid System Approach to Determine the Ranking of a Debutant Country in Eurovision  Eurovision  Judgment Versus Public Opinion – Evidence from the Eurovision Song Contest  GiovanniThanks for your feedback  Using the forum to give feedback is a good idea  It allows others to see and comment on suggestions  We might set up a proper feedback forum but for the moment this topic will have to suffice  I also agree that the forum is a bit clunky  However we have a large list of feature requests and only limited resources for the moment  it might take us some time to address this  Apologies  I dont think the prize money in this competition is that relevant the prize is relatively small  Correct me if Im wrong but I think contestants are driven by intrinsic factors A karma system that rewards forum posts is a good idea  Again apologies for any delay in implementing this there are lots of features on our to do list Anthony Manish thanks for the feedback  The site is hosted on an Amazon EC server on the east coast of America Its a fast server but the site has been more popular than we expected Were currently working on speeding up the site by reducing the number database queries  We may have to implement auto scaling if the site keeps growing so rapidly  Anthony Just made a change which should speed things up  Let me know if it has made a difference for you  Jonathan thanks for your feedback x  Were currently working on caching database queries  There are a lot of good suggestions here that well try before autoscaling \\xa0 Please use this topic to give us feedback  If youd rather do so in private email me at anthony goldbloomkaggle com  Use this topic to discuss any competitions you would like us to run  If you would rather contact me privately email anthony goldbloomkaggle com  I accidentally deleted the following post made by another user  Im reposting it on their behalfAre there categorical andor binary variables in the data set Other than the target variable For instance VariableOpen in test data seems to have categories       If there are categorical variables do we get to know what the categories meanThank You Thanks for the feedback  \\r\\n\\r\\n What sort of features do you have in mind Or can you point to a forum that you we should emulate I just added a quick reply box to make the forum less clunky \\r\\n\\r\\n Great suggestion  I have put this on our extensive features to add list  Colin the choice of scoring system was quite deliberate  Will the competition host considered using Area Under the ROC Curve where participants submit probabilities but  said that he deals with physicians who just want to know the proportion of predictions that are correct Rajstennaj and Colin were really pleased that you believe that theres value to the project  Let me know if there is anyway that we can help to facilitate a community  We did set up the general Kaggle forum under CommunityForum\\r\\n  with such a community in mind  does it provide sufficient infrastructure You are free to start any new threads on that forum RegardsAnthony   out of   thats pretty impressive Pity they didnt enter the Kaggle comp  I think youre right  some competitions exhibit more regularities than others  Soccer may be a difficult sport to model  Hi PG  Not sure that I fully understand the question  Are you referring to the situation where a classifier returns only  or  rather than a score or probability Perhaps you can use an example to illustrate the question Regards Anthony Hi PG  You should give the score for all timestamps  a higher score means the instance is more likely to be a member of the positive   AUC measures your classifiers ability to split the classes  so you dont need to decide which scores predict positive instances  and which predict negative instances   Have I addressed your concern The public leaderboard is only indicative because competitors can use information on their score to get information on a portion of the test dataset  The final results are a quite different and b better reflect actual performance  Thanks for participating in this competition  Ive attached the solution file to this post \\nUPDATE\\xa0The solution is no longer attached but youre welcome to make submissions to this competition  Hi MattThanks for the nice words and the suggestion  Ive posted the solution file  Hi DirkThe Elo Benchmark is based on the training dataset only \\xa0Having had many email conversations with Jeff I can tell you that the seed ratings matter a lot  Youll notice that Jeff made two submissions for the Elo benchmark  thats because hes refining his seeding method  I believe he plans to make a few more refinements Jeff uses an iterative process to seed the rating system  For example he might start by giving everybody  and then letting Elo run for  months  He then seeds Elo with the  month ratings and runs Elo again  He does several iterations of this Does this helpAnthony\\xa0 Hi JohnHave just confirmed with Jeff methodologies will be shared publicly  RegardsAnthony The competition has been designed to make cheating really difficult  At the end of the competition the winners methodologies will be replicated to help ensure everything is above board  Hi MattThe reason we prevent participants from submitting an unlimited number of times is because otherwisea our servers may not be able to handle all the traffic anda it would be easier to decode the portion of the test dataset thats used to calculate the public leaderboard  The technique you describe often referred to as cross validation is very sensible and we encourage others to use it  Anthony Good suggestion  Were open to ideas on how we can facilitate this  My thinking is the best thing to do is to implement a more functional forum which were doing  We can then encourage those who are still working on the problem to continue to use the competition forum as a way to collaborate  Hi DirkWeve updated the data description  thanks for the pointer \\xa0\\r\\nThe competition does require participants to forecast the next four observations \\xa0Weve updated the format of tourismdata csv so that there is always a value in the last row \\xa0Regards\\xa0Anthony Hi GregApologies there was a bug that cut off the last  characters  The problem has been fixed but unfortunately the fix will only apply future submissions  Thanks for pointing this out and sorry for the inconvinience  Anthony Hi MattI believe that Will the competition host is preparing a blog post that discusses some of the methods that people applied to this competition  based on the feedback we received  Is this the sort of thing you had in mindAnthony David this is a great suggestion  The HIV competition shows that Kagglers can do great things \\xa0 My initial concern with any public dataset is that people can look up the answers  We would need researchers to withhold a small portion of the dataset for evaluation  I think the first step is to get in touch with those who set up the Alzheimers project  It also makes sense to contact the Michael J Fox foundation  If anybody has any connection to either of these projects please let me know Otherwise Ill keep you posted on any progress Anthony Jase the score on the full dataset is calculated onthefly  so we actually know who is winning based on the full test dataset Ron the submission that is performing best on the public leaderboard may be different from the submission that is performing best on the full test dataset  We dont link the best submission on the public leaderboard to the best overall submission so that participants dont become confusedconcerned if their scoreposition on the public leaderboard worsens Leigh my thinking as well  In a tradeoff between having a veracious public leaderboard and a veracious end result  the end result is most important Jeff good suggestion   Ive put together an Excel sheet that might be helpful for cross validation  You paste your predictions for months  into column G and it aggregates by player by month and then calculates the RMSE  Hope its helpful Anthony BenThe evaluation method was chosen because Jeff has found that scoring based individual games with RMSE unduly favours systems that predict a draw  Mark Glickman raised another issue  RMSE is better suited to normally distributed rather than binary  outcomes  So in order to use RMSE aggregation is preferable  Of course we could have evaluated on a game by game basis using a different metric My biggest problem with the current evaluation method is that counting a draw as half a win seems a little arbitrary  However in order to benchmark Elo such an assumption is necessary  Mark and Jeff argue that a draw is generally worth half a win  so this assumption isnt too problematic  Anyway hope this gives you some insight into our thinking  RegardsAnthony Jeff please correct me if Im mistaken but I believe systems that predict draws are favoured because a high proportion of games are draws at the top level  per cent in the training dataset  Of course you can do better  but a system that predicts   for every game will perform better than it should  Has anybody tried Trueskill yet  probably a better starting point than Elo  This blog post does a nice job of stepping through Trueskill  Matt am interested in your thinking on this  Why MAE over MSE or RMSE Is it just that the metric is more intuitive or something subtler Hi DavidI have written to the Alzheimers Disease Neuroimaging Initiative ADNI and the Michael J Fox Foundation  I am scheduling meetings with both for September  Will keep you posted on any progress  Can you put up a link to the datapaper you foundThanks again for the suggestion  Its great if we can use the power of this platform to tackle meaningful problems  RegardsAnthony Uri the correlation between the public leaderboard score and overall score is significantly higher now  Here is the solution file for anybody interested   Uri Im reluctant to release confidence interval information because I want to minimize the advantage to early submitters  Early submitters already have the small advantage of having seen their submissions on two different public leaderboards  By releasing confidence interval information Im giving early submitters access to information that isnt available to later entrants  Jase aside from changing the size of the public leaderboard portion of the test dataset we also selected it more sensibly  so it better represents the overall test dataset   JPL a competition using internet chess data is a good suggestion  For interest the reason we are running the competition using top players is because Elo ratings matter most for top players since it is used to determine who can play in which tournaments  Out of interest has anybody entered this competition using Glicko Glicko or Chessmetrics Are either of you happy to send me your unmodified Glicko submission It would be good to add a Glicko Benchmark team to the leaderboard  My email address is anthony goldbloomkaggle com Would like to do the same for Glicko and Chessmetrics if anybody has tried those  I have also contacted Ron about using his Trueskill submission as a Trueskill Benchmark  Jase I posted a link to your Glicko code on the hints page  Its very good of you to share it  Im really surprised that Glicko is performing worse than the Elo benchmark  Do you think this is because Jeff put lots of work into optimally seeding the Elo benchmark Or is Glicko just not as good Vateesh thanks for sending the files  The files that you sent are actually different  I also had a look at your submissions and you have a few files with the same name but different numbers  Also I was not able to replicate the problem as you describe it  Perhaps you can try again and let me know if youre still experiencing the error Was just chatting to Jeff  Time permitting he is going to benchmark some of these other systems  This way they will all be benchmarked on a consistent basis using the same seeding procedure and the same degree of tuning   Hi Vess  This should not be a problem given the way submissions are stored  Uri thanks for pointing out the problem  Were currently working on a big upgrade to the website the new site should be launched by the end of this month  The upgrade will involve a more functional forum  In the meantime I will try and fix this problem  Anthony Uri Im not able to replicate the error either on the live site or on the development version  Can you let me know if you experience it again Hi Hans which post Still cant replicate the bug    intermittent problems are really annoying As mentioned were doing a massive site upgrade at the moment  so thats taking up the majority of our development time  How serious is the problem Can we live with it for the next few weeks until we deploy Kaggle   Eric thanks for the feedback  Theres not really any reason to insist on a particular file extension  Were currently doing a big site upgrade so Ill add this to our list of feature requests  Seyhan the leaderboard portion of the test dataset is selected randomly  It is somewhat representative of the overall standings  I would really like to be more active in the forums  looks like theres some lively discussion happening Ive been flat out working on the site upgrade which is only a few weeks away from launch Anyway Id like to share a few thoughts on this discussion First off there is quite a strong correlation between the public leaderboard and the overall standings  Secondly the lack of relationship between the  scores and the  scores might indicate overfitting  This may be the case if youre experiencing a larger improvement on the  dataset than the  dataset \\xa0 On a related point I notice that youre all performing very well  It could be that youve reached a local maximum i e  the best possible score given the techniques youre using   Just to reemphasis Jeffs point you should pay more attention to your cross validation than to the leaderboard  The leaderboard is calculated on a very small amount of data so it is only indicative  PhillippSorry for the delay in doing this I havent had computer access over the last few days  The Spearman correlation between public scores and overall scores is   I also calculated the correlation for different submission quintiles to make sure the relationship holds at the top it doesTop   \\xa0          Its also worth mentioning that the trouble participants are having  reflects realworld difficulties in formulating a chess rating system  This competition is not just a game but a genuine attempt to explore new approaches to rating chess players Anthony Greg thanks for pointing this out  Im currently traveling but will look into this over the weekend  Wil if you can get historical data from freechess org possibly by agreeing to share the winning method with them wed be happy to host a comp here  This way you could specify that the winning method must be an instant gratification system  It would also result in a system thats tuned to lower ranked players  Thanks for pointing out the error  It has now been fixed  Apologies for any confusion  Cole sorry for the slow response  In this competition all your submissions count  In future we will ask participants to nominate  submissions  Phil that is correct  You must remember that Kaggle hopes to do more than just host fun competitions we want to help solve real problems  This is why were reluctant to force participants to choose just one model they may make a poor choice and the compettion host may end up with a suboptimal model  Our compromise position is to allow partipants to nominate five entries a feature which well roll out for future competitions   Phil number  is correct  Luck will play a part but I suspect the test dataset is large enough to limit its impact   I agree in a competition like this one  But as mentioned above we want to host competitions that are useful as well as fun  An upcoming competition will require participants to predict who has prostate cancer based on  variables  In a competition like that it would be a shame to miss out on the best model  Requiring participants to nominate five submissions seems like a good compromise   Greg this problem has now been fixed  Thanks again for pointing it out  Hi ColeApologies for the ambiguity  The time is as it appears on the competition summary page  adjusts according to the timezone on your computer clock so itll be Saturday or Sunday depending on your timezone You can also see a countdown on the Kaggle home page Anthony\\xa0 Dirk I just changed the file posted on the Data page to a unix format  Hope this solves the problem  Durai apologies for the slow response  All up  countries were represented  Here is the list in order of most participants to fewest United States United Kingdom Australia Canada Thailand India Germany Spain China Netherlands France Italy New Zealand South Africa Sweden Argentina Croatia Ecuador Greece Indonesia Iran Ireland Mexico Poland Portugal Russia Singapore Turkey and Ukraine Ricardo you are correct  I gave the country list for the wrong competition   countries were represented United States Colombia India Australia United Kingdom France Thailand Canada Germany Argentina Japan Afghanistan Albania Austria Belgium Chile China Croatia Ecuador Finland Greece Hong Kong Iran Poland Portugal Slovak Republic Venezuela Uri makes a very good point  One way we could run a competition without knowing future matchups is to have participants rate every  player  Once we know the matchups we can infer predictions based on players ratings The only downsides to this approach are  It doesnt allow for probabilistic predictions since there are many ways to map ratings into probabilities   We couldnt show a live leaderboard  which helps to motivate participants  Interested in others thoughts on this particularly the importance of a live leaderboard   Ron this is fantastic Looks like a sizable proportion of the black dots are sitting in a vertical line  Though Im sure the Elo Benchmark would look much worse   Out of interest what software did you use to generate the viz ps  Im guessing the anomalies that this viz highlights e g  that white is a smaller advantage for lower rated players could inform future versions of your rating system  Philipp I dont fully understand your suggestion  Do you mind trying to explain it again Possibly by reference to an exampleAs a general principle tne problem with attempting to prevent people from using neural networks and the like is that participants use them anyway and then overfit other systems to replicate the neural networks results  I actually think that having neural networks et al in the competition is valuable  Even if they wont be implemented as rating systems they may have some benchmarking value  Assuming they predict most accurately they give a sense for what level of predictive accuracy is possible from any given dataset  As an aside if we require participants to submit ratings and dont \\r\\ngive them access to the matchups that theyll be scored on this should\\r\\n force participants to create a rating system    shouldnt it\\n BTW Jeff re  I have been and continue to be amazed by the level of participation so far \\xa0 I had no idea so many people would participate  Congratulations on organising such a popular competition PEW what criteria would you use to evaluate such systemsBTW I think youd be surprised at the proportion of the top  who are building rating systems   Philipp thanks for pointing out this bug  The error was only aesthetic  had been accidently hardcoded into the new theme  The platform was still only permitting two submissions  Anyway the error has been fixed   Philipp thanks for your nice words Hopefully having a more professional look and feel will help us attract interesting competitions with bigger prize pools   Hi allWondering why the benchmark is still leading when it is publically available   Have people had trouble replicating the authors methodology Or is everybody trying their own approaches Anthony Hi JesseYou are correct this is instruction is wrong  The monthly columns  mm should be  lines long including the header and the quarterly columns qq should be  lines long The examplesubmission csv file available on the data page gives an example  Im at a conference today but will correct the instruction as soon as I get the opportunity  Something was amiss  There was an error in the data uploaded on Kaggle Kaggles fault not the authors The changes are not particularly big so models that performed well on the previous dataset should continue to perform well  To give you the opportunity to rerun your models and make new entries we have extended the competition deadline by two weeks and lifted the daily submission limit to three per day  And I believe George intends to release the code used to create the benchmark  Apologies for the error Dont hesitate to ask if you have any questions  Unfortunately the movie isnt out in Australia yet weve still got another week to wait  Sorry for the slow response  Ive been flat out with the new site launch  Below is the list of rows used to calculate the public leaderboard Dirk Ive changed the line break format  Let me know if this doesnt fix the problem   Jason theres a bug that prevents users seeing previous scores when they have longish technique descriptions  We are aware of the problem and will fix it as soon as we can Diogo thanks for pointing out this error  We will setup pagination on the submission page shortly   Tim Kaggle is currently in the process of putting together a  league table which ranks participants based on competition performances If you perform well in this competition it will count towards your  ranking   Phil I made an error in the ten per cent listed above try scoring with the following rows Steffen you can enter using a model coded in any language  JohnDrew I presume those who enter using software other than R are still eligible for prizes\\xa0 Diogo thanks for pointing out this bug  Few minor teething problems with the new site  we should have them sorted out before long   YuchunApologies for this error  The public leaderboard is portion of the test dataset is actually the first  per cent because we hadnt implemented the code to select a random portion of the leaderboard yet  For info the reason we kept getting different  per cents is because the random seed in the database was set to zero which told our code to choose a random random seed Anthony Hi JonIts a fixed  per cent chosen randomly Anthony Hi TamasAs your results suggest the order does matter and the IDs dont  Anthony JC I agree that those who enter early have an advantage  However the main source of advantage comes from the fact that they have had the opportunity to spend longer on the problem and try more things  Philipp the current leader has made  entries  If this competition took ternary scores loss win draw this would amount to  possible combinations  making Phillips  entries\\xa0 a drop in the ocean  In fact the test dataset is richer because participants predict the probability of victory Nonetheless for future competitions we will ask participants to nominate five entries that count towards the final standings   PEW we are not requiring participants to guess but rather encouraging them to rely on their cross validation when determining which models to choose  The problem with allowing people to enter many times and try many parameter tweaks is that they are more likely to accidentally overfit on the test dataset  By this I mean they are more likely to find a parameter tweak that works well on the test dataset but doesnt work as well for future chess games On your second point you are correct to say that I am worried about statistical guessing  The requirement that participants submit code does not obviate this concern because models can be overfitted once the answers are known  In the extreme case somebody could fit a decision tree that classifies every game perfectly if they know the answers  Showing the standings but not the scores makes statistical guessing only slightly more difficult because participants are close enough that the leaderboard ordering gives meaningful feedback on which guesses are better and which are worse As an aside it seems that I have failed to convey the message that the public leaderboard is purely indicative and that cross validation is  important  I would even go so far as to say that it may be problematic if the public leaderboard bears too close a resemblance to the overall standings   I like Uris suggestion  It gets around the problem that LT mentons while potentially encouraging people to try things beyond parameter tweaks  Couple of potential problems  A participant exhausts the submission limit and another entrant makes and shares a breakthrough eg the use of Chessmetrics in this competition Anybody who has exhausted the submission limit wont have the opportunity to build on the breakthrough  This seems less than ideal given that we want to get the best results possible   It might encourage people to make all their entries at the end so that they dont reveal the strength of their hand  What do others think Philipp Kaggle has been experiencing a massive lift in site visits and\\r\\n signups since the new site launched from  unique visitors to   This accounts for the increase in entries  Thanks everyone for making this an amazing competitionBig congratulations to the winner Outis  Also to the runner up Jeremy Howard who only joined the competition late in the piece and to Martin Reichert who finished third Hopefully well get some of the top ten to tell us about their methods on the blog  In the meantime I encourage you all to tell us a little about what you tried on the forums  Also for interest heres a chart that shows how the best score evolved over time  Rapid improvements initially but after a month progress stalled as participants approached the fronteir of what is possible from this dataset  I think I can help with this I dont give names just score combinationsscore publicscore                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Jeff can I post the test labels on the forum I only seem to have the aggregate solution on hand attached  Jeff do you have the game by game labelsEdit looks like you posted a minute before me Hi NickYoure welcome to bring additional data as long as its publicly available Anthony Attached is some sample Python code that generates forecasts based on the last known travel time  Im new to Python so happy to hear any feedback on the code  File didnt attached  Heres the codeimport csvimport datetimerhopenRTAData csvr read in the data whopensampleNaivePython csvw create a file where the entry will be savedrhCSV  csv readerrhtimeStamp                                 an Array with the cutoff pointsforecastHorizon   forecast horizon in lots of  minutes e g     minutes  minutes   hour  This is used for calculating the forecast time stampsrow   inialise the row variablefor data in rhCSV loop through the data\\xa0\\xa0\\xa0 if row   if the first row then write the header\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for j in rangelendata\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh write  dataj\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh writen\\xa0\\xa0\\xa0 if data in timeStamp if the row is a cutoff point\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for i in forecastHorizon for each forecast horizon write the cutoff travel time as the forecast the definition of Naive \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dateStr  strdatetime datetimeintdataintdataintdataintdataintdata  datetime timedeltai calculte the time stamp given the forecast horizin \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh writedateStr write the timestamp to the first column of the CSV\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for j in rangelendata\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh write  dataj write the cutoff travel time to the subsequent columns \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh writen\\xa0\\xa0\\xa0 row  rh closewh close Dirk thanks for pointing this out  Ive written to the RTA about this and they responded sayingIndeed\\xa0 our control room have confirmed significant increase in traffic volumes following the removal of the tolls  This has had an impact on the overall travel times across the M Something to be aware of when using the older data   Hi Dennis The  per cent doesnt count towards the final standings and is selected at random across the  timestamps and  routes  As for the SMTP error its been fixed  The problem was the result of a flood of signups which caused Google to shut off our mail server  Were now using our own mail server  Anthony Apologies for the error its deciseconds not centiseconds  so  is   seconds  Ive fixed the description   This is something that should be dealt with on a case by case basis  If you find a dataset youd like to use ask on the forum and Ill run it by the RTA  For information Im trying to get hold of some incident data  Will keep you posted on this   Lee this is great Dirk did the same thing with some Python sample code I wrote for the social networking competition  If you guys keep showing me how things can be done better I may become a half decent coder  Toppy thanks for the pointer  A higher priority at the moment is to get forum attachments working again  Hi Peter Ill follow up in this  At the very least we should be able to provide information on the length of different routes  Anthony Hi Carlos Unfortunately not  Clause  c in Kaggles Terms and Conditions saysc            employees or agents of the Competition Host are not \\r\\neligible to participate in any Competition posted by the Competition \\r\\nHost\\r\\n\\tTo answer the second question we would more information about the nature of the business and what your friend does Anthony C does seem to be an expressive language  Im a Linux user though so not inclined to pick it up   Armin I agree  Makes more sense for me compile this information once for everybody  Will try and get it done this week  Daniel Dennis is correct in saying that averaging the values leads to floating point numbers  The answers are integers but the RMSE is calculated using floating point arithmetic   Daniel and Dennis are correct  Keep in mind that the  per cent is a random selection of the  that doesnt count towards the final standings which are calculated based on the other  per cent  The cutoff times are all between am and pm  They were selected using a simple formula that favoured high volatility cutoff times over low volatility cutoff times  So youll see more peak hour morning and afternoon cutoff times  The rationale behind this is that its more important to predict accurately during high volatility times so we want to favour models that do best at these times  That explains why the RMSE is higher than for randomly chosen cutoff points  Aidan have asked the RTA about this  This was the response  The cutoff is due to free flow conditions imposed by the system during data unavailability Ive written again asking for a little more detail  Will post the response when it comes   Paresh thanks for the thought provoking question  I agree with Dennis I am more interested in the time delay than the percentage delay  On a related matter we think it is more important to predict correctly when travel times are volatile e g  before and after work  To favour models that predict more accurately during high volatility times we selected more high volatility cutoff points so youll notice more cutoff points during the morning and afternoon  Phil thanks for sharing this  Just got to find a Windows machine to run it on     Hi Markus I can help out on the second part of your query Ive posted some PHP AUC code on another forum post  software packages like R have easy to use packages that calculate AUC  Anthony Rasmus apologies I deleted the wrong post  Anyway you asked how travel times are measured  There are regularly spaced loops along the M  These loops measure each cars speed and the number of cars that travel across the loop every three minutes  travel times are then calculated using a formula  The formula has been tested and calibrated using test cars that travel along the freeway and record their travel times   Thomas I selected specific cutoff times randomly but chose timeday combinations that are volatile across the dataset  Vitalie the volumes data is used to calculate travel time  see this post for more info  Our priority at the moment is to get the incident loop error and route length data together  However I can find out if this data can be made available if you think it might be useful  As Dennis says itll be highly correlated with travel time and we obviously wouldnt release it for the blanked out times   Jeremy I wasnt aware that public documents with traffic details were available  To the extent that any information is available for blanked out times this would most definitely be considered cheating As for question  I am aware of this in fact the issue came up in another post  The rules state that the winning model must be implementable by the RTA in order to be eligible for the prize  The averaging model passes this test  As an aside I dont believe the temporal leakage invalidates the algorithms developed in this competition   Benjamin once we get the incident data I will put in a request for this data   I have some information on suspect loop readings that Im working to release  This has information on when loop readings may be unreliable for various reasons  I dont yet know whether or not this will help with the free flow issue  Anyway I will upload them as soon as I can get it into a useful format I suspect the reason the free flow times are different is because route lengths are different  Rob on your point about missing data it might be helpful if I explain how I put the files together  I received data in the following formatroute IDtimestamptravel time xxx xxxI transposed them into in the hope that theyd be more manageable  When timestamps were missing I just filled in a blank row  Hassan the most important file is RTAData csv  You can create a sample entry by  downloading RTAData csv and createHistorical php attached to the same directory  navigating to that directory in the terminalcommand prompt  running php createHistorical php This will create an entry based on a historical average for that timeday and is a good starting point  BJB very generous of you to upload a Java code  Ive now enabled  java file uploads so you should be able to upload the file   burak the times in sampleEntry csv are the times you need to generate forecasts for  Theres more info on how the  cutoff points were selected in this forum post  Aaron you raise a good point  According to the route definitions I have route  extends from loop A to loop A while route  extends from A to A so  should encompass all of   Denniss observation that sometimes  has longer throughput times than  is strange  Ill double check the definitions with the RTA   Aaron another good question  Have also passed this on to the RTA   The number directly to the left of the team name is the teams position and the number to the right of the team name is the teams score or Root Mean Squared Error RMSE   Alexander to me this means that the algorithm can take a timestamp as an input and can generate forecasts for the next mins mins etc  \\xa0Lee thanks for pointing this out This post  post  Alexander there is no truncation of floats \\xa0 Mmm    my message seems to have disappeared from the board  Anyway heres a repeat Aaron the units are deciseconds Nick actually its a hybrid approach  You can nominate five entries that count towards the final standings  You do this from the submissions page  the last five are chosen by default  At the end of the competition the best of your five nominated entries counts towards your final position  And Nick on your new question the one of the five you nominate that scores best on the  per cent counts  The  per cent is meaningless as far as the final standings are concerned \\xa0 Jose do you want me to ask if its permissible to use NOAA data If so are you asking about the data that Brad mentions above Really nice feedback  very thought provoking \\xa0The API suggestion is nice  It does seem that it would prevent people from using the future to predict the present  However the testtraining split is still necessary to prevent overfitting and we could still only give partial leaderboard feedback the API doesnt secure against overfitted parameter tweaks  Also the API approach would add new problems\\xa0  models will take longer to run because of the delay in receiving data points  as you say it would add a huge load on Kaggles servers\\xa0As for the problems you list here are my responsesPredictions can’t use all available prior data since the test data doesn’t provide resultsThis is necessary to ensure against overfitting  If all the data is used to calibrate a model its impossible to know if the model will fit future datasets as well  \\xa0Limited training and test data creates too much variance between the public score and actual scoreThe mistake made in the first competition was with the size of the public leaderboard portion of the test dataset my fault not Jeffs  It was too small which lead to the low correlation between public and overall scores  For the RTA competition we raised the proportion to ensure a stronger correlation  This proportion was calibrated after some testing of the correlation between the two parts of the test dataset  We intend to continue this practice going forward \\xa0Model parameters can’t be tuned because actual scores aren’t provided\\xa0If we allowed parameter feedback on the whole test dataset this would almost definitely lead to overfilling parameter tweaks that work on the test dataset but wont work for for future datasets \\xa0Number of submissions is severely limited because they are so large this will become a bigger problem as larger test datasets are created\\xa0I dont think more daily submissions are necessary because the majority of model building should be done with reference to a cross validation dataset \\xa0Leaderboard doesn’t reflect actual leadersAgain this was my mistake  I made the public leaderboard portion of the test dataset too small  This is not a flaw with the general approach \\xa0Future data can be used to predict the pastJeff suggested a really nice solution to this test set includes some spurious games so that people can’t mine the test set for useful data about the future  These spurious games wouldnt be used in final evaluation \\xa0The API also provides a really nice answer to this problem  Attached is some R code to create a GLM entry for this competition  As always happy to hear feedback from others about how this could have been done more elegantly  Anthony Nathaniel is right  the data is correct its just a problem with heading formatting  Will fix this shortly and reupload the data   Eleni just uploaded RouteLengthApprox csv which has approximate route length data   Konstantin just uploaded RTAError csv the is valid data  Its available on the data page  Finally  fixed the headings  Just to reiterate all the data are correct  its just the capitalization in the headings that caused trouble As for the inconsistent numbers of delimiters also fixed  my software package stopped printing delimiters when there were no more values or NAs in a row  Jack the country of birth issue is now fixed  Please download the latest version of the data   P V Kiran it means that if your solution is implemented using a software package that is not available to the University of Melbourne it must be possible to translate your solution into a different packagelanguage   Mooma I appreciate your frustration but sensor malfunctions  are part and parcel of dealing with realworld data  If we had the data ready at the outset we might have excluded failed sensors and downweighted the impact of partially failed sensors when evaluating predictions   Konstantin Dennis is correct it is not safe to assume that there is no errors in the control data   Jose and Joseph just spoke to the RTA about this  The answer is no because it might allow future weather conditions to be used to predict the present  Ahmed just got an answer from the RTA on this  Heres the responseThe answer is  maybe  RTA would request that anyone wishing to use the data for further research purposes write to the RTA and make their case  describing what they wish to do ie the purpose of the research and how they would use the data  The RTA will consider each application on its merits \\xa0Let me know if youd like me to pass on the relevant email address  Im reluctant to do it in the forum but will offer an introduction to anybody who asks  Just elaborate a little the types of solutions that cant be implemented are those that are encumbered by patents or other intellectual property restrictions  Michelangelo truth is that you can submit any real number we suggest a number between  and  because of the convenient interpretation  AUC ranks your scores  the higher the score the more confident you are that the instance is a member of the positive class  Many thanks to everyone for all your great activity on this fascinating problem  insightful questions and comments on the forum good early results on the leaderboard and interesting discussions\\xa0There have been a lot of questions about exactly what constitutes an acceptable model for the RTA  So far my guidance on this matter has possibly been too fuzzy and I hear a lot of you looking for more definite rules  Therefore we have come up with the following specific rule regarding the allowed model inputs\\xa0Your model can be of any form you like as long as it takes its input only from the following parameters Time of prediction Day of week Is holiday Month of year Route number to be predicted The time taken for route r for datetime t where\\xa0 r is any route and t is any time less than the datetime being predicted for as  many routes and datetimes as you wish The sensor accuracy measurements for any routes r and datestimes t defined as above The estimated route distances as provided by KaggleTo clarify the following are not permitted The use of any data other than those provided by Kaggle for this competition and the list of NSW holidays  The time taken for any routes in the future compared to the prediction being made  your model can still be trained using all data as long as the resultant model only uses the inputs listed above Furthermore the algorithm must not be encumbered by patent or other IP issues and must be fully documented such that the RTA can completely replicate it without relying on any black box libraries or systems  Hi Alexander  No  Using full timestamp makes it possible for a model to implicitly incorporate external data and future data   You may also use holiday data extracted from the PDF file that you linked to in order to get holiday information for previous years  However we will not be providing a file of this information directly   This is correct Anthony personIDs refers to all the columns that have investigator IDs e g  column  has investigator  column  has investigator   Ignore the comment numerical values that should be    As Jeremy Howard pointed out earlier in this thread the key point that answers most of these questions is that the limitation is only on the functional form of the final model  More specifically\\xa0Xiaoshi Lu You can build your model  filtering aggregating etc  using all the datetime information you like  The final functional form that you end up with however should only use the predictors listed above \\xa0Mooma The inputs listed include this The time taken for route r for datetime t where\\xa0 r is any route and t is any time less than the datetime being predicted for as many routes and datetimes as you wish  So what you ask is specifically allowed  Of course for you to create your input file which includes for example the time taken one hour earlier you will need to use the full datetime  However the resultant model will not directly use this  instead it will only use the time taken on that route as allowed by the rules \\xa0Alexander Groznetsky Imagine using a very flexible model neural net for instance which trains with all datetime info included in the input parameters  It might implicitly end up using the route times later in the day to predict those earlier This is an example of how a model could be useless in practice even although it appears highly predictive on the competition data \\xa0 Matthew  Using GPL code is fine \\xa0  The isholiday variable can be a direct input rather than a variable that is derived by reference to a timestamp \\xa0  You contact me directly at anthony goldbloomkaggle com \\xa0Dennis you can use isspringbreak rather than isholiday \\xa0 David its really neat For info it works in Safari but the page videos are aligned a little strangely  Martin Dane is correct the information in\\xa0RouteLengthApprox csv is in metres  So\\xa0route  is approximately  km \\xa0 The In the Money indicator is based on the public leaderboard only  It doesnt reveal anything about the final standings  Martin when I open the file it shows  and   What application are you usingAnthony I believe it refers to grants made when the researcher was at another university \\xa0 Nicholas a Matlab solution is fine as long you dont include libraries that use patented or undocumentedsecret algorithms \\xa0 Apologies Will  I was on a plane and only just got your message  Will make the adjustment this afternoon Id also like to congratulate the top teams and congratulate Dirk for running an excellent competition \\xa0 Thanks to everybody who participated and a big thanks to Dirk for putting together a really nicely designed competition  The test labels are attached to this post  Rafael  and  are fine   is also fine as long as the data is derived entirely from the time series as you say  \\xa0 Reginald please email your submission to anthony goldbloomkaggle com and Ill have a look  Apologies I didnt clarify this with Mahmoud before the launch but we have discussed this offline This competition requires you to choose five entries that count towards the final result  To choose five entries visit your submissions page and click the star next to the relevant entry to select it  If you do not choose any entries your last five entries will be chosen by default  Michelangelo the  per cent comes from the test dataset  Eu Jin Lok the sampling is done randomly   For anybody interest heres the actual solution  Hi GregThe answers will be made available on the forum \\xa0I can ask whether the data can be used for publishing research if you likeKind RegardsAnthony\\xa0 Hi Greg and SuhendarThe university doesnt want the data to be used for any purpose other than for this competition Anthony Hi GregIt would be nice if the dataset could be used for other work  However if we dont allow competition hosts to place restrictions on the use of their data then we wouldnt get access to it in the first place \\xa0Will post the solution file now RegardsAnthony The solution file is attached to this post Thanks all for participatingAnthony  Entries made before we fixed the leaderboard were scored incorrectly  I have now rescored the relevant entries  The error was the fault of Kaggle and not the competition organizers \\xa0ApologiesAnthony Hi CerinApologies for the errors  They all stemmed from the fact that the servers hard drive filled up  Ive cleared some space \\xa0For information were currently rewriting the entire site for the Heritage Health Prize  You can expect the next version to be faster and include many more features Thanks for your patienceAnthony Hi CerinAli is right your entries will count towards the final standings \\xa0Anthony Hi allSubmitting from multiple accounts is most definitely against the rules \\xa0We have done some analysis and found that it happens very rarely  However we are working to put the systems in place to identify and block those who attempt to do it Kind RegardsAnthony The solution is attached Thanks all for participatingAnthony HarriThanks for the thoughtful post  The IJCNN people agree with you and have decided not to disqualify Shen \\xa0As mentioned above Kaggle will soon have the systems in place to detect multiple accounts in real time so that such issues dont arise Anthony I have sympathy for peoples frustrations  In this case the competition host decided that the results should stand  so we are facilitating their decision Chris makes a good point about the rules being scattered throughout the site  We will be sure to address this in future competitions  We will also ensure that they are tightly enforced  For information a lot of effort has gone into framing the Heritage Health Prize rules Finally thanks for the feedback  Its discussions like this that will help us improve Kaggle  Kaggle has received legal advice after the controversy surrounding this competition \\xa0We have been advised that it sets a dangerous precedent for us to ignore our own terms and conditions notably clause   preventing multiple signups  We have therefore acted in accordance with this clause\\xa0disqualifying those who clearly submitted from multiple accounts Thank you all for your patience on this issue and rest assured that we are working to ensure that it is not a feature of future competitions  Entrants are welcome to use other data to develop and test their algorithms and entries until  UTC on April   if the data are i freely available to all other Entrants’ and i published or a link provided to the data in the “External Data” on this Forum topic within one\\xa0 week of an entry submission using the other data \\xa0 Entrants may not use any data other than the Data Sets after  UTC on April   without prior approval  Also covered by Slate and Forbes\\n \\n \\nand the Wall Street Journal a couple of weeks ago\\n  And Smarter Planet\\n  The criteria was that somebody had to\\r\\n  make at least one claim in Y \\r\\n  be eligible to make a claim in Y\\r\\n\\r\\nOutliers have been removed from the dataset as well as those suffering from stigmatized diseases  Just to clarify when Jeremy says we cleaned it as much as we can we didnt do much to the claims data on purpose  We figure it makes more sense for you to make your own cleaning assumptions rather than have us impose them on you  Not only are patients who died in Y not in the dataset but patients who died in Y are also not in the dataset because they didnt remain eligible to claim for the whole of Y  Apologies this was an error  Thanks for drawing our attention to it \\r\\n\\r\\nThe missing values are for those people who have been in hospital for more than two weeks  They should be replaced with a   You can either do this yourself or download the updated dataset  \\r\\n\\r\\nFor information members who have in hospital for more than two weeks have been grouped for privacy reasons they are rare so may otherwise be identifiable  The implication of this grouping is that if you expect somebody to be in hospital for more than two weeks you should predict  days  \\r\\n\\r\\nThis grouping should not have a big impact because\\r\\na  members who are in hospital for more than two weeks are rare about one per cent of members\\r\\nb  the evaluation metric favors algorithms that accurately predict fewer days in hospital on the assumption that these are more preventable  Dorofino\\r\\n\\r\\nGreat idea Forming a team is a really good way to learn  \\r\\n\\r\\nAre you affiliated with the New York R Users Group For info Ive heard rumblings about them setting up a team  \\r\\n \\r\\n\\r\\nGood luck with this\\r\\n\\r\\nAnthony Hi Rich\\r\\n\\r\\nJust spoke to HPN about this  For the moment they dont want to provide general guidance and ask that you make a request through the contact us form  Your request should detail the topic of your proposed research  Definitely worth making it clear that youre just looking to publish the method that you use to enter the competition  \\r\\n\\r\\nAnthony The years are sequential  We are not revealing what years Yn refer to nor whether or not they refer to calendar years for data privacy reasons  Apologies for the missing values it was an error  You can either replace the missing values with  or download the updated data set  \\r\\n\\r\\nIf youre interested in the reason for the missing data see\\r\\n  Hi bacg\\r\\n\\r\\n  DaysInHospital refers to Y the second year while the claims refer to Y the first year \\r\\n\\r\\n  Not everything that has a length of stay counts as a hospitalization  In fact you dont have enough detail in the Claims table to calculate DaysInHospital  The detail has been suppressed for privacy reasons \\r\\n\\r\\nAnthony Hi mbenjam\\r\\n\\r\\nWe would have loved to release more detailed data but have to be mindful of data privacy  \\r\\n\\r\\nAnthony Have received advice from the HPN lawyers  Im really sad to say that the answer is no on all accounts  Wgn the intention is not to rule out the publication of research  Ive passed on your message to HPN and a clarification will be forthcoming  The lawyers are taking a conservative stance on this issue  Apologies its really disappointing to have people ruled for this reason  flsdcom I have a meeting with them in  minutes  I will be sure to raise this point  In response to ashashos original question I have sought a reexamination of the issue  The HPN lawyers explained that the reason for the hard line is that they have no way to verify that residency permits comply with US legislation  Im really sorry to say that theres not more I can do  I want to reassure everyone that HPN is working hard behind the scenes to clarify the IP issue  It is not their intention to prevent people from using standard tools nor to discourage anyone from applying their innovative ideas to this problem \\xa0\\nFor background at Mondays launch event Dr Richard Merkin the man behind the prize spoke of the long tradition of innovation that has resulted from past prizes  He spoke of\\n\\nthe Longitude Prize    apparently Newton and Galileo had attempted to solve this problem but the winner was a self educated clockmaker from Yorkshire\\nNapoleons food preservation prize  won by a confectioner and resulted in the invention of canned food\\nthe Orteig Prize to fly nonstop from New York to Paris    won by the unlikely Charles Lindbergh \\xa0\\n\\nIt is his hope that this prize will spur similar innovation to solve one of Americas most vexing problems \\nWe appreciate your patience while we await clarification \\nKind Regards\\nAnthony This is a sample of the final dataset but the final dataset is not in the Terabyte range  To the best of my knowledge this dataset is on the larger side for medical datasets which tend to be quite small \\r\\n\\r\\nThis algorithm will not need to operate in a realtime environment and so there is no restriction on execution time  The decision to predict days in hospital was made to make the test dataset richer  so we can better sort out good algorithms from bad  The logarithm in the evaluation metric was chosen to favor models that predict short stays more accurately as these are assumed to be more readily preventable  \\r\\n\\r\\nAs for the question of nefarious intentions I can tell you what I know about Dr Richard Merkin the man behind the prize  He is a big philanthropist who devotes time and resources to funding scientific projects schools and the arts \\r\\n\\r\\nIn my opinion HPN did not need to put up  million to get an amazing algorithm  Kaggle has found in its own competitions that with prizes as small as  or a chess DVD participants approach the limit of whats possible on a dataset  In our communications with HPN we have been told that the  million prize is an attempt to draw mass attention to this prize and the issue in general  Dr Merkin wants to promote the potential for medical data mining in lowering healthcare costs  The prize also serves to introduce a large number of talented data scientists to medical data \\r\\n\\r\\nFinally rest assured that HPN are working hard behind the scenes to clarify the IP issue  mgomari one issue we have to keep in mind are the tradeoffs in releasing data  For data privacy reasons HPN have a granularity threshold which theyre not willing to breach  The data anonymization team represented by keleman in the forums are trying to release CPTCodes probably at an aggregated level  Apparently its pretty lineball and releasng DaysInHospitalY might put this in jeopardy  I describe the data privacy considerations like a waterbed you push down on one part of the bed and it creates a bulge somewhere else \\r\\n\\r\\nAfter May  youll be able to use DaysInHospitalY and DaysInHospitalY to predict DaysInHospitalY \\r\\n\\r\\nogenex even if we release DaysInHospitalY you wont be able to do a consistency check  Not all length of stays count as hospitalizations as calculated for this competition and you dont have enough detail in this dataset to work out which count and which dont  For those who dont know jphoward was Kaggles most successful competitor before joining the team  His tutorial gives really clear explanations of the tools and techniques that made him such a successful competitor  Hi Jim\\r\\n\\r\\nThat is correct  For information the reason for the misnomer is that it was days when we sent it to the anonymization team but they had to group the days to ensure the required level of data privacy \\r\\n\\r\\nAnthony sciolist yes teams are required to publish publicly  ashojaee the clarifications havent been made yet  mkarbowski as jphoward keeps pointing out theres often a massive disconnect between reality and the contents of a transactional database  See ejloks humorous post for even odder records\\r\\n  Agree  See the updated evaluation page\\r\\n  We intentionally decided against cleaning the data so as not to impose our assumptions on participants  We want the forum to be tightly integrated into the site e g  to be able to link to forum posts from profiles and vice versa  YAF is the best  NET forum software out there and integrating it into Kaggle is more trouble than its worth  \\r\\n\\r\\nAlso moserware is a brilliant programmer so its the type of thing he could put together in less than a week  Realworld data is messy \\r\\n\\r\\nWell put up a data dictionary soon  quotedaveime\\nSeriously I understand the need for randomizing and anoymizing the data but unless they have some way to unrandomize it afterwards any algorithms we create will serve no real world application \\nquote\\ndaveime the data is messy not because its been peturbed but because its realworld data  Anonymization focused on generalizing again not peturbing   The the nineyear old pregnant males actually exist in the raw data \\nFor info Im told that this is one of the cleaner medical claims datasets around  fjn Pi does not have to be an integer  blonchar youre correct HPN are limited in what it can release by the need to protect patient privacy  mgomari the answer to both questions is yes  jesensky you will be able to use DaysInHospitalY and DaysInHospitalY as an input to DaysInHospitalY \\r\\n\\r\\nI like your thinking on the USE OF OTHER DATA loophole if the answer had been no  Creative thinking cybaea many thanks for a great discovery After doing some digging weve discovered that the oddeven observation is an artifact of the cleaning procedure  \\r\\n\\r\\nWe have worked out a remedy and it will be applied to the dataset that will be released on May   In the meantime it shouldnt make a huge different to models that are currently being developed  boegel yes  On May  we will be issuing significantly more data  DayInHospitalY csv will be changed then  liveflow I may be misunderstanding the question but the competition requires participants to use data from Y Y and Y to predict Y  No  Some Y patients are no longer eligible in Y  We still provide Y patients who arent eligible in Y because theyre useful to train on  Information Man that is not the intent of the rule  The HPN lawyers are working on clarifying this at the moment  DougieD every member listed in DaysInHospitalY is eligible to claim in Y  so if they have  DIH  they are  above  The same will apply for the members listed in DaysInHospitalY and DaysInHospitalY when we release those files  irwint good pickup  thanks Now fixed  gschmidt not sure if this answers your question but the geographic spread is limited to the area in which HPN operates southern California I believe  \\r\\n\\r\\nAs to whether patients change doctors on May  youll have a few years worth of data so will be able to work this out  alexx the HPN lawyers are working on a clarification  This will be released by the time entries can be made on May   metaxab the competition was designed this way to replicate how the model might be used in real life  In a real life situation you wouldnt be able to predict hospitalization with contemporaneous claims  DaysInHospitalY is derived from the claims table where a hospital stay includes an inpatient stay or an emergency visit  Note you dont have enough information to calculate DaysInHospitalY from the claims table  Hi Drew\\r\\n\\r\\nIt will be in place by May  when entries are accepted  Anybody who accepted the existing rules will receive the notification via email  \\r\\n\\r\\nAnthony You will get some procedure code information in the May  release  I understand the frustration but data privacy is a priority for HPN  For generating features I recommend SQLLite  though MySQL does the same thing  I know Jeremy and Jeff like Cs Linq  For building models I use R  rks we will post a sample entry with the rest of the data on May   trezza and RHM Y contains data for a  period  trezza unfortunately not  The anonymization team have identified this as a data privacy risk  Hi Allan\\nThats because some members have had claims suppressed  In release  coming soon well make it clear which members this applies to \\nAnthony\\xa0 Hi Domcastro \\n  Can I use R\\nYes\\n  Can I use Weka\\nYes\\n  Can I use Excel\\nYes\\n  If I organise the data in a novel way and just use a standard processing algorithm such as Naive Bayes is this OK\\nYes You must preserve the order in Target csv  Release zip does supersede Release zip  \\xa0 Unfortunately not  Apologies for any inconvenience  Darragh its a list of all members in the dataset  No  Chris just heard back from the data anonymization team  Members have been renumbered  mkwan you fill in the team wizard when you make your first entry  Team mergers will be granted at the organizers discretion  Yes  We cant give you an HPN benchmark because theyve not tackled this problem before \\xa0 boegel DaysInHospitalY contains members who made a claim in Y and were eligible to make a claim in Y  DaysInHospitalY contains members who made a claim in Y and were eligible to make a claim in Y  Similarly target csv contains members who made\\r\\n a claim in Y and were eligible to make a claim in Y  To be eligible means to be an HPN member regardless of whether or not a claim was made \\nTherefore the  members in DaysInHospitalY are not missing from target csv but rather didnt make a claim in Y or werent eligible to make a claim in Y  Therefore all members in target csv were eligible to make a claim in Y  so we have an answer\\r\\n for each of these members \\nJESENSKY by my calculation  members appear in DaysInHospitalY and DaysInHospitalY but not DaysInHospitalY perhaps you can confirm this figure  These members are missing from DaysInHospitalY because they didnt make a claim in Y despite\\r\\n being eligible \\nApologies if we didnt communicate this effectively in the description pages  DanB youre right about the selection bias  But because HPN are releasing almost no information on the members themselves theres nothing to model on for patients without claims  ProTester theres nothing in the raw data that distinguishes a death from a patient that leaves HPN for another provider  SSRC mapping LOS to DIH is impossible  Not every LOS entry corresponds with a DIH e g hospice stay One reason somebody may have DIH in y but no claims is if they werent eligible to claim in Y in which case their Y claims wouldve been removed  George there are  members in the dataset but you are only tested on  members  Thats because the extra  members arent eligible to claim in Y or didnt claim in Y  They have only been provided to help you train your model  Tom SF Haines Jeremy is not the author of the rules  He is merely trying his best to point people to the section that makes the rules as competitor friendly as possible given HPNs requirements \\nAlso if you would like to publish your algorithm I strongly encourage you to put in a research request using the Contact Us form   is the maximum  Ive said this before but I think \\r\\nJeremys tutorial is really excellent although it is not focussed on HHP  He is hoping to get the opportunity to do an HHP tutorial in the next few months  \\xa0 She will be added in the next release \\xa0 The intent of that provision is to prevent the data being shared with those who have not agreed to the competition rules  Jeff was just referring to the measures he would take to ensure the data isnt accessible to others  Jose thanks for your diligence on this  Its difficult for us to give specific guidelines  Again HPN is just trying to prevent the data from being accessible to those who havent accepted the rules  Jim its being assessed against Y hospitalizations  Bernhard your interpretation sounds about right to me \\xa0 Thanks Dave  The data description has been fixed  Hi Bobby\\nCan you clarify what you mean by this Are you asking if they are obliged to share their model if they finish in first place\\nAnthony Hi Willem\\n\\nto what extend the results have to be identical for example small differences in the random number generator may give different results although they should be similar\\r\\n\\nThey do need to be identical  You can give your random number generator a seed to make sure the resultls are the same each time \\n\\nin how much time should the results be reproduceable my current best result is a mix of many models each may take minutes to hours to generate\\r\\n\\nThere is no rule about execution time   \\n\\nthe algorithm should produce similar results on a new dataset this doesnt sound very realistic I dont think there is any way to win this competition without optimizing for this specific dataset  Results on other datasets may be very bad with the\\r\\n given optimizations  Probably very good results can be produced by the same algorithm after some tuning but this is a process that requires a lot of knowledge about the used algorithms and a lot of time and patience \\r\\n\\nNot sure I follow why this is an issue  Remember the Milestone prize is judged in a portion of the test dataset that participants have not been given any feedback on  Perhaps Im misunderstanding the concern \\nHTH\\nAntthony Regarding the requirement that solutions be identical\\nWillem it would be better to have participants spend time on innovation rather than reproducibility however its important to have strict rules so that the competition remains as fair as possible \\nB Yang with regards to the compiler issue we can address it if the issue arises  For example we might start by ensuring that the same compiler is used for verification \\nSali Mali it is exceptable to describe the algorithm and not how it is derived  We are seeking clarification from HPN on the inconsistency that you describe  Apologies for the delay \\nRegarding the requirement that the algorithm perform similarly on a separate dataset\\nThis is best answered by explaining the rationale behind the rule  It is there to catch any cheating or blatant overfitting  If youre not blatantly overfitting then youre likely to be on safe ground \\xa0 Hi all\\nNot ignoring this thread  Just seeking clarification from HPN on one issue  \\nAnthony John\\nOnly the lowest of the five entries count  Note for the milestone prize only one can be selected \\nAnthony I have checked with HPN and a milestone prize winner can choose not to disclose their method but will not be eligible for the milestone prizes \\r\\n Sorry for the delay on this was just clarifying some issues with HPN \\n\\nIs it inconsistent as Sali Mali pointed out in another thread to require documentation of the winning algorithms be publicly disclosed to all competitors given Rule  Entrant Representations \\xa0It seems that this disclosure will encourage other competitors\\r\\n to use aspects of the winning Prediction Algorithm which cause violation directly or otherwise of i  iii and possibly iv of that Rule \\r\\n\\nRule  does not apply to the extent that it prevents a competitors other than a milestone prizewinner from using code published by a milestone prizewinner in accordance with competition rules and b a milestone prizewinner from competing subsequently\\r\\n in the competition using code for which it was awarded the milestone prize \\n\\nCan you clarify that code libraries and software specifications\\xa0are not required\\xa0to be publicly disclosed to competitors \\xa0These materials and intellectual property appear to be referenced separately from Prediction Algorithm and documentation \\r\\n\\n\\nChris correctly points to Jeremys response in an earlier forum post\\n“Only the paper describing the algorithm will be posted publicly  The paper must fully describe the algorithm  If other competitors find that its missing key information or doesnt behave as advertised then they can appeal  The idea of course is that\\r\\n progress prize winners will fully share the results theyve used to that point so that all competitors can benefit for the remainder of the comp and so that the overall outcome for health care is improved ”\\n\\n\\n\\nWill Kaggle or Heritage have a moderation or appeals process for handling competitor complaints \\xa0From the winning entrants pointofview they would not want to be forced through the review process to allow backdoor answers to code and libraries which\\r\\n accelerate a competitors integration of the winning solution  \\n\\nKaggle and the HHP judging panel will moderate the appeals process \\n\\n\\nCan you comment on the spirit and fairness of the public disclosure of the Prediction Algorithm documentation and its impact on competitiveness \\xa0In particular if the documentation truly does meet the requirement of enabling a skilled computer science\\r\\n practitioner to reproduce the winning result then this places the winning team at an unfair disadavantage all competitors will have access to their algorithms and research in addition to the winning algorithm \\r\\n\\n\\nThis rule is in place to promote collaboration  Those who would prefer not to share can opt out of the prize \\n\\n\\nCan you provide more detailed clarification on the level of documentation required by conditional milestone winners \\xa0The guideline provided by the rules would cover a range of details and description spanning from lecture notes to detailed tutorial\\r\\n to whitepaper to conference paper etc  \\n\\nHopefully this was adequately dealt with in Jeremys response requoted above  Let me know if further clarification is needed \\n\\n\\nCan you comment on the reproducibility requirement \\xa0For example it is possible to construct algorithms with stochastic elements that may not be precisely reproducible even using the same random seed is it sufficient for these algorithms to reproduce\\r\\n the submission approximately \\xa0What if they dont reproduce exactly or reproduce at a prediction accuracy that is worse than the submission score possibly worse than other competitor submissions \\xa0\\r\\n\\n\\nExactly reproducibility is required \\xa0   Correct SirGuessalot thanks for the pointer  Its been added to our issue tracker  I must admit we have higher priority issues to tackle but well get there eventually \\r\\n Just to keep you all in the loop the plan is to announce the milestone prize winners at OReillys Strataconf   Will let you know the exact date as soon as\\r\\n were told \\xa0\\xa0 Full milestone prize rankings will be released after the announcement is made  The rules do not prohibit Oracle Data Miner   Hi all \\nHPN are currently looking for data scientists\\nHeritage Provider Network  the sponsor of the Heritage Health Prize  is looking to hire data scientists to take its data and analytics department to the next level \\xa0 If you are interested in healthcare join the largest physicians group in California\\r\\n and one of the largest in the United States and use your data mining skills to make a difference in the provision of health care to individuals throughout Southern California \\nIf interested please send an email indicating your interest to \\r\\ndatascientistheritagemed com \\nAnthony Provisional milestone prize winners will receive an email over the weekend  An announcement will be made at Strataconf on September \\n  Correct  libraryrandomForestsetwdCUsersantgoldbloomDropboxKaggleCompetitionsCredit Scoringtraining  read csvcstraining csvRF  randomForesttrainingctrainingSeriousDlqinyrs                   sampsizecdo traceTRUEimportanceTRUEntreeforestTRUEtest  read csvcstest csvpred  data framepredictRFtestcnamespred  SeriousDlqinyrswrite csvpredfilesampleEntry csv Alec setting the random seed is a good idea \\nDomcastro your hypothesis is correct \\xa0 Youre correct  Shouldnt include headers   Congratulations team Market Makers and Willem Great coverage in the Wall Street Journal here\\r\\n  For those interested heres the footage from the award ceremony\\r\\n   Does being a member of HPN mean you usually referred to an innetwork provider of say lab testing unless obviosuly it is some specialty unavailable Yes  Can you be a member of HPN and have govt sponsored insurance eg Medicare MediCal Yes for\\r\\n Medicare  I can follow up on MediCal if you like Have passed these questions onto HPN  Will respond as soon as I get an answer \\xa0 On October  the judges in their sole discretion decide whether or not the documentation is sufficient taking account of the comments made on this forum \\xa0If they decide the documentation is not sufficient they can impel the winners to address their\\r\\n concerns in the seven days following October   If the winners are asked to resubmit participants have another  days from November  to raise any additional complaints \\xa0\\nThe judging panel\\xa0are experienced academic reviewers\\n \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 Hi all\\nWe are in the process of liaising with the judges  Well report their decision as soon as we have everybodys feedback  We have made a slight change to the\\xa0Terms and Conditions\\xa0adding  \\n\\nNo individual or entity may share solutions or code for any competition or collaborate in any way with any other individual or entity that is participating as a separate individual or entity for the same competition  The foregoing shall not apply to any\\r\\n public communications such as forum participation or blog posts \\n\\r\\nWe are also aware that the rules havent been as clear as we might have liked  From now on before you download the data for any new competition you will be reminded that\\n\\nyou cannot sign up to Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and\\nprivately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums \\n\\nWeve reached out to several teams about this issue  Please let us know ASAP if you have multiple accounts and weve not reached out to you  \\nWe are aware that the rules havent been as clear as we might have liked  Please be reminded that\\n\\nyou cannot sign up to Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and\\r\\nprivately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums \\r\\n\\nWeve reached out to several teams about this issue  Please let us know ASAP if you have multiple accounts and weve not reached out to you \\n Sounds like theres a thriving community in Melb which looks to have been the strongest performing city  Congrats all One of my coworkers\\xa0said were really doing well if you think of Kaggle every time you see the Facebook logo  \\xa0 Nice \\nFor interest we typically see strong metrics on Kaggle during holidays because people have more discretionary time  which at least suggests our community isnt too busy with family  Another possible explanation is that people have exhausted their travel budget both time and money on holiday travel and need to wait a little while before booking more travel   Clear and entertaining  Nice work Why does lower bound get mentioned so much more than upper bound Ive played with PCA before but never association plots or MCA  Glad to see an example usage and be able to add these to my toolkit  Thank you\\nFor the association plots I assume the width of the box refers to the number of Tweets referring that that airline\\nI assume   is the proportion of comovement explained by the first dimension  Is that correct Is it typical for the first dimension to explain so much of the comovement Any thoughts on how to interpret this dimension \\nSmall nit You might want to change res to reason  I initially assumed res stood for residual  And reduce the font size for the x axis label on plot   Thats the most interesting plot to me but its hard to read the labels because they overlap   This is a nice notebook  \\nSuggestions\\n\\nTo make this easier to follow for those who havent yet looked at the data itd be great if you added a section showing a few rows  Or possibly even a few exploratory chartshistograms  Perhaps after the Loading the data section  \\nItd also be nice to see the before and after you preprocess the data ie before and after the Using textmining to format our data section  \\nRename the Using textmining to format our data to something like Cleaning the data \\n Great I always look at the top rated notebooks before looking at the data because the notebooks usually give me a sense for whats in the data and what I could do with it   Love it Interesting that for everyone other than Woodrow Wilson the names popularity monotonically declines over the course of the presidency  \\nDwight looks like it increases in popularity during WW which makes sense  One suggestion is to add years to the x axis label for each chart to make things like this easier to spot  I Tweeted this script and somebody replied asking \\nIs there a corresponding drop in the name frequency of the losing presidential candidate right after the election   I was thinking another interesting extension would be to answer the question Whats most influential in determining baby name trends out of  \\n\\nPresidents and first ladies   \\nMusician that was  for longest on the billboard charts in a given year  \\nBest actoractress in the Oscars  \\nBasketball football baseball MVPs  \\nNobel prize winner names  \\nTime person of the year\\n\\nIf nobody else tackles this I might try it  \\nThis builds off a conversation I had with my coworker Meghan who said itd be interesting to see whether Presidents or royal babies had a bigger impact on baby names  From this page\\n  This is great  \\nIm surprised North America is not higher for sugar  The sweetness of food was one of the first things I noticed when we moved to the US from Australia  Although it could be because a lot of the sweetness comes from high fructose corn syrup which is not captured   Nicely done and fun writing style  \\nOne additional conclusion is that real data is messy   Big Data Borat captures it best \\nIn Data Science  of time spent prepare data  of time spent complain about need for prepare data  \\n  Interesting how noisy the very early years are  I suspect the s data is very poor quality   Really nice script  \\nInteresting to see the temperature uncertainty chart  Gives a nice visual of when the data starts becoming more reliable  \\nAlso nice idea to put dt into a variable importance plot to see its relative important  Obviously would have been more interesting if wed provided more data  \\nOne suggestion is to better label your plots  Theres some good stuff here but it stakes a while to figure out what each chart is showing I actually looked at your code to figure it out  I suspect this script will be more popular with some labels that make it easier to follow   Sven have you been able to figure out an interpretation of this chart Thanks  Itd be helpful if you labeled the charts and possibly added sub label pointing to your interpretation   It may not be useful for the reasons you mention but it looks nice   Would be cool to see the by city version  I assume you didnt use it initially because of the size of the data set BTW I assume red  hot Would be helpful to have a key   Its awesome Really nicely put together  Bluefool I thought you came out really well   Is this a work in progress Or is there an error The charts are showing up blank for me  Juanchaco this is neat but itd be easier to follow if you added a description between charts  At the moment Im scanning the codecomments to try and figure what each chart is showing   Cool  As someone who lives in San Francisco Im curious   Akshay this script would be more interesting if you found a neat way to visualize temperature by country   Just a heads up that were still working through the winners solutions  Will need more time to before announcing the final results as official   Quick update We will announce the official results on Wednesday March  at am ET   Julian responded in the other thread \\n  Hi all\\nThe results on the final leaderboard are now official  Congratulations to the winners and all involved  This is among the hardest and most ambitious competitions weve hosted  We couldnt be prouder of the results  \\nThe competition has received some press coverage with a chance of more to come\\n \\n \\nAnthony This is the photo from the Kaggle office  This lunch was one of the highlights of my six years of Kaggle  Not something I will forget in a hurry   Minor comment theres a typo in the title Prelimnary should be Preliminary   I only make this comment because its a nice script and I dont want grammar sticklers to be put off the typo   Jeff interesting  Seems like makes the system a little conservative in the handling of new players   Change made  Thanks for the feedback   Cool to get real world SKU data but how would I create a recommendation engine with just SKUs and ie without customer data I guess Ill see when you upload your code   Thanks Allen Implemented most of these changes   Allen Jeff or anyone else how do people typically prevent ratings from becoming stale with Trueskill for example nonactive racers maintaining high rankings  I am planning to make the rankings apply over a  month rolling window but am curious if there were other approaches such as the inclusion of some kind of time decay    This version should now be correct  The model does not systematically make money but its only using very basic features barrier weight and rider  Hopefully its a good start and somebody can take it and build on it   Hi all\\nJust a follow up on the request to not share solutions as mentioned above this is not a legal obligation but rather a request from the host  We try and avoid requests like this because it limits the learning that comes out of a competition  \\nOur takeaway from this thread is that if there is a confidentiality request we will flag it up front to avoid an unwelcome surprise at the end of a competition  \\nAnthony Rakhlin one thing were experimenting with is asking hosts to write blog posts summarizing the outcomes from a competition  This wont be timely because itll happen after theyve spoken to the winners and digested the results  Unclear what reception we will get from hosts but its something were testing out   Foxtrot did you find a bug or a mistake in my code Or is it that the returns just dont look right given how simple the features are Hi Foxtrot  I commented out that line because I changed the problem to predict the probability of victory for each horse rather than  position  \\nxgb XGBClassifierobjectivebinarylogistic fitdftrain dropdftrainwinpositionmarketidaxis\\nSo I thats not the source of leakage  Im not actually certain there is leakage  If you run the code multiple times the return switches from being positive to being negative depending on the traintest split  Having run it a bunch of times I suspect the expected return is actually negative   Right  But that doesnt happen in this case because Im predicting probabilities  So the predictions are          Foxtrot nice pickup  thanks  Uncommented the shuffle the deck line  \\nI suspect theres more leakage in this analysis because the trainingtest split is random and not timebased  The way to get around this for this data set is to train on the form table and test against the runners table  Or even more useful would be to get less anonymized data if Luke or someone else has it   At the every least race dates are valuable make it possible to protect against leakage  \\nBut more generally getting access to the complete raw data is valuable  Everything one does to disguise the data destroys information and limits what a data scientist can do with it  Some small examples related to knowing the venue\\n  might give information on what surface the horses are running on\\n  a km race at one racecourse could be a straight whereas at other courses it could involve a turn \\nThis might impact the performance of specific horses and cant be accounted for when the variable is venueid  \\nFinally for boosting engagement with the data set anonymized data is less fun to play with than richer raw data  I reversed Moody Bluefools downvote \\nBy upvoting  I dont have database write privileges    Chris I built a model to project out my Sep  and Oct  results  Shame theres no Oct  race  According to my model Id finish first Queue XKCD   \\n William nice Kernel  Looks like your predicting author based on the number of comments subject etc  Whats the thinking behind predicting who the author is I was thinking it might be interesting the predict the number of comments a proxy for how interesting the article is\\nAlso I was looking at your error chart and your comment that It seems like we are   better than chance   Curious how youre measuring that I tried to read it off the chart but couldnt see where it came from   William fyi   That was quick Your code is much cleaner  Puts pressure on me to go back and clean up mine   This was really just a quick analysis  If I had more time Id have actually looked at the words being used in the posts rather than guessingrelying on memory  E g  I probably shouldve included RNN  I could also have included package names Tensorflow Keras etc  In fact another version of this might look at the packages that are most commonly used by winners   Ps  Thanks for the bug report on the file download  Well look into it  I was wondering why the data set had  downloads    Jordan looks great Now that the data is in CSV format I featured it  \\nWriting a first exploratory Kernel to show people whats in the data is typically helpful for driving driving engagement from the community   Sounds good  I was thinking itd be fun to apply Trueskill to this dataset I used it here  Id like to do it but probably wont have time over the next few weeks unfortunately   Marginal Revolution featured Will Novaks analysis of their posts  If there are interesting findings made on Techcrunch data Ill send it across to Techcrunch  Theytheirreaders may be similarly interested   David I have been able to create the RDS file\\nlibraryggmap\\nmapdata  getopenstreetmapbbox  c        colorbw scale  round\\nsaveRDSmapdatafilestpete rds\\n\\nHow do I create the text file \\nAlso why arent you using OSM files the Open Street Map XML file format rather than using GGMap to export to txt or RDS \\nIve spent a while trying to import OSM files for plotting in Python but with no success  Wondering if you also found this difficult  \\nps  This is for my GPS watch data so the coordinates above are not the Chicago coordinates  \\n  Anokas thanks My bearing calculation is incorrect  My next step with this notebook is to debug it  \\nHow would I use\\nnp raddegnp arctan\\n Anokas I figured out my problem  I was feeding latitude and longitude into the calculate bearing function in the wrong order feeding in lat as long and vice versa  \\nAdded your more elegant bearing calculation formula as well  Thanks  Ryan appreciate you raising the concern  I want to share Kaggles perspective \\nFirst off internally we are incredibly excited about the launch of codeonly competitions  Its the biggest change weve made to competitions since we launched in   It increases the range of competitions we can run including time series competitions such as this one reinforcement learning competitions and competitions on larger data sets \\nOn this competition specifically this is the first step in what we hope will turn into a bigger relationship between Kaggle Two Sigma and the community  This first competition is aimed at testing out the concept of codeonly competitions gauging the communitys interest in financethemed competitions and getting a sense for the types of signals the community finds on a typical financialmarketsrelated data set  \\nrakhlin regarding your comments about anonymization and the use of an API these were primarily driven by Kaggle  The anonymization is in place to discourage people from looking up the answers  The API allows this to be a proper time series competition  We try to make our competitions fair this means making it difficult for the rare participant who is inclined to circumvent the rules \\nUltimately our goal is to give our community as many opportunities as we can  This means a mix of commercial competitions research competitions playground competitions getting started competitions and most recently open data sets  Ultimately the community will select what interests them  And Kaggle will continue to offer the things that resonate \\nAnthony Did you try the my submissions tab on the left hand side dashboard You could also look at some of the weather data and Kernels that others have shared \\n  Hi Jackie  Perhaps this dataset is a good starting point   Totally agree with frustrations expressed here  Neither Kaggle DHS nor the community wants geo restrictions \\nIn this case its a legal restriction  DHS worked hard to allow any international participation albeit without prize money \\nFrom Kaggles perspective we prefer to host a competition with a geo restriction and make the opportunity available to the community than not host  We think this is a better albeit imperfect outcome  I understand and agree with frustrations around nonUS citizenspermanent residents being ineligible for prize money  None of Kaggle DHS or the community wants this restriction \\nThe reason for the restriction is the America COMPETES Act  DHSs legal team worked hard to find a way to allow international participation at all albeit without prize money while not violating the act \\nFrom Kaggles perspective we had a choice host this competition with a geo restriction or not host at all  This was a tough choice  Hosting the competition meant running a twospeed competition where some are eligible for prize money while others are not  Doing this violates our desire for meritocracy where we offer equal opportunities to every community member regardless of education background and of course country \\nIf we dont host then we dont expose our community to one of the most interesting and valuable datasets thats ever been hosted on Kaggle  As weve seen over the past five years with the rise of deep neural networks the availability of datasets allows us to push forward to science of machine learning  Not exposing this dataset to our community meant depriving our community and the machine learning world more generally of a chance to push forward the science with a novel and challenging dataset \\nWe decided that it was better to make the dataset available  Many in this forum have said this was a mistake  This was a challenging issue I hope this at least gives you a window into our thinking \\nAnthony kitefoil   James those speeds are a actually little slow  I will upload my latest data soon but thats  knots upwind and  knots downwind Ive improved with training  And there are kitefoilers who are much quicker than me    RounakBanik nice dataset Can it be updated to be current If you can make it current and update this notebook I will send it to the TED team  \\nWe did this with Marginal Revolution and it ended up getting featured on their blog\\n \\nWho knows maybe the TED team will be really interested and can highlight it in some way   Rounak apologies  I missed that youd updated this  I will send to TED  Fingers crossed they will pick it up Kamil and HamaChi I actually dont think HamaChis expectations are too high  Having demanding users is healthy  It pushes us to improve our products \\nWere aiming to make Kaggle Kernels into the leading cloudbased workbench for data science not just a free compute environment  Were migrating onto GCP and making some architecture changes that we expect will make a big difference \\nStay tuned \\nAnthony We have spent considerable time discussing the situation internally here at Kaggle and with Zillow  We thought Kaggle was putting sufficient emphasis on Zillow’s eligibility criteria by posting it on the competition overview page  Its clear from this thread that this nonstandard eligibility rule was not sufficiently publicized to the community \\nAs an acknowledgement of this we are making the following adjustments\\n\\nWe are reinstating the affected teams so they will be eligible to receive points and prize money\\nWe are adding supplementary prizes if needed  These prizes aim to avoid penalizing teams that would have been in the top three teams if we didn’t reinstate teams who aren’t in compliance with the eligibility criteria  For example a team that’s in fourth place on the final leaderboard will be awarded nd place prize money if the nd and rd place teams don’t meet the eligibility criteria \\n\\nNote the eligibility criteria for round two remains unchanged pasted below for convenience  We will soon be reaching out to everyone who places in the top  teams to verify their employment or institutional affiliation to ensure compliance with the official competition rules  Anybody who works for a company that is referenced in the eligibility criteria will not be accepted into round two  If you were on a team with one or more members who are ineligible you will still be accepted to move forward into round two but the teammates who doesn’t qualify will not  \\nIn the future we will make a bigger effort to make nonstandard rules clearer posting them as a pinned forum post for example and inviting questions and clarifications  But please also remember that the rules are important  Ultimately it is each Kagglers responsibility to read the rules before choosing to participate  Nobody wants a situation where Kagglers are putting considerable effort into a competition that they are ineligible to participate in  \\nAnthony\\nEligibility Criteria\\nMembers of the following entities are not eligible to participate in either round of the Zillow Prize Contest  any commercial entity that engages in the sale valuation or analytics of residential or commercial real estate  any entity that offers services in the leasing and property management space including vacation rentals and  any entity that monetizes residential real estate related data  Officers directors employees and advisory board members and their immediate families and members of the same household of Sponsor Kaggle and each of Sponsor’s and Kaggle’s respective affiliates subsidiaries agents judges and advertising and promotion agencies  In addition you are not eligible to participate in the Zillow Prize Contest if you are a a resident of a country designated as an embargoed country by the United States Treasury’s Office of Foreign Assets Control see   for additional information or b are an individual that appears on the United States Treasury’s Office of Foreign Assets Control Specially Designated Nationals and Blocked Persons List see   for additional information  Bojan totally agree that we dont want to incentive Kagglers to hide personal information  In this case nobody will be removed from the round one leaderboard because of personal information they have shared  And everybody will have to volunteer their affiliations to be eligible for round two  That removes any discrimination based on publicly shared user information or Wired Magazine profiles   \\nCompetitions with eligibility criteria are quite rare so we havent figured out all the nuances  Cant promise we wont make mistakes in the future but we aim to keep improving   We spoke about that option in connection with the TSA competition  Its not clear that it was legal in that case  \\nThis question is moving into a more general discussion of eligibility criteria  If we want to move the conversation to a more general discussion of eligibility criteria I suggest we move it to general   The eligibility criteria still applies to entry to round   So only teams that meet the eligibility criteria will be accepted into round    Have you tried Trueskill before It should be more powerful because from memory it doesnt assume a fixed standard deviation so it takes into account the certainty about a rating when adjusting ratings after a game  Itd be interesting to compare the performance of Elo vs Trueskill  Theres a nice Python implementation of Trueskill   Love this story  Tenacity counts for a lot  Well done Ryan On Monday Numerai announced that they were giving away their cryptocurrency to Kaggle users with a rating above Novice and accounts created before March   This wasnt done in partnership with Kaggle  we had no idea it was coming  \\nFollowing that announcement there has been a massive increase in the number of login attempts to the Kaggle website  These are attempts to break into Kaggle accounts in order to claim the cyptocurrency airdrop  This is both stressing our systems and putting Kaggle accounts at risk \\nAs a result we have\\n\\nremoved the ability add   as your website URL\\nforced a password reset for anybody whose website was set to  \\nsent an email to the Numerai CEO letting him know that this has happened\\n\\nIf you find that your password has been reset please go through the Forgot Password flow \\nAnthony  Im always happy when the Kaggle credential gives our community opportunities that they wouldnt otherwise have   In this case the way offer was structured effectively created a bounty for hacking Kaggle accounts  And the fact that we had no notice meant that we couldnt think through the implications and prepare   \\nwere going to look at changes to the Kaggle login flow this week  \\nNumerai has suspended the Kaggle Airdrop  \\n nathanforyou nice feature  \\nNote These community guidelines are replaced by revised guidelines available here  \\n\\nThe Kaggle community has a lot of diversity with members from over  countries and skill levels ranging from those learning Python through to the researchers who created deep neural networks  We have had competition winners with backgrounds ranging from computer science to English literature  However all our users share a common thread you love working with data \\nAs our community grows we want to make sure that Kaggle continues to be welcoming  To that end we are introducing guidelines to ensure everyone has the same expectations about discourse in the forums\\n\\nBe patient  Be friendly   \\nDiscuss ideas dont make it personal   \\nThreats of any kind are unacceptable    \\nLowlevel harassment is still harassment   \\n\\nThe Kaggle team determines whether content is appropriate  If you see something that violates these guidelines you can bring it to our attention using the flag option on messages and topics  If you have a serious concern you should report it to supportkaggle com  All reports will be kept confidential   \\nSad to share that Kaggle Master Vlado Tomecek passed away on Sunday October   \\nI received the note attached above from Dana Vlados sister  \\nI remember sitting in on his call with Draper Labs the competition that he won  His solution involved a tremendous tenacity and creativity   In our exchange Dana mentioned that data science was Vlados passion and main focus   On behalf of the Kaggle team we wish Vlados family well  And we will miss his presence in the community   justinminsk this is nice FWIW I suggest updating the title to something like Identifying the best college wine  The kernel is more fun than the title suggests    Faraz has this been address If not send me a private message with a link to the comment and Ill make sure we take a look ASAP   I’m a longtime competition lurker whos finally mustered the courage to join a competition 😅  I picked this competition because I find the topic interesting if I was going to college today I’d probably pick biology  And also as a chance to try out Google AutoML Vision  \\nKaggle is part of Google now which gives me exposure to some of the technologies that are generating excitement inside Google  AutoML is a tool with a lot of buzz  \\nI hadn’t paid much attention to the Automated Machine Learning Tools AMLTs until the KaggleDays competition in San Francisco this past April  Two AMLTs Google AutoML and H ended up participating in the hackathon and getting top  performances here’s the Google AI writeup of their performance  While I believe it requires human creativity to do problem setup and domainspecific feature engineering KaggleDays left me wondering whether humans need to be doing generic feature engineering architecture selection picking activation functions and setting learning rates  \\nI plan to share details of my experiments and experiences with Google AutoML Vision in this thread  I’m also open to suggestions from others in the community for things I should try  But bear with me if I take sometime to respond like many other Kagglers I have a busy day job  First Few Experiments\\nI started with the images created by xhlulu’s kernel thank you xhlulu  In that kernel xhlulu converts the data to x RGB jpegs  \\nBy default Google AutoML randomly splits between TRAIN VALIDATION and TEST  And the predicted outputs labels and confidence scores for each image I define a threshold and it outputs all labels and confidences above that confidence threshold  \\nI made predictions for both sites and then picked the label that had a higher confidence score across both sites  Below is a table of my scores  Note AUC is the accuracy metric that Google AutoML Vision reports  Haven’t put the time into figuring out what it means but I assume it’s probably treating each class as binary onoff and then aggregating up \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Public Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n    \\n   \\n    \\n   \\n  \\n\\n\\nI then made small modifications xhlulu’s code to create x RGB pngs to test the impact of higher resolution images \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n\\n\\n  is my best score  It’s a result of a poorly thought out experiment so I’m surprised it’s my best score  I trained a model for  hours that included the control images  But after inspecting the testset predictions I realized my model was often picking labels   which are control labels that don’t appear in the test set  I ended up filtering the testset predictions to go to the next most confident label when a label  was predicted  \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n    \\n   \\n    \\n   \\n  \\n\\n\\nGoogle AutoML Vision allows me to define my own TRAIN VALIDATION and TEST splits  For my next experiments I’m going to start using this feature  I’m going to define my own splits to\\n  Include the control data in TRAIN but not VALIDATION or TEST to allow the model to use the control data for training but to discourage the model from predicting labels  on the competition test set\\n  Split by experiment rather than randomly  Experience with Google AutoML Vision\\nI’m getting pretty decent results considering how naive my models are  I’ve found Google AutoML easy in many ways  To get a basic model all I need to do is\\n   upload the RGB images to Google Cloud Storage\\n   upload a CSV file with a pointer to the GCS bucket and the target label\\nThere are a bunch of limitations with the products  My biggest issues so far have been\\n  inability to add other metadata e g  would like to be able to add metadata on controls plate id and position on plate  I can probably get around this using a postprocessing step but it’d be nice to be able to add this metadata into the single model \\n  there isn’t a feature that allows batch prediction on a large number of images  I work around this by hitting the prediction API  times to generate my submission file  \\n  the model evaluation page is not very helpful for debugging model performance  \\n  I had to use a lot of hours of training to get good results you can see that the models keep improving with additional training time  This means I have to wait a long time and spend a lot of money \\n  can’t change the loss function  Based on the forums it seems like the loss function might be an important setting for this competition  \\nThere are some more minor frustrations that I had to workaround which I’m happy to share if others are planning to try it out and want to learn from some of the frictions I encountered  lkhphuc unlikely you need anywhere near as many training hours if youre not using AutoML  The upside of AutoML is that you dont have to set any hyperparameters and still get a decent model  One of the drawbacks is its so compute intensive   xhlulu I actually regret converting to png  It would have been a purer comparison if Id also used jpeg  \\nI tried running the px conversion in a kernel but I didnt have quite enough compute time  Let me find out how I can share the px dataset with everyone   xhlulu unfortunately cant share the dataset through the datasets platform  Thatd allow users to access the dataset without having to accept the competitions rules   apap Im picking the site that has the highest confidence score across the two sites  For my best model LB   my predictions for site  and site  agree  of the time   Not sure  Sorry   zaharch sorry for the delay  I wanted to see if AutoML could be run out of Kernels  Turns out it can\\n \\nThe  hour limit for kernels is not an issue because my kernel kicks off a  hour training job and then stops  \\nMy next step is to share my inference code which sends the test images to the trained model and generates predicted labels  \\nAutoML doesnt give architecture or hyper parameter recommendations  It just creates an end point that you can send images to and get back predicted labels and the models confidence in that label    ttylacm dont know what hardware Google AutoML is using under the hood  Its invisible to the user  Good chance its using TPUs though    ratthachat thanks Looks like youre a few places ahead of me on the leaderboard  Watch out I have a few new ideas to try 😏  Oops  I made a change that introduced an error in version   Version  works  Im currently committing a version  which should work as well    OK now also added the code for doing inference using Google AutoML\\n \\nI didnt do my strongest model to avoid messing with the leaderboard I picked a model that performs a little below to strongest performing kernel  To change models all one needs to do is change the modelid     ankitkp giuliasavorgnan and jiangkun improving reliability of kernels is a big push for the team  I shared this thread  \\nOne question that came up are you all using GPUs Or just CPUs jiangkun I dont follow  What does that mean Managed to jump to   by exploiting the structure of the structure of the data mentioned here   zaharch Im really interested in seeing this as well  I am curious to try TPUs sometime and would be interested in summary of your experience  Also curious if you were able to realize a performance speedup when using TPUs   Nice Thanks for sharing   Really love this writeup  Particularly how you step through the different things you tried and how they mapped to improvements in your score  Very easy to follow   deep there is a mistake in the dataset  San Francisco is listed as  square miles  When its actually  square miles  \\nAlso itd be helpful to have a description of the data including where it came from  And to have the Area column as an integer or float rather than a strong   Im going to add a pointer to the recovered cases dataset in the pinned dataset thread     sasrdw this dataset seems to have a decent amount of what youre looking for\\n \\n\\nThat version is three years old so may be worth updating  Although at a glance many of the measures are updated pretty infrequently   This looks like a nice version of that dataset\\n \\nAdded it to the data sharing thread\\n  davidbnn posted a dataset of global weather conditions at\\n  I wonder if Google Trends data might be interesting Perhaps searches for hand washing or hand sanitizer in a particular city might correspond with a lower transmission rate  Or even just searches for COVID in a particular city might correspond with a lower transmission rate indicate people are taking the virus more seriously in that city  hannesmarais Im not an epidemiologist but at a glance this looks really impressive  \\nI had a quick look and saw a few places where your system seems to be answering the prompt \\nThis seems to give an answer to the persistence on surface question\\n\\nThis seems to give an answer to the immune response question\\n\\nAre there many places where you think your system is accurately answering the prompt Are you in a position to give a bit more background on what your system is doing savannareid I would also be really interested  At a glance and to my untrained eye hannesmarais system looks really impressive  jaimeblasco uploaded one for school closures   I assume country level data is still likely useful\\nIf you can match the files in that dataset I can likely update it on their behalf  Otherwise I suggest you just create a new version   Nice Mind also sharing it in this thread Nice to have all public datasets in one thread  As I understand it this has been an influential preprint showing the relationship between temperature and humidity and transmission rate  \\nIt looks at  Chinese cities and finds that a  degree celsius increase in temperature leads to a   drop in R  And a  increase in relative humidity leads to a   drop in R \\nAs I understand it R is a a commonly used measure by epidemiologists known as the effective reproductive number and is the average number of secondary cases per infectious case  \\nIf R is greater than  then an epidemic will spread  If its less than  it will die out  My understanding is that the estimate of R for COVID is in the range of    cpmpml nice dataset Do you mind adding a pointer here\\n \\nWant to have all external datasets on the same thread  The Economist did an analysis of tourism flows tofrom China a few weeks ago  Could look at the dataset they used as a starting point  absolutely Agree completely with Pauls guidance  \\nBig goal is to product findings easy for the medical and health policy communities to digest   This is a nice notebook  Itd be helpful if you also linked to the paper as well as providing the excerpt   ajrwhite this is very useful  I find key phrases most useful part of your notebook  Few requests\\n\\nCan you print more key than  key phrases when your search returns more results e g  chronic respiratory diseases \\ncan you print key phrases for additional  you look at risk factors e g  hypertension  \\nCan you use your approach to look at words like weather temperature and humidity That is shown to have an impact on transmission rate e g  referenced here  Itd be interesting to see what your approach turns up   \\n Getting estimates for R and R from the literature would be great  One problem with those snippets is it doesnt have the actual numbers   We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the Kaggle community  The research on COVID is moving quickly making it hard for virologists and the health policy community to stay on top of the latest  They are working long days and need information presented in as clear and concise a format as possible  We’ve formatted the summary page to be digestible to them  \\nThe page is currently organized into three sections findings tools and datasets  Im going to focus on the findings and tools sections since theyre most relevant to this challenge  \\nFindings map to the ten tasks issued by the White House Office Of Technology Policy addressing open questions about COVID  Our goal is to help answer as many open questions as possible and represent the state of knowledge on each of those questions  \\nFindings should be focused concise extract quotes and numbers out of papers and also provide a link to the underlying source  It’s helpful if you can produce findings that map as close as possible to the format of whats currently presented on the summary page  Some of the most impactful work so far have involved simple methods like string matching and regular expressions  \\nFor those working on tools Google Scholar and AI Semantic Search are already mature products  If you are building a tool make sure your tool adds value beyond those products  \\nIf you have contributions that you think we should be highlighting on the summary page please email me at akaggle com or share on this thread \\nAlso feel free to ask any clarifying questions on this thread    Most of the datasets have come from the forecasting challenges  However for those interested Ill provide some guidance on what datasets are helpful \\nIts useful to share individual datasets with promising signals  Its even more useful if you can join to other relevant datasets that virologists and the health policy community can analyze  And by all means use any datasets to try and produce findings  If you do make useful findings please document them in a clearly written notebook thats easy for others to follow   We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the Kaggle community  Virologists and the health policy community are working long days and need information in as clear and concise a format as possible  We’ve formatted this page to be as easily digestible to them as possible \\nThe page is currently organized into three sections findings tools and datasets  Im going to focus on datasets and findings since they are most relevant to this challenge  \\nDatasets So far the forecasting challenge has done a nice job of surfacing potentially useful datasets  The most valuable contributions have been joining those datasets to create valuable resources for testing which factors impact transmission  Shout out to davidbnn who joined each region in the Johns Hopkins University data to the nearest weather station  \\nFindings These  clearly written notebooks that help show which factors have an impact on transmission rate  \\nCalls to action\\n  create enriched datasets that would allow researchers to easily test which factors impact transmission  These datasets should be well documented and kept updated  \\n  write simple notebooks that show the impact of factors on transmission useful whether or not you find a correlation\\nIf you have contributions that you think we should be highlighting on the summary page please email me at akaggle com   somethingkag this is not a template but ajrwhites  notebook does a really nice job addressing a handful of the subtasks  \\nIt directly addresses the subtasks  The informationdata is very clearly presented   skylord nice  Are you updating this daily If so Ill include it   Franck you should absolutely be open to looking at external data sources to answer the questions  One nice thing about starting with the papers is we build up a picture for what has already been studied and where the gaps are   No  This is not a supervised machine learning competition  We are hosting a supervised COVID machine learning competition at\\n  ajrwhite  really nicely said  Ken added to summary page   Thanks nofoosports  At a glance your notebook looks really nice  Likely have some folk with medicalpublic health backgrounds starting to help out with curation from tomorrow so planning to take a close look with them  Ken thanks for grouping the notebooks by task  Makes them much easier for us to process   Thanks Mike  Will take a look today   remanuele please add as a Kaggle dataset  That makes it easily accessible to those writing notebooks   For those interest Safegraph have a really interesting human movement dataset  Its location data from millions of anonymized smart phones  Its currently on AWS but they can move it to GCP to make it easier to use from a Kaggle notebook if theres sufficient interest \\nYou can get access by filling out this form  I received quite a bit of feedback from the healthcare and health policy communities about our page summarizing community contributions \\nAs well as including the title of the paper itd be helpful if we also shared the name of the journal in the summary tables  The name of the journal is a proxy for the the quality of the paper a result published in JAMA or The Lancet carries more weight \\nMore challenging is to also classify the type of evidence that a study is based on  Theres a hierarchy of evidence and its helpful to know what evidence was used to draw a particular conclusion  Here are a few references that explain the hierarchy of evidence\\n   \\n      ajrwhite can you update this notebook to include the new data dump\\nAlso is it easy for you to add the journal name to the article title column per this guidance If so itd be great if you could put include it  \\nIf you can Id format itd be great if you could put it outside the link to make it easier to distinguish from the title\\nPersistence of coronaviruses on inanimate surfaces and their inactivation with biocidal agents The Journal of Hospital Infection mikehoney a really nice tool for exploring the data but doesnt directly give me the answers to specific tasks without me choosing my own filters   mlconsult the Chloroquine section had mostly relevant articles so added it to the summary page under Effectiveness of drugs being developed and tried to treat COVID patients  skylord added here \\nBTW you may want to crosspost to this challenge This seems to be an important paper  Is it in the CORD dataset zohrarezgui nice idea to pull down clinical trials data  But the dataset has no information on the therapy being trialed though  Is it possible to pull more information about the nature of trial mlconsult nice added  \\nCan you pull how large the difference is between  and \\nAlso what does relscore mean nofoosports this is great Ive started adding your answers to the contributions page \\nA couple of things that would be helpful\\n\\nstart ordering by date I like to present results chronologically on the contributions page\\nalso helpful if you can start breaking out answers to subtasks a little more\\n\\nFor example for this subtask theres\\nRange of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery \\nitd be helpful to have papers and excerpts that just answer \\n Range of incubation periods for the disease in humans \\n how this varies across age and health status\\n how long individuals are contagious\\nYoull notice Im starting to break up even the subtasks on the contributions page to make them more digestible  This is feedback were consistently getting our audience is very busy and want things as scannable as possible  Another request its even helpful if you can break out subtasks into several components  As an example take this subtask Range of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery \\nitd be helpful to have papers and excerpts that just answer\\n Range of incubation periods for the disease in humans\\n how this varies across age and health status\\n how long individuals are contagious\\nIm starting to break up even the subtasks on the contributions page to make them more digestible  The feedback were consistently getting is our audience is very busy and want things as scannable as possible \\nI just shared this feedback on nofoosportss excellent notebook but wanted to make it more broadly visible  Also a nit but you have grouped two subtasks accidentally\\nPrevalence of asymptomatic shedding and transmission e g  particularly children and Seasonality of transmission  csheesun were typically finding results easier to read when contributors are submitting one notebook per task   We had another call today and received another round of feedback  First off were getting good feedback that   is heading in a really helpful direction  \\nThe big issue is still that theres limited signal on how strong the underlying paper is  Since COVID is so new and most articles are in preprint journals medRxiv and bioRxiv that doesnt give much signal I think this is consistent with what savannareid has been saying  \\nThey are interested in whether we can mine other signals from the dataset  I think were going to get more feedback in the next week Ill provide it when I get it but an example that came up was pulling out the number of participants in a study the more participants the more weight should be given to a study for eg  \\nThis didnt come up on the call but I was also wondering about authors previous publication record might be a useful signal   \\nWould love to add a column thats a proxy for publication quality by the end of the week if anyone can crack the code on that  \\ndavidmezzetti started a discussion thread on the topic so suggest the conversation continue there   Another bit of feedback we received is that the health policy and medical communities are interested in tools that might help them get to answers more quickly themselves  The dream tool is a questionanswering engine that can reliably answer freeform questions  But theres interest in seeing other tools as well that might go beyond what things like Google Scholar and other existing search engines can do  If anyone builds a tool that they think meets this criteria please either share it here or email me    Ultimately our goal is to be as useful as possible to health policy decision makers  So you are free to search Pubmed if you think itll improve your results   amazing youve made such quick progress  Look forward to tomorrows update   A recent paper suggests a potential link between transmission and air pollution\\n \\nPaper suggests that explains regional difference is Italy between transmission rate   mlconsult thanks for sharing  paultimothymooney owns looking at tools being developed  Hes going to take a look at this \\nIf he thinks its promising itll definitely need to be stood up as a web app  Feedback were getting is many of the end users are not technical enough to fork a notebook   Nice work mlconsult  paultimothymooney as an FYI this is now a web app   One of the main bits of feedback were getting about the results were presenting is that decision makers would like to better understand the strength of the evidence in a study  davidmezzetti has made a really nice start by including  level of evidence  \\nI spoke with Stelios Serghiou and Byron Wallace  Byron actually uses NLP to assess the quality of clinical trials  \\nIn speaking to Stelios and a few others Im getting the feedback that pulling out the underlying study design might be even more helpful than level of evidence  For each study type there are measurements that can help proxy the strength of the evidence  Below is a table that attempts to map study type to measurements  These are the things that Stelios looks for when hes assessing the quality of a paper  \\nItd be great if you can attempt to extract study type and the corresponding measurements to indicate evidence strength  This applies to those presenting task answers in tables as well as those who are building searchquestion answering tools   \\n Level of Evidence  Study Type                                   Measurements to indicate evidence strength                                                                                                                                                                            \\n      \\n I                  Meta analysis       I heterogeneity and tau squared heterogeneity    number of studies   \\n II                 Experimental Studies Randomized               Randomization     Sample size     Loss of follow up     Length of follow up\\n III              Experimental Studies NonRandomized               Randomization     Sample size     Loss of follow up     Length of follow up \\n III              Observational studies  Prospective Cohort      Sample size    Loss of follow up  \\n III              Observational studies Retrospective Cohort     Sample size    Sampling method how was the sample captured\\n III              Observational studies  Case Control            Sample size    Selection bias for controls when controls come from a different population from cases \\n                    Observational Crosssectional                  Sample size    Sampling method how was the sample captured    Temporality did the exposure e g  COVID come before the outcome e g  hospitalization desired or after undesired \\n IV                 Observational Case series  case study      \\n                                                                                                                                                                                                      Note savannareid pointed out that this doesnt include modeldriven results like those used to estimate things like reproduction rate  For modeldriven results measurements of evidence strength are likely simple size some measure of model fit and some information on where the underlying data was drawn from   aravindmc nice work Weve listed it on the contributions page \\nAs Paul said were getting a lot of feedback about including measures of the strength of evidence  Shared some of the feedback we are hearing here  Itd be great if you could extract and include some of that metadata in your search results   arturkiulian absolutely  The overall objective is to product something useful to the healthcare and health policy communities  If those things help you produce more relevant results then please do use them   ajrwhite and davidmezzetti and others looking to contribute the the contributions page savannareid hand coded the design and sample size for all the risk factors you can see\\xa0what it looks like here  She also reformatted the tables  \\nCan you try and pull update your risk factors notebooks for the new data dump And can you try and pull out design and sample size\\nAlso if you can follow this table format we can dump it directly onto the page we now have a more automated process for generating the tables\\n\\nDateStudyStudy LinkJournalSevereFatalityDesignSample\\n Hi all  We now have a team of medical students helping to curate the results that go on the contributions page  Theyve been reviewing the results of notebooks that focus on the  transmission incubation and environmental stability task as a starting point  Each student owns curating a table for each of the  subtasks  \\nThe goal for these reviews is to\\n  to come up with a standard format for those subtasks just like savannareid has done for risk factors  This will give you a target format to produce your results in  \\n  give feedback to authors of notebooks that are reviewed on how to make your algorithms more relevantaccurateuseful\\nThe goal is to continue to populate the contributions page which is gaining a nice following attractive  K unique visitors in the last week up  from the week before  And when we have a meaningful amount of coverage and a solid process for keeping the page uptodate as the new papers come in with less manual curation than we are currently doing were going to aim to start publishing the updated literature review with a leading medical journal so that its more visible in the medical community\\nEncourage those of you who are interested in contributing notebooks in the format helpful for the contributions page to join the Slack channel with the team of medical students  Here is an invite link  Im going to attempt to pass on feedback Im hearing in this thread  But joining that slack channel gives you more direct and interactive way to hear from the experts who are reviewing your work  \\nFinally if you want to see the curation process in action and the notebooks that are being reviewed you can check out this Google Doc  Please let me know if you have a notebook you want reviewed for a particular subtask and I can add it to the queue the queue is maintained on the MASTER SHEET tab   Some consistent themes in the feedback so far\\n  often algorithms are pulling snippets from the abstract  When the key number finding or result is in the full text of the article with the summary in the abstract being too high level to be useful  \\n  often algorithms are pulling snippets that are a reference to work from another paper rather than work that is core to the paper being cited This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID  We have heard feedback that an AIpowered literature review is a powerful way to synthesize new research  I have started a new thread outlining how you can feed into an AIpowered literature review  \\nThat thread aims to be a clear and direct way you can make contributions that will have impact  I am unpinning this thread to direct conversation there    Nice update  One change that would be helpful is if you can break out Study Study Link and Journal into three separate columns per this guidance  That will result in less manual curation to get the results into the format for the contributions page  sasrdw thanks for starting this thread   david any sense for how the IMHE model would have performed in these competitions Curious where would it have placed   Wow  This is really nice  Interesting that the Kaggle community keeps getting stronger vs IHME  I guess there are three possible explanations\\n  the Kaggle models are getting better as more data comes in more empirical and less theoretical than IHME\\n  the Kaggle models are getting better because people have more time to work on them\\n  IHME are better at forecasting further out\\nSounds like you believe its  david Also really curious about week  is the trend continuing or are we plateauing vs IHME  Assume you left it off because we dont have enough data for week  yet   david funnily enough I was just looking at his dashboard\\n \\nI assume theyre all outputting data in a standard format if hes able to put them on the same dashboard  Is it easy for you to benchmark the performance of those other models as well \\nWere trying to figure out whether the top Kaggle models are likely adding value beyond the standard epidemiological models  Were considering launching a longer running competition and combining top performing Kaggle models into an IHMEstyle dashboard    For interest section   of this paper discusses the metrics used by the epidemiological modeling community   I did some more benchmarking \\nI compared the current leader for week   and  with IHME and LANL models across three loss functions MAE RMSE and RMSLE  Im only comparing the forecasts for fatalities for US states \\nIn week  and  the Kaggle leader clearly wins on RMSLE  But performs a lot less well on the other loss functions  \\n cpmpml regarding your comment about the Kaggle comp optimizing different things the same is probably true for IHME and LANL  \\nOn MAE and RMSE it looks like you basically get good scores by getting New York and maybe New Jersey correct they account for  of fatalities\\n\\nAnd getting New York right involves predicting a data revision  NYC revised their fatalities measure to include probable COVID deaths on April   This revision started included people who died but never received a COVID test but had COVIDlike symptoms \\nThis chart comes from inversions excellent notebook that shows charts for LANL IHME and two Kaggle ensembles on a state by state basis \\n cpmpml the basis for the claim that getting New York right is most important comes from the observation that New York accounts for  of the total absolute error for the kagglelead and IHME models so that state is driving most of the error  \\nI havent had a chance to take a close look at whats happening with your model  Your numbers are really impressive across the board    unfortunately I have to return to my day job now though this was a small weekend project that I did during my daughters nap  \\ndavid I bet if you asked IHME and LANL if the benchmarking is fair theyd also have complaints  as well optimized for a different loss function etc  Nonetheless I was curious to see what would happen if I benchmarked across a range of metrics that the models didnt necessarily optimize for  I was hoping it might tell me something about the robustness of results \\nTo explain why this setup LANL only does forecasting for US states so needed to restrict to this subset to include them  And LANL and IHME model updates dont perfectly correspond with our competition dates   Nice presentation  Thanks for sharing   Ive put together got a notebook that benchmarks the performance of professional epidemiological models and compares them with a strong Kaggle model \\nI was curious to keep track of how the Kaggle models compare with the professional  epidemiological models  For now Im only tracking one Kaggle model the previous weeks leader to avoid making a selection expost  So the week  model Im using is one of sasrdws and davids models  Its currently doing really well  Although interestingly its making very different forecasts from the professional epidemiological models \\nI have created a benchmark panel that includes actuals and the forecasts from different models at different points in time \\nHeres the notebook that generates the benchmark panel  \\nEncourage others interested to play with it either add one of your models or play with some Kaggle ensembles can download submission files from public notebooks  mrisdal you are dominating enough leaderboards  I need to pick up my game    mlconsult couldnt find a task that this addresses but included on the contributions page because its come up as an open question recently   Ha  Id not known about this  For others   Donovan weve looked into this and it turns out that a bug with our process meant that we hadnt received the past few weeks of queries  Weve found your email and you will receive a response shortly as will others who slipped through the cracks  Apologies\\r\\n to you and others who have not received a response as a result of this error  Thanks for the nice wishes  Of course Kaggle wouldnt exist without a brilliant community of data scientists who can solve really challenging problems  Looking forward to seeing what we can do in  Hi guys\\xa0\\nGlad you like  This dataset reminds me of the RTA data which was really popular \\nOn the IP question when no rules are explicitly stated the Kaggle \\r\\nterms and conditions prevail  Specifically clause  \\nBy accepting an Award you agree to grant a license to the Competition Host to use any Model used or consulted by You in generating Your Entry in any way the Competition Host thinks fit  This license will be nonexclusive unless otherwise specified \\nAnthony pham you do not have enough detail in the claims data to reproduce the DIH properly  Youve likely reproduced DIH from claims data as accurately as is possible \\xa0 Jason the anonymization guys have withheld this information intentionally to make the data set more secure  Sorry Further to Wills point those who followed the Netflix Prize will remember the jump from the Simon Funk discovery \\xa0 Darragh I passed your question onto HPN  Heres the reply\\nIs there a delay between the scheduling of the surgery and when it takes place \\xa0Yes  \\xa0But that is just a matter of scheduling not something forced by the government  \\xa0It would also of course depend on how urgent the surgery is  Just to clarify  the results page will show the leaderboard for all competitors regardless of whether they used future information or not  We will make an honourable mention to the leading competitor who doesnt use future information however their entry will be audited  Kaggle is currently developing a league table that ranks competitors  When it comes to this competition your position on the leaderboard which is indifferent to the use of future information will be what counts towards your Kaggle ranking   INFORMS can offer an awardhonourable mention to those who dont use future data  However the Kaggle leaderboard will not seperate those who use future information from those who dont  Uri you raise an interesting point  However is five months long enough for somebodys rating to move enough for you to notice this Given the way the competition has been setup theres no way to prevent people from using future data  Even if the winner presents a model that doesnt include future data they may have overfitted to replicate the predictions of a model that does include future data  In theory yes  The problem is that theres no way to be certain that the winner didnt use future information  Even when we check the winning model its possible they have used a model with future information to probe the test dataset  This first chart how the leading score has changed on a daybyday basis  The red line shows the Elo benchmark and the blue line shows the leading score  The Elo benchmark was outperformed within  hours which is why its always above the best entry  Interesting to see some recent progress after a period of stagnation well done Philipp  My guess is that any major improvement from this point on will be the result of somebody trying something quite different This chart shows the number of daily entries  Higher early but seems to have stabilised at around  per day  Happy to put up other charts if people have requests   Philipp theres certainly a largish gap between the top five  Of course this is purely indicative  What really matters is the score difference on the final leaderboard     Philipp great suggesion Weve got a stack of features we want to implement but Ill put this in our long term wish list   I tried puting up a general forum for such discussions but found that it was very lightly used  Features in the pipeline include  fixing bugs or incomplete features on the new site  upgrades to Kaggle infrastructure to allow us to score very large entries  Kaggle ranking system  an Elo for Kagglers based on Microsoft Trueskill  extended social networking features including live chat recent activity feeds      Philipps\\xa0 competition analytics suggetion and possibly some other data viz toolsCompetitions in the pipeline include predicting social network connections predicting the likely success of grant applications for a large Australian University forecasting travel times for freeways in Melbourne Australia predicting prostate cancer from a high dimensional dataset subject to ethics approval diagnosing breast cancer from mammographics density images also subject to ethics approvalAny other suggestions Any thoughts on what our priorities ought to be JasonLT are you thinking along the lines of karma points for participating in forum discussions Or would you like the forums to be more of a QA with Stackoverflow style ratingsI like the idea of guest blog posts and community tutorials  After the chess competition ends some might be interested in posting details of their workflowmethodcode LT the general forum has been taken down for the moment  When I get a little time I will attempt to revive it and start encouraging people to use it   Philipp  it may not matter that people only compete in a handful of competitions because each competition contains quite a lot of information  Unlike a single chess game participants are competing against many players  Regardless well do plenty of testing with Trueskill before implementation  As for the points system points seem a little abitrary  I like the idea of ratings that account for the strength of a competitions participants  I tend to agree with your point on forum participation points  The Stackoverflow approach seem like a nice way around the problem There are lots of directions we could take Kaggle  But for the moment were focused on competiitons   You can email me the file if you like anthony goldbloomkaggle com  Id be happy to take a look at it   Dielson good pick up  Dennis is correct  the date format doesnt matter  What is important is that you put the correct data in the correct cells  JoseI notice that you are now on the leaderboard  It can take a few minutes before you show up  Anthony Edith thanks for the feedback  We agree with your comments and we are working on making the terms more competitor friendly   Wu Wei a route is made up of several loops  A figure of   means that  per cent of the loops in the given route are giving suspect readings   mgomari the difference between  and  is counted as two days \\r\\n\\r\\nOverlaps were accounted for so were not double counted  No  Again for privacy reasons  DaysInHospital is calculated based on the LengthOfStay variable  However you dont have enough detail to calculate DaysInHospital from LengthOfStay  this is very useful  Can you use your approach to look at words like weather temperature and humidity That is shown to have an impact on transmission rate e g  referenced here \\nItd be interesting to see what your approach turns up    Wow amazing you got this done so quickly davidmezzetti would it be easy to add the hFactor of the last author As I understand it the lastauthor is the corresponding author and the credibility of the study rests on their shoulders  That could be another helpful proxy   davidmezzetti ignore the hfactor suggestion  Ive just run it past a few people and the feedback were getting is that its probably not such a useful metric  \\nWere getting feedback that some measure of sample size would be really valuable in conjunction with the level of evidence metric  The kind of guidance we were given was \\n For level I either the number of studies or the total number of participants across all studies\\n For level II the number of participants in the randomized control trial etc  \\nItd be great if you could take a swing at this   mlconsult really appreciate you doing this It makes our job of curating much easier   hannesmarais I have been meaning to go back through your system and see how its been performing since you made improvements  Are there questions that it does particularly well on If so which ones\\nAlso can you explain the difference between the question sets original Kaggle vs extended Kaggle from Savanna Reid Thanks for the thoughtful comments  \\nFirst off as always we will not make retrospective changes to how we handle past competitions including this one  When issues like this come up we use it as an opportunity to evaluate how we might improve in the future \\nInternally our debate focused on three issues  \\n\\nrecognition for those who completed stage one but not stage two \\nachievements and how the competition appears on profiles th out of  looks more impressive than th out of   \\nhow points are handled  \\n\\n  recognition for those who completed stage one but not stage two\\nWe need to view the stage one leaderboard as having no weight if it gets a weight we incentivize overfitting or hand labeling for stage one \\n  achievements and how the competition appears on profiles\\nIf we did what Julian suggests and add stage one participants to the bottom of the stage two leaderboard we undermine our rankings by making it very easy for somebody to get an impressivelooking top  achievement by finishing th out of  with a naive submission  \\n    how points are handled \\nThe one change we will make in future is the way points are handled  We will add a multiplier to the number of points for a twostage competition  We have not settled on a formula for doing this yet but commit to communicating it clearly in the rules of the next twostage competition \\nThese are difficult issues but we think this approach strikes the best balance between competing considerations   It is a mistake  were sorry for it but weve decided not to correct it because it might not be fair to some contestants if we change the data midstream \\nShouldnt be too importantonly happened to  chunks \\nIts the same mistake that caused a few chunks to have some missing data within the chunks\\r\\n\\r\\n  If you were  per cent sure that somebody would spend  days in hospital in Y and  per cent sure they would spend  day in hospital than you might predict that they spend would   days in hospital  ChrisR nice to see you competing in this  Sampling is random  Sali Mali has pointed out that there is an error in the AUC\\r\\ncalculation for entries with tied scores that is when two or more scores have\\r\\nprecisely the same value  We will look at the problem over the next  hours and will rescore all entries  ApologiesAnthony The AUC calculation glitch has been fixed and all entries have been rescored  Sali Mali thanks again for pointing this out  Hi Edward  You will appear on the leaderboard as soon as you make your first submission  Hi Edward  Try using examplesubmission csv available at   and replacing the score column with your predicted scores  If youre still having trouble email the file to me anthony goldbloomkaggle com and Ill have a look  Attached is some sample code that can be used to constuct an entry that generates a forecast based on the average travel time on a given route on a given day of the week at a given time  Mmm    file didnt attached  Heres the codephprh  fopenRTAData csv r File to read fromwh  fopensampleHistorical csv w Write the entry to this filedatedefaulttimezonesetGMT Purely to prevent the interpreter from raising a warningtimeStamp  array                               an Array with the humping off pointsforecastHorizon  array forecast horizon in lots of  minutes e g     minutes  minutes   hour  This is used for calculating the forecast time stampsforeach timeStamp as ts  \\xa0\\xa0\\xa0 foreach forecastHorizon as f  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 forecastTimeStamp  dateN H istrtotimetsf find day of week hour and minute that corresponds to each of the timestamps \\xa0\\xa0\\xa0 row  while data  fgetcsvrh    FALSE  loop through the datafile\\xa0\\xa0\\xa0 if row    Write the header \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 colCount  countdata \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  colCount c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh  datac\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewhn\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 if  inarray dateN H i strtotimedataforecastTimeStamp \\xa0  if the day of week hour and minute that corresponds to a forecast timestamp is found then save to an array called tsArray \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  countdata c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 if  emptydatac  datac  x  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 tsArraydateN H i strtotimedatac  datac\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 rowforeach timeStamp as ts  \\xa0\\xa0\\xa0 foreach forecastHorizon as f  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh dateYmd Histrtotimetsf\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  colCount   c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh   arraysumtsArraydateN H istrtotimetsfccounttsArraydateN H istrtotimetsfc writes the average for a given day of the week hour and minute to the submission file \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewhn\\xa0\\xa0\\xa0 fcloserhfclosewh David I believe that when loops the measuring device fail travel times are estimated  Im working towards putting together data on when travel time readings are suspect   Andrew good discovery  Ill pass the question onto the RTA Edit Wouldnt it be obvious if they werent making the adjustment since peak traffic times would change  This code does generate a sample entry  To use it a download the PHP interpreterb create a file name xxx php copy the code above and download the data files to the same directoryc run the command PHP xxx php Youre correct  the future is used to predict the present  However I dont think the temporal leakage invalidates the algorithms developed in this competition   Rob thanks for jumping in \\xa0 Nathaniel thanks for pointing this out  Definitely worth investigating  The Number of Successful Grants and Number of Unsuccessful Grants  fields dont change in the test dataset for obvious reasons  The journal citations also remain constant in the test dataset to prevent participants using the future to predict the past  Nathaniel I have looked at the problem in some detail and have spoken to the University of Melbourne  They are looking into it and hope to have an answer for us tomorrow before they break for Christmas   The university has spent the last two days on the problem  They suspect its an internal inconsistency in their database the figures are drawn from different parts of their database  Well have to wait until the end of the Christmas break to get a final verdict   Deepak thanks for pointing this out  We will ask  the university about this as well  Unfortunately we cant expect an answer until early next year   B Yang first off congratulations again on a fantastic performanceYour frustration is understandable but we cannot enforce rules that dont exist  what is common sense to some is not common sense to others  As Jeremy points out in the RTA competition the rules say The winning entry has to be a general algorithm that can be implemented by the RTA  An algorithm that involved looking up future answers could not be implemented by the RTA   The university has done an investigation and has found that the issue arises from an inconsistency in their database  Y Y Y etc refer to different years  We havent revealed which years to help keep the data private  Eu Jin youve obviously not seen this\\r\\n  Domcastro one of Kaggles first suggestions was to remove the registration fee  \\r\\n\\r\\nFor info the registration fee wasnt ever to raise money but to try and deter people who werent serious from downloading this sensitive data  Kaggle pointed out that anybody with malevolent intentions would probably still pay the modest registration fee so its effect would be to deter people who didnt think they had a chance of winning  Kaggle went on to argue that these people may also come from interesting backgrounds and may be the ones most likely to apply creative thinking to the problem  frankthedefalcos com or the women who have been treated for erectile dysfunction  made another update to CSVs  Covers the new data and adds a bunch of new columns A nudge we still have tables that need to be claimed  If you think its something you can help with please see the relevant list of tasks  Made another update to the target CSVs\\n changes to risk factors removed TB changed BMI to overweight added an extra two columns to break out the results column and added extra data\\n changed to TIE cleaned up the seasonality tables added extra data fixed issues found via error checking sapal I just tried it from Incognito and it worked for me  And others have been getting in  Perhaps you hit it during a temporary outage Suggest trying again   Updated the target CSVs with additional tables from the Transmission Incubation and Environmental Stability task   that was quick This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID  In February  COVID papers were published per day in March that increased to  by mid April that number is up to   It is very difficult to stay up with the literature  \\nWeve been taking some of the most promising notebooks and working with a large team of volunteers epidemiologists MDs and medical students to turn those results into a regularly updating literature review  You can see the work in progress here  \\nNow that we have a clearer understanding of what this needs to look like were calling on our community to more directly feed this effort by outputting results in a standard format  All notebooks should output CSV files in the formats listed in the task descriptions  \\nHow you can contribute\\nRead the instructions within each individual task and make a submission\\nWe also encourage participants that have made submissions and want feedback to join this Slack channel  It includes the domain experts who are curating the tables   mlconsult looks like youve done a really nice job of pulling out sample column  Nice one \\nOther than that column Im not seeing a close mapping to the table format mentioned above  Am I missing something  Oops my bad  Read too quickly  dirktheeng weve been chatting with kylelo and team about the importance of having tables  As paultimothymooney mentions theyre working on approach to making the tables machine readable  \\nDo you think you could reliably take images of tables and turn them into Pandas Dataframes Defer to kylelo but that may be a way to get you tables sooner  \\nAnthony Wow  This is impressive  Itd be great if you loaded it as a Kaggle datasets Just noticed paultimothymooney already has it on Kaggle  Just added a challenge for sharing useful COVID related datasets \\nMotivation for adding that challenge is that a lot of datasets shared in this thread a are really useful b potentially less relevant for the forecasting challenge  We wanted to create a specific outlet for all COVID related datasets    davidbnn this is great Will save a lot of users a lot of pain and makes it easier to explore the impact of weather  Nice work nightranger nice Starting to join datacreate feature matrices is a really nice way to make the datasets surfaced in this thread more useful   koryto one direction you can take is trying to combine as many of the datasets mentioned here into your feature matrix as you can make fit   Nice Glad to see that larger and larger feature matrices being built up   In case its helpful to compare with past pandemics\\nSARS dataset\\n \\nEbola dataset\\n \\nMERS dataset\\n \\nht sudalairajkumar I found these datasets from one of his Tweets  I suspect its pretty easy to get on a countrybycountry basis  Question is whether theres a good global source  \\nBTW trying to keep this thread relatively clear for actually datasets  Were using this thread to ask questions and discuss ideas for datasets  Nice idea but a better fit for discussion in this thread   Pretty sure this is the weather data many of us have been looking for to look at the impact of temperature and humidity on transmission rate\\n   koryto  this is a great direction Makes a lot of sense to start joining a lot of these datasets into a table   I uploaded a dataset of doctors and nurses per capita for  countries from the OECD   cpmpml pointed out that having the number of recovered cases could be helpful  Just pointing out thats available in this dataset already shared by sudalairajkumar   Nice I would havent thought of this dataset   Love this chart And the title is funny   The accuracy threshold will be announced when we release the full claims dataset on May   rudychev received an answer from HPN on this  A patient who visits a clinic outside the network should be captured in this dataset  Of course as Jeremy keeps reiterating there is always a disconnect between reality and the contents of a database  DIH includes inpatient admissions and emergency room visits  As mentioned previously you dont have enough detail to calculate it from the claims table  In this dataset missing PayDelay either means unknown or greater than   In the May  release the anonymization team will topcode PayDelay so there will be fewer missing values and  will mean   RalphH DaysInHospital counts days not nights  So if DaysInHospital is  then they have not been to the hospital at all  If they were in and out of the ER then DaysInHospital would be   cacross HPN had a granularity threshold that they wanted to remain below  Some LOSs had to be suppressed to achieve this target  If there is a blank LOS and SupLOS is  then this is how it was when it came out of the HPN dataset  If there is a blank\\r\\n LOS and SupLOS is  then the LOS has been suppressed  Hope that helps  Out of interest why arent people rerunning old approaches that had previously been scored on the new cross validation dataset Hi allJust to let you know that we have extended the deadline for this competition by just over a week  Both Jeff and I will be travellng around mid November so wouldnt be able to deal with the competitions conclusion Anhony Apologies I hadnt antipicated that this might be an unpopular move  I should have canvassed opinion first  If others also disapprove I will changed back the deadline Kaggle is not a dictatorship The downside of changing back the deadline is that it limits our ability to generate publicity  This bothers me becausea  top performers deserve recognitionb  publicity for the competition is publicity for Kaggle and more publicity  more members  more competitions andc  it lessens the chances of getting FIDEs attention A compromise might be to extend the previous deadline by three days to Wednesday November  when Jeff is offline but I am available  Thoughts Hi PhilippThe Chessbase articles were written by Jeff he has a relationship with the editor  Jeff being away when the competition finishes means that its unlikely that Chessbase will report on the end of the competition a real pity if we hope to grab FIDEs attention  It is unfortunate that were both away when the competition ends Obviously not foreseen when it launched otherwise we would have set a different deadline  Anthony Jeff we must have posted simultaneously  You raise a good point  If Philipp and others are OK with the th then we should go with the compromise date  This would mean that Ill be available to report preliminary results and should mean were ready to report the final results by the time you return  Preliminary will be unconfirmed results from the raw leaderboard  Final results after the top ten have all agreed to share their methodology  Ive changed the deadline to the th  As for Uri breathing down your neck remember that the public leaderboard is only indicative and that the final standings may be different   Hi ArtemFor the intuiton behind AUC have a read of the evaluation page  Kaggle implementation of AUC works roughly as follows  Sort submissions from highest to lowest  Goes down the sorted list and for each prediction plot a \\r\\npoint on a graph that represents the cummulative percentage of class A predictions against the \\r\\ncummulative percentage of class B predictions    Join up all the points to form a \\r\\ncurve  The AUC is the area under this curve \\r\\nHT Phil Brierley for this explanation William no thresholding is required which is part of the beauty of AUC  In fact given that the algorithm works by sorting participants make submissions containing any real number  higher means more confidence that the observation is of the positive class Hope this response doesnt serve to confuse people Anthony Artem Ive gone through the steps using your example data  Let me know if Ive made any errors The Kaggle algorithm basically works as followsFirst order the data predicted        real     Then calculate the totals for each class in the totals  totals  Initialise the cumulative percentagespercentslast  percentslast  Iterate for each solutionsubmission pair counts  counts  counts  counts  percents  countstotalspercents\\xa0  countstotalsrectangle  percentspercentslastpercentslasttriangle  percentspercentslastpercentspercentslast area  area  rectangle  trianglepercentslast  percentspercentslast  percentsSo in your exampleFirst submissionsolution paircounts  counts  percents   percents  triangle  rectangle  Cumulative area  percentslast   percentslast  counts   counts  percents   percents  triangle  rectangle   Cumulative area   percentslast   percentslast  counts   counts  percents   percents  triangle  rectangle  Cumulative area   percentslast   percentslast  counts   counts  percents  percents  triangle  rectangle  Cumulative area   AUC    Also heres Kaggles PHP code to calculate AUCprivate function AUCsubmission solution \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 arraymultisortsubmission SORTNUMERIC SORTDESC solution\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 total  arrayA B\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 foreach solution as s \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if s  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 totalA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 elseif s  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 totalB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentA    \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentB    \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 area    \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countA  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countB  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 foreach submission as k \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if nextissame  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lastpercentA  thispercentA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lastpercentB  thispercentB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifsolutionindex   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countA   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  else \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countB   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifindex  countsolution   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifsubmissionindex \\xa0 submissionindex\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 mycount  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentA  countA  totalA \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentB  countB  totalB \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 triangle  thispercentB  lastpercentB  thispercentA  lastpercentA    \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rectangle  thispercentB  lastpercentB  lastpercentA \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 A  rectangle  triangle \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 area  A \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 AUC  area \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return AUC Apologies Uri and LT  seems that any reply is redundant now  Also big thanks to all those who participated in forum discussions  You helped make this a far more interesting competition   Frank this is great I particularly like the heatmap  is it possible to zoomAlso itd be neat to see some animation on the M map  showing how travel times evolve over the course of a dayweek dots getting bigger and smaller  Though I suspect this might be a lot of work Anthony  Thanks B Yang  The benefit of publishing code is that you get sensible suggestions in return   William thanks for the question \\xa0  Teams are allowed to merge   One individual cannot be part of several teams our systems ensure this anyway     as long as somebody doesnt have multiple accounts Agree that we should make this more explicit in the future \\xa0As for finding people who submit from multiple accounts we are actually in the process of implementing rules that alert us when it looks like this is happening  In the future for large prize money competitions we may look at verifying identities \\xa0 No  Towards the end of this competition you will be asked to nominate five entries that count towards the final result   Maintaining this thread to describe known issues  If you post an issue its helpful if you can link to a notebook that is prefixed the the title ISSUE The CONVENIENT files have negative daily case and death counts in some places  The RAW files are cumulative case and death counts  The CONVENIENT files the diff of the RAW files  \\nHeres a link the notebook demonstrating the issue with some of my early attempts to understand it   Please add any requests you have for this dataset in this thread  If any requests get a number of upvotes indicating some amount of broader interest I will attempt to address the request   Add data in the JHU daily report files for US counties and the global file \\nFor global file this includes IncidenceRate and CaseFatalityRatio \\nFor US county file it includes IncidentRate PeopleTested PeopleHospitalized MortalityRate TestingRate and HospitalizationRate Include population by country in the global metadata file   Remove provincestate from the country level data to make it more convenient to process   Yes  And also means you can schedule a notebook to run on a recurring basis   xhlulu I used GCPs Cloud Scheduler combined with the Kaggle API  \\nBut a few people have asked about scheduling functionality so we are considering adding it as a feature  If this is interesting to you itd be great if you could contribute to the discussion thread that mrisdal started earlier today    fvcoppen I want to add additional data sources over time  So if you find good data sources you think I should add please let me know   sagnik  have you seen this post I describe the issue there  Let me know if you have suggestions for resolving it   maithiltandel it looks like a nice dataset  In the dataset description itd be helpful if you explained more about the dataset itself rather than the findings  For example where the dataset comes from a link to the raw source perhaps a link to the notebook you used to transform it   Looks like a great dataset can you add documentation Would love to know more about the dataset where its from what it can be used for etc   sansuthi I Tweeted the dataset  Do you have a Twitter account you want me to link to in a thread on that Tweet  mohit fyi think link you posted is broken  I think this is the link you meant to post works if you copy the text version clicking the link URL takes you to   Have an issues with the dataset that you want resolved Requests for additional metrics that should be added Please add your suggestions to this thread   kaviml thanks for the lovely post  Its really motivating for us to receive comments like this  \\nsurajjha in replying Im trying to compete with herbison to be the best among the staffs 😂  Jokes aside thank you also for your nice words   To the Kaggle community\\nToday were sharing that D  Sculley will be taking over as Kaggle’s new leader alongside other ecosystemfocused machine learning efforts  Ben and I are leaving Kaggle and Google for our next adventure going back to our startup roots  \\nKaggle recently passed its  year anniversary  We started with a lighthearted competition aimed at predicting the voting matrix for the Eurovision Song Contest in   Back then it would have been hard to imagine that Kaggle would play a meaningful role in the future of machine learning and AI  \\nThere are several aspects of what Kaggle has achieved that we are really proud of  First and most importantly the impact on peoples’ lives  Many have learned machine learning through Kaggle  Of our almost MM users  MM have submitted to the Titanic getting started competition and almost MM have completed exercises from Kaggle’s courses   K college courses have run classroom competitions with K students submitting to those competitions  \\nAnd it’s not just those new to machine learning who use Kaggle to learn  Advanced users get handson experience on lots of different problems and the opportunity to learn from the winning solution  As Grandmaster Vladimir Iglovikov likes to say “I think about machine learning competitions as about a gym but for a machine learning muscles”  \\nKaggle has also provided a credential to the machine learning world  We have given those who are sufficiently determined another way to break into the field  Already back in  Facebook was using Kaggle to find strong machine learning talent  By  we were well established as a great way to land an elite AI job  Today wellregarded AI companies like NVidia and H ai hire teams of Kaggle grandmasters   \\nWe’re also proud of the role the Kaggle community has played in highlighting what works well in practice   machine learning papers per day are published on Arxiv and countless machine learning packages are developed  Kaggle users explore these techniques in a competitive environment and spread those that work  Frameworks like Keras and XGBoost took off in the Kaggle community along with useful preprocessing and data augmentation libraries like albumentations for computer vision  Many techniques have spread through Kaggle including UNets for segmentation denoising autoencoders and adversarial validation  And Kaggle has helped prove out new applications for machine learning including medical imaging and automated essay grading \\nAnd finally we’re proud of the fact that while we started with machine learning competitions we’ve launched other services  Notebooks have increased the ways our users can share learnings and our courses have made Kaggle more approachable to new users  And there’s no machine learning without data and we’re proud of our collection of over K public datasets which makes Kaggle one of the world’s largest repositories of public datasets  \\nOf course none of this would have been possible without both the community and the Kaggle team past and present  Over the years we have had the privilege of meeting so many passionate and energetic community members with so many inspiring stories  And the opportunity to work with a talented and motivated team  We’re grateful to all of you   \\nAnd while we are proud of what Kaggle has achieved so far we’re also very excited to see what the D  and the team accomplish in the years ahead  D  has been operating on the cutting edge of machine learning for the past  years working on very large scale machine learning systems inside Google doing foundational research on topics like MLOps and leading large research teams  D  also has a long history with Kaggle starting with the semisupervised machine learning challenge he launched in  which as it happens was won by some familiar faces  We’re excited what the combination of D ’s background and history with Kaggle will bring to the future  \\nFor those who want to get to know D  better he is going to be answering the most upvoted questions over on this post  \\nAnthony and Ben'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data_g['clean_messages'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f04f0",
   "metadata": {},
   "source": [
    "<b> STEMMING </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "319387aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words_list=text.split()\n",
    "    new_list=[]\n",
    "    for i in words_list:\n",
    "        word=stemmer.stem(i)\n",
    "        new_list.append(word)\n",
    "        \n",
    "    words = new_list\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d439c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_posts'] = train_data['clean_posts'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aa194b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im find the lack of me in these post veri alarm sex can be bore if it in the same posit often for exampl me and my girlfriend are current in an environ where we have to creativ use cowgirl and missionari there isnt enough give new mean to game theori hello entp grin that all it take than we convers and they do most of the flirt while i acknowledg their presenc and return their word with smooth wordplay and more cheeki grin this lack of balanc and hand eye coordin real iq test i score internet iq test are funni i score s or higher now like the former respons of this thread i will mention that i dont believ in the iq test befor you banish you know your an entp when you vanish from a site for a year and a half return and find peopl are still comment on your post and like your ideasthought you know your an entp when you i over think thing sometim i go by the old sherlock holm quot perhap when a man has special knowledg and special power like my own it rather encourag him to seek a complex cheshirewolf tumblr com so is i d post not realli ive never thought of ei or jp as real function i judg myself on what i use i use ne and ti as my domin fe for emot and rare si i also use ni due to me strength you know though that was ingeni after say it i realli want to tri it and see what happen with me play a first person shooter in the back while we drive around i want to see the look on out of all of them the rock paper one is the best it make me lol you guy are lucki d im realli high up on the tumblr system so did you hear about that new first person shooter game ive been rock the hell out of the soundtrack on my auto sound equip that will shake the heaven we manag to put a coupl pss in no the way he connect thing was veri ne ne domin are just as awar of their environ as se domin exampl shawn spencer or patrick jane both entp well charli i will be the first to admit i do get jealous like you do i chalk it up to my w heart mix with my domin w s and s both like to be notic s like to be known not the same d ill upload the same clip with the mic away from my mouth than you wont hear anyth ninja assassin style but with splatter tik tok is a realli great song as long as you can mental block out the singer i love the beat it make me bounc drop io vswck d mic realli close to my mouth and smokin ace assassin ball play in the background sociabl extrovert im an extrovert and im not sociabl sherlock in the movi was an entp normal hes play as a extj in the book hes an estj as i said the movi look good except for it be call sherlock holm oh i never had fear of kiss a guy i will kiss an anim too so there was noth to vanish just person tast and me not like it the guy i kiss didnt know me it was one of those sound pretti much like my area and what im go through right now tri to figur out which way i want to take my life i want to do so mani thing the biggest problem is that i know if i dont d i was oper under the impress that you were femal i never look at your boxi okay i help out my gay friend all the time and one of them has develop a littl crush on me i get red tt you just describ me and im live the worst nightmar im trap in one place with one one around onli dull wood if i was a serial killer this would be the perfect place but sad im tbh and bias sound like a shadow infp i think mayb he was hurt and turn estj i can tell becaus he has some of the typic infp trait left over check list im sorri it seem that you have came at a bad time weve alreadi reach our quota of infj howev be your femal and i like femal i will make you a deal i will kick one im antp lean toward e im easi for both entp and intp to identifi with i also imagin entp interrog would go a littl bit like jack from except more mechan rig up shock treatment equip in an abandon build out of an old car batti jumper it was a compliment trust me im just as psychopath d except i have emoticon theyr just weird one like laugh when i get hurt or at peopl run themselv over with their lawn mower no it like a theme for where i live and that is whi i know it by heart and i usual dont leav until the thing end but in the mean time in between time you work your thing ill work mine d d im the mbp pleasur to meet you damn need to trust my instinct more i would have been closer i was go to say infp exfp lean toward s with the way she respond d my friend even my gay and lesbian one alway come to me for advic i bow to my entp master entp are so great if it wasnt for entp i wouldnt have been abl to build what im build duck duck duck shotgun what me i never do that becaus it hard to be sad about lose someon you like when you knew you were right and give yourself a big pat on the back becaus your awesom and alway correct oh you dont have to tell me that most of them are stupid i know this that is whi i play with them and it make me laugh d as im go to take neuropsycholog and i have a few psychologist d im a nightowl i wake up between pm and stay awak till am person opinion back by theori would suggest that intp are the most social difficult while intj can be social indiffer but they will also use social situat if the the need aris person stock that i have on my desktop that ive download from random stock site and stock photobucket ill tell you when i open photoshop glad you like it static d thank made for a friend sever hour of work i construct everi line by static ill have to get to your avatar later if one of my fellow teammat doesnt psychologist dont keep me around long enough to diagnosi me i like to toy with them what i have diagnosi myself with and had a few psychologist friend a few other friend tell me i have is'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['clean_posts'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd2382c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_g['clean_messages'] = forum_data_g['clean_messages'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cbe4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi tanya kaggl will maintain a rate system if you win but your inelig for prize money you will still get a strong rate anthoni here are some paper that analyz eurovis vote pattern you might find some of them help gather comparison of eurovis song contest simul with actual result reveal shift pattern of collus vote allianc eurovis song contest is vote polit or cultur ginburgh and nouri suleman efstathiou and johnson eurovis song contest as a friendship network dekker more research enjoylov thi neighbor love thi kin vote bias in the eurovis song contest cultur and religion explain the bias in eurovis song contest vote hybrid system approach to determin the rank of a debut countri in eurovis eurovis judgment versus public opinion – evid from the eurovis song contest giovannithank for your feedback use the forum to give feedback is a good idea it allow other to see and comment on suggest we might set up a proper feedback forum but for the moment this topic will have to suffic i also agre that the forum is a bit clunki howev we have a larg list of featur request and onli limit resourc for the moment it might take us some time to address this apolog i dont think the prize money in this competit is that relev the prize is relat small correct me if im wrong but i think contest are driven by intrins factor a karma system that reward forum post is a good idea again apolog for ani delay in implement this there are lot of featur on our to do list anthoni manish thank for the feedback the site is host on an amazon ec server on the east coast of america it a fast server but the site has been more popular than we expect were current work on speed up the site by reduc the number databas queri we may have to implement auto scale if the site keep grow so rapid anthoni just made a chang which should speed thing up let me know if it has made a differ for you jonathan thank for your feedback x were current work on cach databas queri there are a lot of good suggest here that well tri befor autosc pleas use this topic to give us feedback if youd rather do so in privat email me at anthoni goldbloomkaggl com use this topic to discuss ani competit you would like us to run if you would rather contact me privat email anthoni goldbloomkaggl com i accident delet the follow post made by anoth user im repost it on their behalfar there categor andor binari variabl in the data set other than the target variabl for instanc variableopen in test data seem to have categori if there are categor variabl do we get to know what the categori meanthank you thank for the feedback what sort of featur do you have in mind or can you point to a forum that you we should emul i just ad a quick repli box to make the forum less clunki great suggest i have put this on our extens featur to add list colin the choic of score system was quit deliber will the competit host consid use area under the roc curv where particip submit probabl but said that he deal with physician who just want to know the proport of predict that are correct rajstennaj and colin were realli pleas that you believ that there valu to the project let me know if there is anyway that we can help to facilit a communiti we did set up the general kaggl forum under communityforum with such a communiti in mind doe it provid suffici infrastructur you are free to start ani new thread on that forum regardsanthoni out of that pretti impress piti they didnt enter the kaggl comp i think your right some competit exhibit more regular than other soccer may be a difficult sport to model hi pg not sure that i fulli understand the question are you refer to the situat where a classifi return onli or rather than a score or probabl perhap you can use an exampl to illustr the question regard anthoni hi pg you should give the score for all timestamp a higher score mean the instanc is more like to be a member of the posit auc measur your classifi abil to split the class so you dont need to decid which score predict posit instanc and which predict negat instanc have i address your concern the public leaderboard is onli indic becaus competitor can use inform on their score to get inform on a portion of the test dataset the final result are a quit differ and b better reflect actual perform thank for particip in this competit ive attach the solut file to this post updat the solut is no longer attach but your welcom to make submiss to this competit hi mattthank for the nice word and the suggest ive post the solut file hi dirkth elo benchmark is base on the train dataset onli have had mani email convers with jeff i can tell you that the seed rate matter a lot youll notic that jeff made two submiss for the elo benchmark that becaus hes refin his seed method i believ he plan to make a few more refin jeff use an iter process to seed the rate system for exampl he might start by give everybodi and then let elo run for month he then seed elo with the month rate and run elo again he doe sever iter of this doe this helpanthoni hi johnhav just confirm with jeff methodolog will be share public regardsanthoni the competit has been design to make cheat realli difficult at the end of the competit the winner methodolog will be replic to help ensur everyth is abov board hi mattth reason we prevent particip from submit an unlimit number of time is becaus otherwisea our server may not be abl to handl all the traffic anda it would be easier to decod the portion of the test dataset that use to calcul the public leaderboard the techniqu you describ often refer to as cross valid is veri sensibl and we encourag other to use it anthoni good suggest were open to idea on how we can facilit this my think is the best thing to do is to implement a more function forum which were do we can then encourag those who are still work on the problem to continu to use the competit forum as a way to collabor hi dirkwev updat the data descript thank for the pointer the competit doe requir particip to forecast the next four observ weve updat the format of tourismdata csv so that there is alway a valu in the last row regard anthoni hi gregapolog there was a bug that cut off the last charact the problem has been fix but unfortun the fix will onli appli futur submiss thank for point this out and sorri for the inconvini anthoni hi matti believ that will the competit host is prepar a blog post that discuss some of the method that peopl appli to this competit base on the feedback we receiv is this the sort of thing you had in mindanthoni david this is a great suggest the hiv competit show that kaggler can do great thing my initi concern with ani public dataset is that peopl can look up the answer we would need research to withhold a small portion of the dataset for evalu i think the first step is to get in touch with those who set up the alzheim project it also make sens to contact the michael j fox foundat if anybodi has ani connect to either of these project pleas let me know otherwis ill keep you post on ani progress anthoni jase the score on the full dataset is calcul onthefli so we actual know who is win base on the full test dataset ron the submiss that is perform best on the public leaderboard may be differ from the submiss that is perform best on the full test dataset we dont link the best submiss on the public leaderboard to the best overal submiss so that particip dont becom confusedconcern if their scoreposit on the public leaderboard worsen leigh my think as well in a tradeoff between have a veraci public leaderboard and a veraci end result the end result is most import jeff good suggest ive put togeth an excel sheet that might be help for cross valid you past your predict for month into column g and it aggreg by player by month and then calcul the rmse hope it help anthoni benth evalu method was chosen becaus jeff has found that score base individu game with rmse unduli favour system that predict a draw mark glickman rais anoth issu rmse is better suit to normal distribut rather than binari outcom so in order to use rmse aggreg is prefer of cours we could have evalu on a game by game basi use a differ metric my biggest problem with the current evalu method is that count a draw as half a win seem a littl arbitrari howev in order to benchmark elo such an assumpt is necessari mark and jeff argu that a draw is general worth half a win so this assumpt isnt too problemat anyway hope this give you some insight into our think regardsanthoni jeff pleas correct me if im mistaken but i believ system that predict draw are favour becaus a high proport of game are draw at the top level per cent in the train dataset of cours you can do better but a system that predict for everi game will perform better than it should has anybodi tri trueskil yet probabl a better start point than elo this blog post doe a nice job of step through trueskil matt am interest in your think on this whi mae over mse or rmse is it just that the metric is more intuit or someth subtler hi davidi have written to the alzheim diseas neuroimag initi adni and the michael j fox foundat i am schedul meet with both for septemb will keep you post on ani progress can you put up a link to the datapap you foundthank again for the suggest it great if we can use the power of this platform to tackl meaning problem regardsanthoni uri the correl between the public leaderboard score and overal score is signific higher now here is the solut file for anybodi interest uri im reluct to releas confid interv inform becaus i want to minim the advantag to earli submitt earli submitt alreadi have the small advantag of have seen their submiss on two differ public leaderboard by releas confid interv inform im give earli submitt access to inform that isnt avail to later entrant jase asid from chang the size of the public leaderboard portion of the test dataset we also select it more sensibl so it better repres the overal test dataset jpl a competit use internet chess data is a good suggest for interest the reason we are run the competit use top player is becaus elo rate matter most for top player sinc it is use to determin who can play in which tournament out of interest has anybodi enter this competit use glicko glicko or chessmetr are either of you happi to send me your unmodifi glicko submiss it would be good to add a glicko benchmark team to the leaderboard my email address is anthoni goldbloomkaggl com would like to do the same for glicko and chessmetr if anybodi has tri those i have also contact ron about use his trueskil submiss as a trueskil benchmark jase i post a link to your glicko code on the hint page it veri good of you to share it im realli surpris that glicko is perform wors than the elo benchmark do you think this is becaus jeff put lot of work into optim seed the elo benchmark or is glicko just not as good vateesh thank for send the file the file that you sent are actual differ i also had a look at your submiss and you have a few file with the same name but differ number also i was not abl to replic the problem as you describ it perhap you can tri again and let me know if your still experienc the error was just chat to jeff time permit he is go to benchmark some of these other system this way they will all be benchmark on a consist basi use the same seed procedur and the same degre of tune hi vess this should not be a problem given the way submiss are store uri thank for point out the problem were current work on a big upgrad to the websit the new site should be launch by the end of this month the upgrad will involv a more function forum in the meantim i will tri and fix this problem anthoni uri im not abl to replic the error either on the live site or on the develop version can you let me know if you experi it again hi han which post still cant replic the bug intermitt problem are realli annoy as mention were do a massiv site upgrad at the moment so that take up the major of our develop time how serious is the problem can we live with it for the next few week until we deploy kaggl eric thank for the feedback there not realli ani reason to insist on a particular file extens were current do a big site upgrad so ill add this to our list of featur request seyhan the leaderboard portion of the test dataset is select random it is somewhat repres of the overal stand i would realli like to be more activ in the forum look like there some live discuss happen ive been flat out work on the site upgrad which is onli a few week away from launch anyway id like to share a few thought on this discuss first off there is quit a strong correl between the public leaderboard and the overal stand second the lack of relationship between the score and the score might indic overfit this may be the case if your experienc a larger improv on the dataset than the dataset on a relat point i notic that your all perform veri well it could be that youv reach a local maximum i e the best possibl score given the techniqu your use just to reemphasi jeff point you should pay more attent to your cross valid than to the leaderboard the leaderboard is calcul on a veri small amount of data so it is onli indic phillippsorri for the delay in do this i havent had comput access over the last few day the spearman correl between public score and overal score is i also calcul the correl for differ submiss quintil to make sure the relationship hold at the top it doestop it also worth mention that the troubl particip are have reflect realworld difficulti in formul a chess rate system this competit is not just a game but a genuin attempt to explor new approach to rate chess player anthoni greg thank for point this out im current travel but will look into this over the weekend wil if you can get histor data from freechess org possibl by agre to share the win method with them wed be happi to host a comp here this way you could specifi that the win method must be an instant gratif system it would also result in a system that tune to lower rank player thank for point out the error it has now been fix apolog for ani confus cole sorri for the slow respons in this competit all your submiss count in futur we will ask particip to nomin submiss phil that is correct you must rememb that kaggl hope to do more than just host fun competit we want to help solv real problem this is whi were reluct to forc particip to choos just one model they may make a poor choic and the compett host may end up with a suboptim model our compromis posit is to allow partip to nomin five entri a featur which well roll out for futur competit phil number is correct luck will play a part but i suspect the test dataset is larg enough to limit it impact i agre in a competit like this one but as mention abov we want to host competit that are use as well as fun an upcom competit will requir particip to predict who has prostat cancer base on variabl in a competit like that it would be a shame to miss out on the best model requir particip to nomin five submiss seem like a good compromis greg this problem has now been fix thank again for point it out hi coleapolog for the ambigu the time is as it appear on the competit summari page adjust accord to the timezon on your comput clock so itll be saturday or sunday depend on your timezon you can also see a countdown on the kaggl home page anthoni dirk i just chang the file post on the data page to a unix format hope this solv the problem durai apolog for the slow respons all up countri were repres here is the list in order of most particip to fewest unit state unit kingdom australia canada thailand india germani spain china netherland franc itali new zealand south africa sweden argentina croatia ecuador greec indonesia iran ireland mexico poland portug russia singapor turkey and ukrain ricardo you are correct i gave the countri list for the wrong competit countri were repres unit state colombia india australia unit kingdom franc thailand canada germani argentina japan afghanistan albania austria belgium chile china croatia ecuador finland greec hong kong iran poland portug slovak republ venezuela uri make a veri good point one way we could run a competit without know futur matchup is to have particip rate everi player onc we know the matchup we can infer predict base on player rate the onli downsid to this approach are it doesnt allow for probabilist predict sinc there are mani way to map rate into probabl we couldnt show a live leaderboard which help to motiv particip interest in other thought on this particular the import of a live leaderboard ron this is fantast look like a sizabl proport of the black dot are sit in a vertic line though im sure the elo benchmark would look much wors out of interest what softwar did you use to generat the viz ps im guess the anomali that this viz highlight e g that white is a smaller advantag for lower rate player could inform futur version of your rate system philipp i dont fulli understand your suggest do you mind tri to explain it again possibl by refer to an examplea a general principl tne problem with attempt to prevent peopl from use neural network and the like is that particip use them anyway and then overfit other system to replic the neural network result i actual think that have neural network et al in the competit is valuabl even if they wont be implement as rate system they may have some benchmark valu assum they predict most accur they give a sens for what level of predict accuraci is possibl from ani given dataset as an asid if we requir particip to submit rate and dont give them access to the matchup that theyll be score on this should forc particip to creat a rate system shouldnt it btw jeff re i have been and continu to be amaz by the level of particip so far i had no idea so mani peopl would particip congratul on organis such a popular competit pew what criteria would you use to evalu such systemsbtw i think youd be surpris at the proport of the top who are build rate system philipp thank for point out this bug the error was onli aesthet had been accid hardcod into the new theme the platform was still onli permit two submiss anyway the error has been fix philipp thank for your nice word hope have a more profession look and feel will help us attract interest competit with bigger prize pool hi allwond whi the benchmark is still lead when it is public avail have peopl had troubl replic the author methodolog or is everybodi tri their own approach anthoni hi jesseyou are correct this is instruct is wrong the month column mm should be line long includ the header and the quarter column qq should be line long the examplesubmiss csv file avail on the data page give an exampl im at a confer today but will correct the instruct as soon as i get the opportun someth was amiss there was an error in the data upload on kaggl kaggl fault not the author the chang are not particular big so model that perform well on the previous dataset should continu to perform well to give you the opportun to rerun your model and make new entri we have extend the competit deadlin by two week and lift the daili submiss limit to three per day and i believ georg intend to releas the code use to creat the benchmark apolog for the error dont hesit to ask if you have ani question unfortun the movi isnt out in australia yet weve still got anoth week to wait sorri for the slow respons ive been flat out with the new site launch below is the list of row use to calcul the public leaderboard dirk ive chang the line break format let me know if this doesnt fix the problem jason there a bug that prevent user see previous score when they have longish techniqu descript we are awar of the problem and will fix it as soon as we can diogo thank for point out this error we will setup pagin on the submiss page short tim kaggl is current in the process of put togeth a leagu tabl which rank particip base on competit perform if you perform well in this competit it will count toward your rank phil i made an error in the ten per cent list abov tri score with the follow row steffen you can enter use a model code in ani languag johndrew i presum those who enter use softwar other than r are still elig for prize diogo thank for point out this bug few minor teeth problem with the new site we should have them sort out befor long yuchunapolog for this error the public leaderboard is portion of the test dataset is actual the first per cent becaus we hadnt implement the code to select a random portion of the leaderboard yet for info the reason we kept get differ per cent is becaus the random seed in the databas was set to zero which told our code to choos a random random seed anthoni hi jonit a fix per cent chosen random anthoni hi tamasa your result suggest the order doe matter and the id dont anthoni jc i agre that those who enter earli have an advantag howev the main sourc of advantag come from the fact that they have had the opportun to spend longer on the problem and tri more thing philipp the current leader has made entri if this competit took ternari score loss win draw this would amount to possibl combin make phillip entri a drop in the ocean in fact the test dataset is richer becaus particip predict the probabl of victori nonetheless for futur competit we will ask particip to nomin five entri that count toward the final stand pew we are not requir particip to guess but rather encourag them to reli on their cross valid when determin which model to choos the problem with allow peopl to enter mani time and tri mani paramet tweak is that they are more like to accident overfit on the test dataset by this i mean they are more like to find a paramet tweak that work well on the test dataset but doesnt work as well for futur chess game on your second point you are correct to say that i am worri about statist guess the requir that particip submit code doe not obviat this concern becaus model can be overfit onc the answer are known in the extrem case somebodi could fit a decis tree that classifi everi game perfect if they know the answer show the stand but not the score make statist guess onli slight more difficult becaus particip are close enough that the leaderboard order give meaning feedback on which guess are better and which are wors as an asid it seem that i have fail to convey the messag that the public leaderboard is pure indic and that cross valid is import i would even go so far as to say that it may be problemat if the public leaderboard bear too close a resembl to the overal stand i like uri suggest it get around the problem that lt menton while potenti encourag peopl to tri thing beyond paramet tweak coupl of potenti problem a particip exhaust the submiss limit and anoth entrant make and share a breakthrough eg the use of chessmetr in this competit anybodi who has exhaust the submiss limit wont have the opportun to build on the breakthrough this seem less than ideal given that we want to get the best result possibl it might encourag peopl to make all their entri at the end so that they dont reveal the strength of their hand what do other think philipp kaggl has been experienc a massiv lift in site visit and signup sinc the new site launch from uniqu visitor to this account for the increas in entri thank everyon for make this an amaz competitionbig congratul to the winner outi also to the runner up jeremi howard who onli join the competit late in the piec and to martin reichert who finish third hope well get some of the top ten to tell us about their method on the blog in the meantim i encourag you all to tell us a littl about what you tri on the forum also for interest here a chart that show how the best score evolv over time rapid improv initi but after a month progress stall as particip approach the fronteir of what is possibl from this dataset i think i can help with this i dont give name just score combinationsscor publicscor jeff can i post the test label on the forum i onli seem to have the aggreg solut on hand attach jeff do you have the game by game labelsedit look like you post a minut befor me hi nickyour welcom to bring addit data as long as it public avail anthoni attach is some sampl python code that generat forecast base on the last known travel time im new to python so happi to hear ani feedback on the code file didnt attach here the codeimport csvimport datetimerhopenrtadata csvr read in the data whopensamplenaivepython csvw creat a file where the entri will be savedrhcsv csv readerrhtimestamp an array with the cutoff pointsforecasthorizon forecast horizon in lot of minut e g minut minut hour this is use for calcul the forecast time stampsrow inialis the row variablefor data in rhcsv loop through the data if row if the first row then write the header for j in rangelendata wh write dataj wh writen if data in timestamp if the row is a cutoff point for i in forecasthorizon for each forecast horizon write the cutoff travel time as the forecast the definit of naiv datestr strdatetim datetimeintdataintdataintdataintdataintdata datetim timedeltai calcult the time stamp given the forecast horizin wh writedatestr write the timestamp to the first column of the csv for j in rangelendata wh write dataj write the cutoff travel time to the subsequ column wh writen row rh closewh close dirk thank for point this out ive written to the rta about this and they respond sayinginde our control room have confirm signific increas in traffic volum follow the remov of the toll this has had an impact on the overal travel time across the m someth to be awar of when use the older data hi denni the per cent doesnt count toward the final stand and is select at random across the timestamp and rout as for the smtp error it been fix the problem was the result of a flood of signup which caus googl to shut off our mail server were now use our own mail server anthoni apolog for the error it decisecond not centisecond so is second ive fix the descript this is someth that should be dealt with on a case by case basi if you find a dataset youd like to use ask on the forum and ill run it by the rta for inform im tri to get hold of some incid data will keep you post on this lee this is great dirk did the same thing with some python sampl code i wrote for the social network competit if you guy keep show me how thing can be done better i may becom a half decent coder toppi thank for the pointer a higher prioriti at the moment is to get forum attach work again hi peter ill follow up in this at the veri least we should be abl to provid inform on the length of differ rout anthoni hi carlo unfortun not claus c in kaggl term and condit saysc employe or agent of the competit host are not elig to particip in ani competit post by the competit host to answer the second question we would more inform about the natur of the busi and what your friend doe anthoni c doe seem to be an express languag im a linux user though so not inclin to pick it up armin i agre make more sens for me compil this inform onc for everybodi will tri and get it done this week daniel denni is correct in say that averag the valu lead to float point number the answer are integ but the rmse is calcul use float point arithmet daniel and denni are correct keep in mind that the per cent is a random select of the that doesnt count toward the final stand which are calcul base on the other per cent the cutoff time are all between am and pm they were select use a simpl formula that favour high volatil cutoff time over low volatil cutoff time so youll see more peak hour morn and afternoon cutoff time the rational behind this is that it more import to predict accur dure high volatil time so we want to favour model that do best at these time that explain whi the rmse is higher than for random chosen cutoff point aidan have ask the rta about this this was the respons the cutoff is due to free flow condit impos by the system dure data unavail ive written again ask for a littl more detail will post the respons when it come paresh thank for the thought provok question i agre with denni i am more interest in the time delay than the percentag delay on a relat matter we think it is more import to predict correct when travel time are volatil e g befor and after work to favour model that predict more accur dure high volatil time we select more high volatil cutoff point so youll notic more cutoff point dure the morn and afternoon phil thank for share this just got to find a window machin to run it on hi markus i can help out on the second part of your queri ive post some php auc code on anoth forum post softwar packag like r have easi to use packag that calcul auc anthoni rasmus apolog i delet the wrong post anyway you ask how travel time are measur there are regular space loop along the m these loop measur each car speed and the number of car that travel across the loop everi three minut travel time are then calcul use a formula the formula has been test and calibr use test car that travel along the freeway and record their travel time thoma i select specif cutoff time random but chose timeday combin that are volatil across the dataset vitali the volum data is use to calcul travel time see this post for more info our prioriti at the moment is to get the incid loop error and rout length data togeth howev i can find out if this data can be made avail if you think it might be use as denni say itll be high correl with travel time and we obvious wouldnt releas it for the blank out time jeremi i wasnt awar that public document with traffic detail were avail to the extent that ani inform is avail for blank out time this would most definit be consid cheat as for question i am awar of this in fact the issu came up in anoth post the rule state that the win model must be implement by the rta in order to be elig for the prize the averag model pass this test as an asid i dont believ the tempor leakag invalid the algorithm develop in this competit benjamin onc we get the incid data i will put in a request for this data i have some inform on suspect loop read that im work to releas this has inform on when loop read may be unreli for various reason i dont yet know whether or not this will help with the free flow issu anyway i will upload them as soon as i can get it into a use format i suspect the reason the free flow time are differ is becaus rout length are differ rob on your point about miss data it might be help if i explain how i put the file togeth i receiv data in the follow formatrout idtimestamptravel time xxx xxxi transpos them into in the hope that theyd be more manag when timestamp were miss i just fill in a blank row hassan the most import file is rtadata csv you can creat a sampl entri by download rtadata csv and createhistor php attach to the same directori navig to that directori in the terminalcommand prompt run php createhistor php this will creat an entri base on a histor averag for that timeday and is a good start point bjb veri generous of you to upload a java code ive now enabl java file upload so you should be abl to upload the file burak the time in sampleentri csv are the time you need to generat forecast for there more info on how the cutoff point were select in this forum post aaron you rais a good point accord to the rout definit i have rout extend from loop a to loop a while rout extend from a to a so should encompass all of denniss observ that sometim has longer throughput time than is strang ill doubl check the definit with the rta aaron anoth good question have also pass this on to the rta the number direct to the left of the team name is the team posit and the number to the right of the team name is the team score or root mean squar error rmse alexand to me this mean that the algorithm can take a timestamp as an input and can generat forecast for the next min min etc lee thank for point this out this post post alexand there is no truncat of float mmm my messag seem to have disappear from the board anyway here a repeat aaron the unit are decisecond nick actual it a hybrid approach you can nomin five entri that count toward the final stand you do this from the submiss page the last five are chosen by default at the end of the competit the best of your five nomin entri count toward your final posit and nick on your new question the one of the five you nomin that score best on the per cent count the per cent is meaningless as far as the final stand are concern jose do you want me to ask if it permiss to use noaa data if so are you ask about the data that brad mention abov realli nice feedback veri thought provok the api suggest is nice it doe seem that it would prevent peopl from use the futur to predict the present howev the testtrain split is still necessari to prevent overfit and we could still onli give partial leaderboard feedback the api doesnt secur against overfit paramet tweak also the api approach would add new problem model will take longer to run becaus of the delay in receiv data point as you say it would add a huge load on kaggl server as for the problem you list here are my responsespredict can't use all avail prior data sinc the test data doesn't provid resultsthi is necessari to ensur against overfit if all the data is use to calibr a model it imposs to know if the model will fit futur dataset as well limit train and test data creat too much varianc between the public score and actual scoreth mistak made in the first competit was with the size of the public leaderboard portion of the test dataset my fault not jeff it was too small which lead to the low correl between public and overal score for the rta competit we rais the proport to ensur a stronger correl this proport was calibr after some test of the correl between the two part of the test dataset we intend to continu this practic go forward model paramet can't be tune becaus actual score aren't provid if we allow paramet feedback on the whole test dataset this would almost definit lead to overfil paramet tweak that work on the test dataset but wont work for for futur dataset number of submiss is sever limit becaus they are so larg this will becom a bigger problem as larger test dataset are creat i dont think more daili submiss are necessari becaus the major of model build should be done with refer to a cross valid dataset leaderboard doesn't reflect actual leadersagain this was my mistak i made the public leaderboard portion of the test dataset too small this is not a flaw with the general approach futur data can be use to predict the pastjeff suggest a realli nice solut to this test set includ some spurious game so that peopl can't mine the test set for use data about the futur these spurious game wouldnt be use in final evalu the api also provid a realli nice answer to this problem attach is some r code to creat a glm entri for this competit as alway happi to hear feedback from other about how this could have been done more eleg anthoni nathaniel is right the data is correct it just a problem with head format will fix this short and reupload the data eleni just upload routelengthapprox csv which has approxim rout length data konstantin just upload rtaerror csv the is valid data it avail on the data page final fix the head just to reiter all the data are correct it just the capit in the head that caus troubl as for the inconsist number of delimit also fix my softwar packag stop print delimit when there were no more valu or nas in a row jack the countri of birth issu is now fix pleas download the latest version of the data p v kiran it mean that if your solut is implement use a softwar packag that is not avail to the univers of melbourn it must be possibl to translat your solut into a differ packagelanguag mooma i appreci your frustrat but sensor malfunct are part and parcel of deal with realworld data if we had the data readi at the outset we might have exclud fail sensor and downweight the impact of partial fail sensor when evalu predict konstantin denni is correct it is not safe to assum that there is no error in the control data jose and joseph just spoke to the rta about this the answer is no becaus it might allow futur weather condit to be use to predict the present ahm just got an answer from the rta on this here the responseth answer is mayb rta would request that anyon wish to use the data for further research purpos write to the rta and make their case describ what they wish to do ie the purpos of the research and how they would use the data the rta will consid each applic on it merit let me know if youd like me to pass on the relev email address im reluct to do it in the forum but will offer an introduct to anybodi who ask just elabor a littl the type of solut that cant be implement are those that are encumb by patent or other intellectu properti restrict michelangelo truth is that you can submit ani real number we suggest a number between and becaus of the conveni interpret auc rank your score the higher the score the more confid you are that the instanc is a member of the posit class mani thank to everyon for all your great activ on this fascin problem insight question and comment on the forum good earli result on the leaderboard and interest discuss there have been a lot of question about exact what constitut an accept model for the rta so far my guidanc on this matter has possibl been too fuzzi and i hear a lot of you look for more definit rule therefor we have come up with the follow specif rule regard the allow model input your model can be of ani form you like as long as it take it input onli from the follow paramet time of predict day of week is holiday month of year rout number to be predict the time taken for rout r for datetim t where r is ani rout and t is ani time less than the datetim be predict for as mani rout and datetim as you wish the sensor accuraci measur for ani rout r and datestim t defin as abov the estim rout distanc as provid by kaggleto clarifi the follow are not permit the use of ani data other than those provid by kaggl for this competit and the list of nsw holiday the time taken for ani rout in the futur compar to the predict be made your model can still be train use all data as long as the result model onli use the input list abov furthermor the algorithm must not be encumb by patent or other ip issu and must be fulli document such that the rta can complet replic it without reli on ani black box librari or system hi alexand no use full timestamp make it possibl for a model to implicit incorpor extern data and futur data you may also use holiday data extract from the pdf file that you link to in order to get holiday inform for previous year howev we will not be provid a file of this inform direct this is correct anthoni personid refer to all the column that have investig id e g column has investig column has investig ignor the comment numer valu that should be as jeremi howard point out earlier in this thread the key point that answer most of these question is that the limit is onli on the function form of the final model more specif xiaoshi lu you can build your model filter aggreg etc use all the datetim inform you like the final function form that you end up with howev should onli use the predictor list abov mooma the input list includ this the time taken for rout r for datetim t where r is ani rout and t is ani time less than the datetim be predict for as mani rout and datetim as you wish so what you ask is specif allow of cours for you to creat your input file which includ for exampl the time taken one hour earlier you will need to use the full datetim howev the result model will not direct use this instead it will onli use the time taken on that rout as allow by the rule alexand groznetski imagin use a veri flexibl model neural net for instanc which train with all datetim info includ in the input paramet it might implicit end up use the rout time later in the day to predict those earlier this is an exampl of how a model could be useless in practic even although it appear high predict on the competit data matthew use gpl code is fine the isholiday variabl can be a direct input rather than a variabl that is deriv by refer to a timestamp you contact me direct at anthoni goldbloomkaggl com denni you can use isspringbreak rather than isholiday david it realli neat for info it work in safari but the page video are align a littl strang martin dane is correct the inform in routelengthapprox csv is in metr so rout is approxim km the in the money indic is base on the public leaderboard onli it doesnt reveal anyth about the final stand martin when i open the file it show and what applic are you usinganthoni i believ it refer to grant made when the research was at anoth univers nichola a matlab solut is fine as long you dont includ librari that use patent or undocumentedsecret algorithm apolog will i was on a plane and onli just got your messag will make the adjust this afternoon id also like to congratul the top team and congratul dirk for run an excel competit thank to everybodi who particip and a big thank to dirk for put togeth a realli nice design competit the test label are attach to this post rafael and are fine is also fine as long as the data is deriv entir from the time seri as you say reginald pleas email your submiss to anthoni goldbloomkaggl com and ill have a look apolog i didnt clarifi this with mahmoud befor the launch but we have discuss this offlin this competit requir you to choos five entri that count toward the final result to choos five entri visit your submiss page and click the star next to the relev entri to select it if you do not choos ani entri your last five entri will be chosen by default michelangelo the per cent come from the test dataset eu jin lok the sampl is done random for anybodi interest here the actual solut hi gregth answer will be made avail on the forum i can ask whether the data can be use for publish research if you likekind regardsanthoni hi greg and suhendarth univers doesnt want the data to be use for ani purpos other than for this competit anthoni hi gregit would be nice if the dataset could be use for other work howev if we dont allow competit host to place restrict on the use of their data then we wouldnt get access to it in the first place will post the solut file now regardsanthoni the solut file is attach to this post thank all for participatinganthoni entri made befor we fix the leaderboard were score incorrect i have now rescor the relev entri the error was the fault of kaggl and not the competit organ apologiesanthoni hi cerinapolog for the error they all stem from the fact that the server hard drive fill up ive clear some space for inform were current rewrit the entir site for the heritag health prize you can expect the next version to be faster and includ mani more featur thank for your patienceanthoni hi cerinali is right your entri will count toward the final stand anthoni hi allsubmit from multipl account is most definit against the rule we have done some analysi and found that it happen veri rare howev we are work to put the system in place to identifi and block those who attempt to do it kind regardsanthoni the solut is attach thank all for participatinganthoni harrithank for the thought post the ijcnn peopl agre with you and have decid not to disqualifi shen as mention abov kaggl will soon have the system in place to detect multipl account in real time so that such issu dont aris anthoni i have sympathi for peopl frustrat in this case the competit host decid that the result should stand so we are facilit their decis chris make a good point about the rule be scatter throughout the site we will be sure to address this in futur competit we will also ensur that they are tight enforc for inform a lot of effort has gone into frame the heritag health prize rule final thank for the feedback it discuss like this that will help us improv kaggl kaggl has receiv legal advic after the controversi surround this competit we have been advis that it set a danger preced for us to ignor our own term and condit notabl claus prevent multipl signup we have therefor act in accord with this claus disqualifi those who clear submit from multipl account thank you all for your patienc on this issu and rest assur that we are work to ensur that it is not a featur of futur competit entrant are welcom to use other data to develop and test their algorithm and entri until utc on april if the data are i freeli avail to all other entrant and i publish or a link provid to the data in the “extern data” on this forum topic within one week of an entri submiss use the other data entrant may not use ani data other than the data set after utc on april without prior approv also cover by slate and forb and the wall street journal a coupl of week ago and smarter planet the criteria was that somebodi had to make at least one claim in y be elig to make a claim in y outlier have been remov from the dataset as well as those suffer from stigmat diseas just to clarifi when jeremi say we clean it as much as we can we didnt do much to the claim data on purpos we figur it make more sens for you to make your own clean assumpt rather than have us impos them on you not onli are patient who die in y not in the dataset but patient who die in y are also not in the dataset becaus they didnt remain elig to claim for the whole of y apolog this was an error thank for draw our attent to it the miss valu are for those peopl who have been in hospit for more than two week they should be replac with a you can either do this yourself or download the updat dataset for inform member who have in hospit for more than two week have been group for privaci reason they are rare so may otherwis be identifi the implic of this group is that if you expect somebodi to be in hospit for more than two week you should predict day this group should not have a big impact becaus a member who are in hospit for more than two week are rare about one per cent of member b the evalu metric favor algorithm that accur predict fewer day in hospit on the assumpt that these are more prevent dorofino great idea form a team is a realli good way to learn are you affili with the new york r user group for info ive heard rumbl about them set up a team good luck with this anthoni hi rich just spoke to hpn about this for the moment they dont want to provid general guidanc and ask that you make a request through the contact us form your request should detail the topic of your propos research definit worth make it clear that your just look to publish the method that you use to enter the competit anthoni the year are sequenti we are not reveal what year yn refer to nor whether or not they refer to calendar year for data privaci reason apolog for the miss valu it was an error you can either replac the miss valu with or download the updat data set if your interest in the reason for the miss data see hi bacg daysinhospit refer to y the second year while the claim refer to y the first year not everyth that has a length of stay count as a hospit in fact you dont have enough detail in the claim tabl to calcul daysinhospit the detail has been suppress for privaci reason anthoni hi mbenjam we would have love to releas more detail data but have to be mind of data privaci anthoni have receiv advic from the hpn lawyer im realli sad to say that the answer is no on all account wgn the intent is not to rule out the public of research ive pass on your messag to hpn and a clarif will be forthcom the lawyer are take a conserv stanc on this issu apolog it realli disappoint to have peopl rule for this reason flsdcom i have a meet with them in minut i will be sure to rais this point in respons to ashasho origin question i have sought a reexamin of the issu the hpn lawyer explain that the reason for the hard line is that they have no way to verifi that resid permit compli with us legisl im realli sorri to say that there not more i can do i want to reassur everyon that hpn is work hard behind the scene to clarifi the ip issu it is not their intent to prevent peopl from use standard tool nor to discourag anyon from appli their innov idea to this problem for background at monday launch event dr richard merkin the man behind the prize spoke of the long tradit of innov that has result from past prize he spoke of the longitud prize appar newton and galileo had attempt to solv this problem but the winner was a self educ clockmak from yorkshir napoleon food preserv prize won by a confection and result in the invent of can food the orteig prize to fli nonstop from new york to pari won by the unlik charl lindbergh it is his hope that this prize will spur similar innov to solv one of america most vex problem we appreci your patienc while we await clarif kind regard anthoni this is a sampl of the final dataset but the final dataset is not in the terabyt rang to the best of my knowledg this dataset is on the larger side for medic dataset which tend to be quit small this algorithm will not need to oper in a realtim environ and so there is no restrict on execut time the decis to predict day in hospit was made to make the test dataset richer so we can better sort out good algorithm from bad the logarithm in the evalu metric was chosen to favor model that predict short stay more accur as these are assum to be more readili prevent as for the question of nefari intent i can tell you what i know about dr richard merkin the man behind the prize he is a big philanthropist who devot time and resourc to fund scientif project school and the art in my opinion hpn did not need to put up million to get an amaz algorithm kaggl has found in it own competit that with prize as small as or a chess dvd particip approach the limit of what possibl on a dataset in our communic with hpn we have been told that the million prize is an attempt to draw mass attent to this prize and the issu in general dr merkin want to promot the potenti for medic data mine in lower healthcar cost the prize also serv to introduc a larg number of talent data scientist to medic data final rest assur that hpn are work hard behind the scene to clarifi the ip issu mgomari one issu we have to keep in mind are the tradeoff in releas data for data privaci reason hpn have a granular threshold which theyr not will to breach the data anonym team repres by keleman in the forum are tri to releas cptcode probabl at an aggreg level appar it pretti linebal and releasng daysinhospitali might put this in jeopardi i describ the data privaci consider like a waterb you push down on one part of the bed and it creat a bulg somewher els after may youll be abl to use daysinhospitali and daysinhospitali to predict daysinhospitali ogenex even if we releas daysinhospitali you wont be abl to do a consist check not all length of stay count as hospit as calcul for this competit and you dont have enough detail in this dataset to work out which count and which dont for those who dont know jphoward was kaggl most success competitor befor join the team his tutori give realli clear explan of the tool and techniqu that made him such a success competitor hi jim that is correct for inform the reason for the misnom is that it was day when we sent it to the anonym team but they had to group the day to ensur the requir level of data privaci anthoni sciolist yes team are requir to publish public ashojae the clarif havent been made yet mkarbowski as jphoward keep point out there often a massiv disconnect between realiti and the content of a transact databas see ejlok humor post for even odder record agre see the updat evalu page we intent decid against clean the data so as not to impos our assumpt on particip we want the forum to be tight integr into the site e g to be abl to link to forum post from profil and vice versa yaf is the best net forum softwar out there and integr it into kaggl is more troubl than it worth also moserwar is a brilliant programm so it the type of thing he could put togeth in less than a week realworld data is messi well put up a data dictionari soon quotedaveim serious i understand the need for random and anoym the data but unless they have some way to unrandom it afterward ani algorithm we creat will serv no real world applic quot daveim the data is messi not becaus it been peturb but becaus it realworld data anonym focus on general again not peturb the the nineyear old pregnant male actual exist in the raw data for info im told that this is one of the cleaner medic claim dataset around fjn pi doe not have to be an integ blonchar your correct hpn are limit in what it can releas by the need to protect patient privaci mgomari the answer to both question is yes jesenski you will be abl to use daysinhospitali and daysinhospitali as an input to daysinhospitali i like your think on the use of other data loophol if the answer had been no creativ think cybaea mani thank for a great discoveri after do some dig weve discov that the oddeven observ is an artifact of the clean procedur we have work out a remedi and it will be appli to the dataset that will be releas on may in the meantim it shouldnt make a huge differ to model that are current be develop boegel yes on may we will be issu signific more data dayinhospitali csv will be chang then liveflow i may be misunderstand the question but the competit requir particip to use data from y y and y to predict y no some y patient are no longer elig in y we still provid y patient who arent elig in y becaus theyr use to train on inform man that is not the intent of the rule the hpn lawyer are work on clarifi this at the moment dougi everi member list in daysinhospitali is elig to claim in y so if they have dih they are abov the same will appli for the member list in daysinhospitali and daysinhospitali when we releas those file irwint good pickup thank now fix gschmidt not sure if this answer your question but the geograph spread is limit to the area in which hpn oper southern california i believ as to whether patient chang doctor on may youll have a few year worth of data so will be abl to work this out alexx the hpn lawyer are work on a clarif this will be releas by the time entri can be made on may metaxab the competit was design this way to replic how the model might be use in real life in a real life situat you wouldnt be abl to predict hospit with contemporan claim daysinhospitali is deriv from the claim tabl where a hospit stay includ an inpati stay or an emerg visit note you dont have enough inform to calcul daysinhospitali from the claim tabl hi drew it will be in place by may when entri are accept anybodi who accept the exist rule will receiv the notif via email anthoni you will get some procedur code inform in the may releas i understand the frustrat but data privaci is a prioriti for hpn for generat featur i recommend sqllite though mysql doe the same thing i know jeremi and jeff like cs linq for build model i use r rks we will post a sampl entri with the rest of the data on may trezza and rhm y contain data for a period trezza unfortun not the anonym team have identifi this as a data privaci risk hi allan that becaus some member have had claim suppress in releas come soon well make it clear which member this appli to anthoni hi domcastro can i use r yes can i use weka yes can i use excel yes if i organis the data in a novel way and just use a standard process algorithm such as naiv bay is this ok yes you must preserv the order in target csv releas zip doe supersed releas zip unfortun not apolog for ani inconveni darragh it a list of all member in the dataset no chris just heard back from the data anonym team member have been renumb mkwan you fill in the team wizard when you make your first entri team merger will be grant at the organ discret yes we cant give you an hpn benchmark becaus theyv not tackl this problem befor boegel daysinhospitali contain member who made a claim in y and were elig to make a claim in y daysinhospitali contain member who made a claim in y and were elig to make a claim in y similar target csv contain member who made a claim in y and were elig to make a claim in y to be elig mean to be an hpn member regardless of whether or not a claim was made therefor the member in daysinhospitali are not miss from target csv but rather didnt make a claim in y or werent elig to make a claim in y therefor all member in target csv were elig to make a claim in y so we have an answer for each of these member jesenski by my calcul member appear in daysinhospitali and daysinhospitali but not daysinhospitali perhap you can confirm this figur these member are miss from daysinhospitali becaus they didnt make a claim in y despit be elig apolog if we didnt communic this effect in the descript page danb your right about the select bias but becaus hpn are releas almost no inform on the member themselv there noth to model on for patient without claim protest there noth in the raw data that distinguish a death from a patient that leav hpn for anoth provid ssrc map los to dih is imposs not everi los entri correspond with a dih e g hospic stay one reason somebodi may have dih in y but no claim is if they werent elig to claim in y in which case their y claim wouldv been remov georg there are member in the dataset but you are onli test on member that becaus the extra member arent elig to claim in y or didnt claim in y they have onli been provid to help you train your model tom sf hain jeremi is not the author of the rule he is mere tri his best to point peopl to the section that make the rule as competitor friend as possibl given hpns requir also if you would like to publish your algorithm i strong encourag you to put in a research request use the contact us form is the maximum ive said this befor but i think jeremi tutori is realli excel although it is not focuss on hhp he is hope to get the opportun to do an hhp tutori in the next few month she will be ad in the next releas the intent of that provis is to prevent the data be share with those who have not agre to the competit rule jeff was just refer to the measur he would take to ensur the data isnt access to other jose thank for your dilig on this it difficult for us to give specif guidelin again hpn is just tri to prevent the data from be access to those who havent accept the rule jim it be assess against y hospit bernhard your interpret sound about right to me thank dave the data descript has been fix hi bobbi can you clarifi what you mean by this are you ask if they are oblig to share their model if they finish in first place anthoni hi willem to what extend the result have to be ident for exampl small differ in the random number generat may give differ result although they should be similar they do need to be ident you can give your random number generat a seed to make sure the resultl are the same each time in how much time should the result be reproduc my current best result is a mix of mani model each may take minut to hour to generat there is no rule about execut time the algorithm should produc similar result on a new dataset this doesnt sound veri realist i dont think there is ani way to win this competit without optim for this specif dataset result on other dataset may be veri bad with the given optim probabl veri good result can be produc by the same algorithm after some tune but this is a process that requir a lot of knowledg about the use algorithm and a lot of time and patienc not sure i follow whi this is an issu rememb the mileston prize is judg in a portion of the test dataset that particip have not been given ani feedback on perhap im misunderstand the concern hth antthoni regard the requir that solut be ident willem it would be better to have particip spend time on innov rather than reproduc howev it import to have strict rule so that the competit remain as fair as possibl b yang with regard to the compil issu we can address it if the issu aris for exampl we might start by ensur that the same compil is use for verif sali mali it is except to describ the algorithm and not how it is deriv we are seek clarif from hpn on the inconsist that you describ apolog for the delay regard the requir that the algorithm perform similar on a separ dataset this is best answer by explain the rational behind the rule it is there to catch ani cheat or blatant overfit if your not blatant overfit then your like to be on safe ground hi all not ignor this thread just seek clarif from hpn on one issu anthoni john onli the lowest of the five entri count note for the mileston prize onli one can be select anthoni i have check with hpn and a mileston prize winner can choos not to disclos their method but will not be elig for the mileston prize sorri for the delay on this was just clarifi some issu with hpn is it inconsist as sali mali point out in anoth thread to requir document of the win algorithm be public disclos to all competitor given rule entrant represent it seem that this disclosur will encourag other competitor to use aspect of the win predict algorithm which caus violat direct or otherwis of i iii and possibl iv of that rule rule doe not appli to the extent that it prevent a competitor other than a mileston prizewinn from use code publish by a mileston prizewinn in accord with competit rule and b a mileston prizewinn from compet subsequ in the competit use code for which it was award the mileston prize can you clarifi that code librari and softwar specif are not requir to be public disclos to competitor these materi and intellectu properti appear to be referenc separ from predict algorithm and document chris correct point to jeremi respons in an earlier forum post “on the paper describ the algorithm will be post public the paper must fulli describ the algorithm if other competitor find that it miss key inform or doesnt behav as advertis then they can appeal the idea of cours is that progress prize winner will fulli share the result theyv use to that point so that all competitor can benefit for the remaind of the comp and so that the overal outcom for health care is improv ” will kaggl or heritag have a moder or appeal process for handl competitor complaint from the win entrant pointofview they would not want to be forc through the review process to allow backdoor answer to code and librari which acceler a competitor integr of the win solut kaggl and the hhp judg panel will moder the appeal process can you comment on the spirit and fair of the public disclosur of the predict algorithm document and it impact on competit in particular if the document truli doe meet the requir of enabl a skill comput scienc practition to reproduc the win result then this place the win team at an unfair disadavantag all competitor will have access to their algorithm and research in addit to the win algorithm this rule is in place to promot collabor those who would prefer not to share can opt out of the prize can you provid more detail clarif on the level of document requir by condit mileston winner the guidelin provid by the rule would cover a rang of detail and descript span from lectur note to detail tutori to whitepap to confer paper etc hope this was adequ dealt with in jeremi respons requot abov let me know if further clarif is need can you comment on the reproduc requir for exampl it is possibl to construct algorithm with stochast element that may not be precis reproduc even use the same random seed is it suffici for these algorithm to reproduc the submiss approxim what if they dont reproduc exact or reproduc at a predict accuraci that is wors than the submiss score possibl wors than other competitor submiss exact reproduc is requir correct sirguessalot thank for the pointer it been ad to our issu tracker i must admit we have higher prioriti issu to tackl but well get there eventu just to keep you all in the loop the plan is to announc the mileston prize winner at oreilli strataconf will let you know the exact date as soon as were told full mileston prize rank will be releas after the announc is made the rule do not prohibit oracl data miner hi all hpn are current look for data scientist heritag provid network the sponsor of the heritag health prize is look to hire data scientist to take it data and analyt depart to the next level if you are interest in healthcar join the largest physician group in california and one of the largest in the unit state and use your data mine skill to make a differ in the provis of health care to individu throughout southern california if interest pleas send an email indic your interest to datascientistheritagem com anthoni provision mileston prize winner will receiv an email over the weekend an announc will be made at strataconf on septemb correct libraryrandomforestsetwdcusersantgoldbloomdropboxkagglecompetitionscredit scoringtrain read csvcstrain csvrf randomforesttrainingctrainingseriousdlqinyr sampsizecdo tracetrueimportancetruentreeforesttruetest read csvcstest csvpred data framepredictrftestcnamespr seriousdlqinyrswrit csvpredfilesampleentri csv alec set the random seed is a good idea domcastro your hypothesi is correct your correct shouldnt includ header congratul team market maker and willem great coverag in the wall street journal here for those interest here the footag from the award ceremoni doe be a member of hpn mean you usual refer to an innetwork provid of say lab test unless obviosuli it is some specialti unavail yes can you be a member of hpn and have govt sponsor insur eg medicar medic yes for medicar i can follow up on medic if you like have pass these question onto hpn will respond as soon as i get an answer on octob the judg in their sole discret decid whether or not the document is suffici take account of the comment made on this forum if they decid the document is not suffici they can impel the winner to address their concern in the seven day follow octob if the winner are ask to resubmit particip have anoth day from novemb to rais ani addit complaint the judg panel are experienc academ review hi all we are in the process of liais with the judg well report their decis as soon as we have everybodi feedback we have made a slight chang to the term and condit ad no individu or entiti may share solut or code for ani competit or collabor in ani way with ani other individu or entiti that is particip as a separ individu or entiti for the same competit the forego shall not appli to ani public communic such as forum particip or blog post we are also awar that the rule havent been as clear as we might have like from now on befor you download the data for ani new competit you will be remind that you cannot sign up to kaggl from multipl account and therefor you cannot submit from multipl account and privat share code or data is not permit outsid of team share data or code is permiss if made avail to all player such as on the forum weve reach out to sever team about this issu pleas let us know asap if you have multipl account and weve not reach out to you we are awar that the rule havent been as clear as we might have like pleas be remind that you cannot sign up to kaggl from multipl account and therefor you cannot submit from multipl account and privat share code or data is not permit outsid of team share data or code is permiss if made avail to all player such as on the forum weve reach out to sever team about this issu pleas let us know asap if you have multipl account and weve not reach out to you sound like there a thrive communiti in melb which look to have been the strongest perform citi congrat all one of my cowork said were realli do well if you think of kaggl everi time you see the facebook logo nice for interest we typic see strong metric on kaggl dure holiday becaus peopl have more discretionari time which at least suggest our communiti isnt too busi with famili anoth possibl explan is that peopl have exhaust their travel budget both time and money on holiday travel and need to wait a littl while befor book more travel clear and entertain nice work whi doe lower bound get mention so much more than upper bound ive play with pca befor but never associ plot or mca glad to see an exampl usag and be abl to add these to my toolkit thank you for the associ plot i assum the width of the box refer to the number of tweet refer that that airlin i assum is the proport of comov explain by the first dimens is that correct is it typic for the first dimens to explain so much of the comov ani thought on how to interpret this dimens small nit you might want to chang res to reason i initi assum res stood for residu and reduc the font size for the x axi label on plot that the most interest plot to me but it hard to read the label becaus they overlap this is a nice notebook suggest to make this easier to follow for those who havent yet look at the data itd be great if you ad a section show a few row or possibl even a few exploratori chartshistogram perhap after the load the data section itd also be nice to see the befor and after you preprocess the data ie befor and after the use textmin to format our data section renam the use textmin to format our data to someth like clean the data great i alway look at the top rate notebook befor look at the data becaus the notebook usual give me a sens for what in the data and what i could do with it love it interest that for everyon other than woodrow wilson the name popular monoton declin over the cours of the presid dwight look like it increas in popular dure ww which make sens one suggest is to add year to the x axi label for each chart to make thing like this easier to spot i tweet this script and somebodi repli ask is there a correspond drop in the name frequenc of the lose presidenti candid right after the elect i was think anoth interest extens would be to answer the question what most influenti in determin babi name trend out of presid and first ladi musician that was for longest on the billboard chart in a given year best actoractress in the oscar basketbal footbal basebal mvps nobel prize winner name time person of the year if nobodi els tackl this i might tri it this build off a convers i had with my cowork meghan who said itd be interest to see whether presid or royal babi had a bigger impact on babi name from this page this is great im surpris north america is not higher for sugar the sweet of food was one of the first thing i notic when we move to the us from australia although it could be becaus a lot of the sweet come from high fructos corn syrup which is not captur nice done and fun write style one addit conclus is that real data is messi big data borat captur it best in data scienc of time spent prepar data of time spent complain about need for prepar data interest how noisi the veri earli year are i suspect the s data is veri poor qualiti realli nice script interest to see the temperatur uncertainti chart give a nice visual of when the data start becom more reliabl also nice idea to put dt into a variabl import plot to see it relat import obvious would have been more interest if wed provid more data one suggest is to better label your plot there some good stuff here but it stake a while to figur out what each chart is show i actual look at your code to figur it out i suspect this script will be more popular with some label that make it easier to follow sven have you been abl to figur out an interpret of this chart thank itd be help if you label the chart and possibl ad sub label point to your interpret it may not be use for the reason you mention but it look nice would be cool to see the by citi version i assum you didnt use it initi becaus of the size of the data set btw i assum red hot would be help to have a key it awesom realli nice put togeth bluefool i thought you came out realli well is this a work in progress or is there an error the chart are show up blank for me juanchaco this is neat but itd be easier to follow if you ad a descript between chart at the moment im scan the codecom to tri and figur what each chart is show cool as someon who live in san francisco im curious akshay this script would be more interest if you found a neat way to visual temperatur by countri just a head up that were still work through the winner solut will need more time to befor announc the final result as offici quick updat we will announc the offici result on wednesday march at am et julian respond in the other thread hi all the result on the final leaderboard are now offici congratul to the winner and all involv this is among the hardest and most ambiti competit weve host we couldnt be prouder of the result the competit has receiv some press coverag with a chanc of more to come anthoni this is the photo from the kaggl offic this lunch was one of the highlight of my six year of kaggl not someth i will forget in a hurri minor comment there a typo in the titl prelimnari should be preliminari i onli make this comment becaus it a nice script and i dont want grammar stickler to be put off the typo jeff interest seem like make the system a littl conserv in the handl of new player chang made thank for the feedback cool to get real world sku data but how would i creat a recommend engin with just skus and ie without custom data i guess ill see when you upload your code thank allen implement most of these chang allen jeff or anyon els how do peopl typic prevent rate from becom stale with trueskil for exampl nonact racer maintain high rank i am plan to make the rank appli over a month roll window but am curious if there were other approach such as the inclus of some kind of time decay this version should now be correct the model doe not systemat make money but it onli use veri basic featur barrier weight and rider hope it a good start and somebodi can take it and build on it hi all just a follow up on the request to not share solut as mention abov this is not a legal oblig but rather a request from the host we tri and avoid request like this becaus it limit the learn that come out of a competit our takeaway from this thread is that if there is a confidenti request we will flag it up front to avoid an unwelcom surpris at the end of a competit anthoni rakhlin one thing were experi with is ask host to write blog post summar the outcom from a competit this wont be time becaus itll happen after theyv spoken to the winner and digest the result unclear what recept we will get from host but it someth were test out foxtrot did you find a bug or a mistak in my code or is it that the return just dont look right given how simpl the featur are hi foxtrot i comment out that line becaus i chang the problem to predict the probabl of victori for each hors rather than posit xgb xgbclassifierobjectivebinarylogist fitdftrain dropdftrainwinpositionmarketidaxi so i that not the sourc of leakag im not actual certain there is leakag if you run the code multipl time the return switch from be posit to be negat depend on the traintest split have run it a bunch of time i suspect the expect return is actual negat right but that doesnt happen in this case becaus im predict probabl so the predict are foxtrot nice pickup thank uncom the shuffl the deck line i suspect there more leakag in this analysi becaus the trainingtest split is random and not timebas the way to get around this for this data set is to train on the form tabl and test against the runner tabl or even more use would be to get less anonym data if luke or someon els has it at the everi least race date are valuabl make it possibl to protect against leakag but more general get access to the complet raw data is valuabl everyth one doe to disguis the data destroy inform and limit what a data scientist can do with it some small exampl relat to know the venu might give inform on what surfac the hors are run on a km race at one racecours could be a straight wherea at other cours it could involv a turn this might impact the perform of specif hors and cant be account for when the variabl is venueid final for boost engag with the data set anonym data is less fun to play with than richer raw data i revers moodi bluefool downvot by upvot i dont have databas write privileg chris i built a model to project out my sep and oct result shame there no oct race accord to my model id finish first queue xkcd william nice kernel look like your predict author base on the number of comment subject etc what the think behind predict who the author is i was think it might be interest the predict the number of comment a proxi for how interest the articl is also i was look at your error chart and your comment that it seem like we are better than chanc curious how your measur that i tri to read it off the chart but couldnt see where it came from william fyi that was quick your code is much cleaner put pressur on me to go back and clean up mine this was realli just a quick analysi if i had more time id have actual look at the word be use in the post rather than guessingr on memori e g i probabl shouldv includ rnn i could also have includ packag name tensorflow kera etc in fact anoth version of this might look at the packag that are most common use by winner ps thank for the bug report on the file download well look into it i was wonder whi the data set had download jordan look great now that the data is in csv format i featur it write a first exploratori kernel to show peopl what in the data is typic help for drive drive engag from the communiti sound good i was think itd be fun to appli trueskil to this dataset i use it here id like to do it but probabl wont have time over the next few week unfortun margin revolut featur will novak analysi of their post if there are interest find made on techcrunch data ill send it across to techcrunch theytheirread may be similar interest david i have been abl to creat the rds file libraryggmap mapdata getopenstreetmapbbox c colorbw scale round saverdsmapdatafilestpet rds how do i creat the text file also whi arent you use osm file the open street map xml file format rather than use ggmap to export to txt or rds ive spent a while tri to import osm file for plot in python but with no success wonder if you also found this difficult ps this is for my gps watch data so the coordin abov are not the chicago coordin anoka thank my bear calcul is incorrect my next step with this notebook is to debug it how would i use np raddegnp arctan anoka i figur out my problem i was feed latitud and longitud into the calcul bear function in the wrong order feed in lat as long and vice versa ad your more eleg bear calcul formula as well thank ryan appreci you rais the concern i want to share kaggl perspect first off intern we are incred excit about the launch of codeon competit it the biggest chang weve made to competit sinc we launch in it increas the rang of competit we can run includ time seri competit such as this one reinforc learn competit and competit on larger data set on this competit specif this is the first step in what we hope will turn into a bigger relationship between kaggl two sigma and the communiti this first competit is aim at test out the concept of codeon competit gaug the communiti interest in financethem competit and get a sens for the type of signal the communiti find on a typic financialmarketsrel data set rakhlin regard your comment about anonym and the use of an api these were primarili driven by kaggl the anonym is in place to discourag peopl from look up the answer the api allow this to be a proper time seri competit we tri to make our competit fair this mean make it difficult for the rare particip who is inclin to circumv the rule ultim our goal is to give our communiti as mani opportun as we can this mean a mix of commerci competit research competit playground competit get start competit and most recent open data set ultim the communiti will select what interest them and kaggl will continu to offer the thing that reson anthoni did you tri the my submiss tab on the left hand side dashboard you could also look at some of the weather data and kernel that other have share hi jacki perhap this dataset is a good start point total agre with frustrat express here neither kaggl dhs nor the communiti want geo restrict in this case it a legal restrict dhs work hard to allow ani intern particip albeit without prize money from kaggl perspect we prefer to host a competit with a geo restrict and make the opportun avail to the communiti than not host we think this is a better albeit imperfect outcom i understand and agre with frustrat around nonus citizensperman resid be inelig for prize money none of kaggl dhs or the communiti want this restrict the reason for the restrict is the america compet act dhss legal team work hard to find a way to allow intern particip at all albeit without prize money while not violat the act from kaggl perspect we had a choic host this competit with a geo restrict or not host at all this was a tough choic host the competit meant run a twospe competit where some are elig for prize money while other are not do this violat our desir for meritocraci where we offer equal opportun to everi communiti member regardless of educ background and of cours countri if we dont host then we dont expos our communiti to one of the most interest and valuabl dataset that ever been host on kaggl as weve seen over the past five year with the rise of deep neural network the avail of dataset allow us to push forward to scienc of machin learn not expos this dataset to our communiti meant depriv our communiti and the machin learn world more general of a chanc to push forward the scienc with a novel and challeng dataset we decid that it was better to make the dataset avail mani in this forum have said this was a mistak this was a challeng issu i hope this at least give you a window into our think anthoni kitefoil jame those speed are a actual littl slow i will upload my latest data soon but that knot upwind and knot downwind ive improv with train and there are kitefoil who are much quicker than me rounakbanik nice dataset can it be updat to be current if you can make it current and updat this notebook i will send it to the ted team we did this with margin revolut and it end up get featur on their blog who know mayb the ted team will be realli interest and can highlight it in some way rounak apolog i miss that youd updat this i will send to ted finger cross they will pick it up kamil and hamachi i actual dont think hamachi expect are too high have demand user is healthi it push us to improv our product were aim to make kaggl kernel into the lead cloudbas workbench for data scienc not just a free comput environ were migrat onto gcp and make some architectur chang that we expect will make a big differ stay tune anthoni we have spent consider time discuss the situat intern here at kaggl and with zillow we thought kaggl was put suffici emphasi on zillow elig criteria by post it on the competit overview page it clear from this thread that this nonstandard elig rule was not suffici public to the communiti as an acknowledg of this we are make the follow adjust we are reinstat the affect team so they will be elig to receiv point and prize money we are ad supplementari prize if need these prize aim to avoid penal team that would have been in the top three team if we didn't reinstat team who aren't in complianc with the elig criteria for exampl a team that in fourth place on the final leaderboard will be award nd place prize money if the nd and rd place team don't meet the elig criteria note the elig criteria for round two remain unchang past below for conveni we will soon be reach out to everyon who place in the top team to verifi their employ or institut affili to ensur complianc with the offici competit rule anybodi who work for a compani that is referenc in the elig criteria will not be accept into round two if you were on a team with one or more member who are inelig you will still be accept to move forward into round two but the teammat who doesn't qualifi will not in the futur we will make a bigger effort to make nonstandard rule clearer post them as a pin forum post for exampl and invit question and clarif but pleas also rememb that the rule are import ultim it is each kaggler respons to read the rule befor choos to particip nobodi want a situat where kaggler are put consider effort into a competit that they are inelig to particip in anthoni elig criteria member of the follow entiti are not elig to particip in either round of the zillow prize contest ani commerci entiti that engag in the sale valuat or analyt of residenti or commerci real estat ani entiti that offer servic in the leas and properti manag space includ vacat rental and ani entiti that monet residenti real estat relat data offic director employe and advisori board member and their immedi famili and member of the same household of sponsor kaggl and each of sponsor and kaggl respect affili subsidiari agent judg and advertis and promot agenc in addit you are not elig to particip in the zillow prize contest if you are a a resid of a countri design as an embargo countri by the unit state treasuri offic of foreign asset control see for addit inform or b are an individu that appear on the unit state treasuri offic of foreign asset control special design nation and block person list see for addit inform bojan total agre that we dont want to incent kaggler to hide person inform in this case nobodi will be remov from the round one leaderboard becaus of person inform they have share and everybodi will have to volunt their affili to be elig for round two that remov ani discrimin base on public share user inform or wire magazin profil competit with elig criteria are quit rare so we havent figur out all the nuanc cant promis we wont make mistak in the futur but we aim to keep improv we spoke about that option in connect with the tsa competit it not clear that it was legal in that case this question is move into a more general discuss of elig criteria if we want to move the convers to a more general discuss of elig criteria i suggest we move it to general the elig criteria still appli to entri to round so onli team that meet the elig criteria will be accept into round have you tri trueskil befor it should be more power becaus from memori it doesnt assum a fix standard deviat so it take into account the certainti about a rate when adjust rate after a game itd be interest to compar the perform of elo vs trueskil there a nice python implement of trueskil love this stori tenac count for a lot well done ryan on monday numerai announc that they were give away their cryptocurr to kaggl user with a rate abov novic and account creat befor march this wasnt done in partnership with kaggl we had no idea it was come follow that announc there has been a massiv increas in the number of login attempt to the kaggl websit these are attempt to break into kaggl account in order to claim the cyptocurr airdrop this is both stress our system and put kaggl account at risk as a result we have remov the abil add as your websit url forc a password reset for anybodi whose websit was set to sent an email to the numerai ceo let him know that this has happen if you find that your password has been reset pleas go through the forgot password flow anthoni im alway happi when the kaggl credenti give our communiti opportun that they wouldnt otherwis have in this case the way offer was structur effect creat a bounti for hack kaggl account and the fact that we had no notic meant that we couldnt think through the implic and prepar were go to look at chang to the kaggl login flow this week numerai has suspend the kaggl airdrop nathanforyou nice featur note these communiti guidelin are replac by revis guidelin avail here the kaggl communiti has a lot of divers with member from over countri and skill level rang from those learn python through to the research who creat deep neural network we have had competit winner with background rang from comput scienc to english literatur howev all our user share a common thread you love work with data as our communiti grow we want to make sure that kaggl continu to be welcom to that end we are introduc guidelin to ensur everyon has the same expect about discours in the forum be patient be friend discuss idea dont make it person threat of ani kind are unaccept lowlevel harass is still harass the kaggl team determin whether content is appropri if you see someth that violat these guidelin you can bring it to our attent use the flag option on messag and topic if you have a serious concern you should report it to supportkaggl com all report will be kept confidenti sad to share that kaggl master vlado tomecek pass away on sunday octob i receiv the note attach abov from dana vlado sister i rememb sit in on his call with draper lab the competit that he won his solut involv a tremend tenac and creativ in our exchang dana mention that data scienc was vlado passion and main focus on behalf of the kaggl team we wish vlado famili well and we will miss his presenc in the communiti justinminsk this is nice fwiw i suggest updat the titl to someth like identifi the best colleg wine the kernel is more fun than the titl suggest faraz has this been address if not send me a privat messag with a link to the comment and ill make sure we take a look asap i'm a longtim competit lurker whos final muster the courag to join a competit 😅 i pick this competit becaus i find the topic interest if i was go to colleg today i'd probabl pick biolog and also as a chanc to tri out googl automl vision kaggl is part of googl now which give me exposur to some of the technolog that are generat excit insid googl automl is a tool with a lot of buzz i hadn't paid much attent to the autom machin learn tool amlt until the kaggleday competit in san francisco this past april two amlt googl automl and h end up particip in the hackathon and get top perform here the googl ai writeup of their perform while i believ it requir human creativ to do problem setup and domainspecif featur engin kaggleday left me wonder whether human need to be do generic featur engin architectur select pick activ function and set learn rate i plan to share detail of my experi and experi with googl automl vision in this thread i'm also open to suggest from other in the communiti for thing i should tri but bear with me if i take sometim to respond like mani other kaggler i have a busi day job first few experi i start with the imag creat by xhlulu kernel thank you xhlulu in that kernel xhlulu convert the data to x rgb jpeg by default googl automl random split between train valid and test and the predict output label and confid score for each imag i defin a threshold and it output all label and confid abov that confid threshold i made predict for both site and then pick the label that had a higher confid score across both site below is a tabl of my score note auc is the accuraci metric that googl automl vision report haven't put the time into figur out what it mean but i assum it probabl treat each class as binari onoff and then aggreg up train hour resolut number of imag extens automl auc public leaderboard score jpeg jpeg jpeg jpeg i then made small modif xhlulu code to creat x rgb pngs to test the impact of higher resolut imag train hour resolut number of imag extens automl auc leaderboard score png png png png png is my best score it a result of a poor thought out experi so i'm surpris it my best score i train a model for hour that includ the control imag but after inspect the testset predict i realiz my model was often pick label which are control label that don't appear in the test set i end up filter the testset predict to go to the next most confid label when a label was predict train hour resolut number of imag extens automl auc leaderboard score yes png yes png yes png yes png yes png yes png googl automl vision allow me to defin my own train valid and test split for my next experi i'm go to start use this featur i'm go to defin my own split to includ the control data in train but not valid or test to allow the model to use the control data for train but to discourag the model from predict label on the competit test set split by experi rather than random experi with googl automl vision i'm get pretti decent result consid how naiv my model are i'v found googl automl easi in mani way to get a basic model all i need to do is upload the rgb imag to googl cloud storag upload a csv file with a pointer to the gcs bucket and the target label there are a bunch of limit with the product my biggest issu so far have been inabl to add other metadata e g would like to be abl to add metadata on control plate id and posit on plate i can probabl get around this use a postprocess step but it'd be nice to be abl to add this metadata into the singl model there isn't a featur that allow batch predict on a larg number of imag i work around this by hit the predict api time to generat my submiss file the model evalu page is not veri help for debug model perform i had to use a lot of hour of train to get good result you can see that the model keep improv with addit train time this mean i have to wait a long time and spend a lot of money can't chang the loss function base on the forum it seem like the loss function might be an import set for this competit there are some more minor frustrat that i had to workaround which i'm happi to share if other are plan to tri it out and want to learn from some of the friction i encount lkhphuc unlik you need anywher near as mani train hour if your not use automl the upsid of automl is that you dont have to set ani hyperparamet and still get a decent model one of the drawback is it so comput intens xhlulu i actual regret convert to png it would have been a purer comparison if id also use jpeg i tri run the px convers in a kernel but i didnt have quit enough comput time let me find out how i can share the px dataset with everyon xhlulu unfortun cant share the dataset through the dataset platform thatd allow user to access the dataset without have to accept the competit rule apap im pick the site that has the highest confid score across the two site for my best model lb my predict for site and site agre of the time not sure sorri zaharch sorri for the delay i want to see if automl could be run out of kernel turn out it can the hour limit for kernel is not an issu becaus my kernel kick off a hour train job and then stop my next step is to share my infer code which send the test imag to the train model and generat predict label automl doesnt give architectur or hyper paramet recommend it just creat an end point that you can send imag to and get back predict label and the model confid in that label ttylacm dont know what hardwar googl automl is use under the hood it invis to the user good chanc it use tpus though ratthachat thank look like your a few place ahead of me on the leaderboard watch out i have a few new idea to tri 😏 oop i made a chang that introduc an error in version version work im current commit a version which should work as well ok now also ad the code for do infer use googl automl i didnt do my strongest model to avoid mess with the leaderboard i pick a model that perform a littl below to strongest perform kernel to chang model all one need to do is chang the modelid ankitkp giuliasavorgnan and jiangkun improv reliabl of kernel is a big push for the team i share this thread one question that came up are you all use gpus or just cpus jiangkun i dont follow what doe that mean manag to jump to by exploit the structur of the structur of the data mention here zaharch im realli interest in see this as well i am curious to tri tpus sometim and would be interest in summari of your experi also curious if you were abl to realiz a perform speedup when use tpus nice thank for share realli love this writeup particular how you step through the differ thing you tri and how they map to improv in your score veri easi to follow deep there is a mistak in the dataset san francisco is list as squar mile when it actual squar mile also itd be help to have a descript of the data includ where it came from and to have the area column as an integ or float rather than a strong im go to add a pointer to the recov case dataset in the pin dataset thread sasrdw this dataset seem to have a decent amount of what your look for that version is three year old so may be worth updat although at a glanc mani of the measur are updat pretti infrequ this look like a nice version of that dataset ad it to the data share thread davidbnn post a dataset of global weather condit at i wonder if googl trend data might be interest perhap search for hand wash or hand sanit in a particular citi might correspond with a lower transmiss rate or even just search for covid in a particular citi might correspond with a lower transmiss rate indic peopl are take the virus more serious in that citi hannesmarai im not an epidemiologist but at a glanc this look realli impress i had a quick look and saw a few place where your system seem to be answer the prompt this seem to give an answer to the persist on surfac question this seem to give an answer to the immun respons question are there mani place where you think your system is accur answer the prompt are you in a posit to give a bit more background on what your system is do savannareid i would also be realli interest at a glanc and to my untrain eye hannesmarai system look realli impress jaimeblasco upload one for school closur i assum countri level data is still like use if you can match the file in that dataset i can like updat it on their behalf otherwis i suggest you just creat a new version nice mind also share it in this thread nice to have all public dataset in one thread as i understand it this has been an influenti preprint show the relationship between temperatur and humid and transmiss rate it look at chines citi and find that a degre celsius increas in temperatur lead to a drop in r and a increas in relat humid lead to a drop in r as i understand it r is a a common use measur by epidemiologist known as the effect reproduct number and is the averag number of secondari case per infecti case if r is greater than then an epidem will spread if it less than it will die out my understand is that the estim of r for covid is in the rang of cpmpml nice dataset do you mind ad a pointer here want to have all extern dataset on the same thread the economist did an analysi of tourism flow tofrom china a few week ago could look at the dataset they use as a start point absolut agre complet with paul guidanc big goal is to product find easi for the medic and health polici communiti to digest this is a nice notebook itd be help if you also link to the paper as well as provid the excerpt ajrwhit this is veri use i find key phrase most use part of your notebook few request can you print more key than key phrase when your search return more result e g chronic respiratori diseas can you print key phrase for addit you look at risk factor e g hypertens can you use your approach to look at word like weather temperatur and humid that is shown to have an impact on transmiss rate e g referenc here itd be interest to see what your approach turn up get estim for r and r from the literatur would be great one problem with those snippet is it doesnt have the actual number we have been ask by our health polici collabor to put togeth a summari page that surfac the most use find from the kaggl communiti the research on covid is move quick make it hard for virologist and the health polici communiti to stay on top of the latest they are work long day and need inform present in as clear and concis a format as possibl we'v format the summari page to be digest to them the page is current organ into three section find tool and dataset im go to focus on the find and tool section sinc theyr most relev to this challeng find map to the ten task issu by the white hous offic of technolog polici address open question about covid our goal is to help answer as mani open question as possibl and repres the state of knowledg on each of those question find should be focus concis extract quot and number out of paper and also provid a link to the under sourc it help if you can produc find that map as close as possibl to the format of what current present on the summari page some of the most impact work so far have involv simpl method like string match and regular express for those work on tool googl scholar and ai semant search are alreadi matur product if you are build a tool make sure your tool add valu beyond those product if you have contribut that you think we should be highlight on the summari page pleas email me at akaggl com or share on this thread also feel free to ask ani clarifi question on this thread most of the dataset have come from the forecast challeng howev for those interest ill provid some guidanc on what dataset are help it use to share individu dataset with promis signal it even more use if you can join to other relev dataset that virologist and the health polici communiti can analyz and by all mean use ani dataset to tri and produc find if you do make use find pleas document them in a clear written notebook that easi for other to follow we have been ask by our health polici collabor to put togeth a summari page that surfac the most use find from the kaggl communiti virologist and the health polici communiti are work long day and need inform in as clear and concis a format as possibl we'v format this page to be as easili digest to them as possibl the page is current organ into three section find tool and dataset im go to focus on dataset and find sinc they are most relev to this challeng dataset so far the forecast challeng has done a nice job of surfac potenti use dataset the most valuabl contribut have been join those dataset to creat valuabl resourc for test which factor impact transmiss shout out to davidbnn who join each region in the john hopkin univers data to the nearest weather station find these clear written notebook that help show which factor have an impact on transmiss rate call to action creat enrich dataset that would allow research to easili test which factor impact transmiss these dataset should be well document and kept updat write simpl notebook that show the impact of factor on transmiss use whether or not you find a correl if you have contribut that you think we should be highlight on the summari page pleas email me at akaggl com somethingkag this is not a templat but ajrwhit notebook doe a realli nice job address a hand of the subtask it direct address the subtask the informationdata is veri clear present skylord nice are you updat this daili if so ill includ it franck you should absolut be open to look at extern data sourc to answer the question one nice thing about start with the paper is we build up a pictur for what has alreadi been studi and where the gap are no this is not a supervis machin learn competit we are host a supervis covid machin learn competit at ajrwhit realli nice said ken ad to summari page thank nofoosport at a glanc your notebook look realli nice like have some folk with medicalpubl health background start to help out with curat from tomorrow so plan to take a close look with them ken thank for group the notebook by task make them much easier for us to process thank mike will take a look today remanuel pleas add as a kaggl dataset that make it easili access to those write notebook for those interest safegraph have a realli interest human movement dataset it locat data from million of anonym smart phone it current on aw but they can move it to gcp to make it easier to use from a kaggl notebook if there suffici interest you can get access by fill out this form i receiv quit a bit of feedback from the healthcar and health polici communiti about our page summar communiti contribut as well as includ the titl of the paper itd be help if we also share the name of the journal in the summari tabl the name of the journal is a proxi for the the qualiti of the paper a result publish in jama or the lancet carri more weight more challeng is to also classifi the type of evid that a studi is base on there a hierarchi of evid and it help to know what evid was use to draw a particular conclus here are a few refer that explain the hierarchi of evid ajrwhit can you updat this notebook to includ the new data dump also is it easi for you to add the journal name to the articl titl column per this guidanc if so itd be great if you could put includ it if you can id format itd be great if you could put it outsid the link to make it easier to distinguish from the titl persist of coronavirus on inanim surfac and their inactiv with biocid agent the journal of hospit infect mikehoney a realli nice tool for explor the data but doesnt direct give me the answer to specif task without me choos my own filter mlconsult the chloroquin section had most relev articl so ad it to the summari page under effect of drug be develop and tri to treat covid patient skylord ad here btw you may want to crosspost to this challeng this seem to be an import paper is it in the cord dataset zohrarezgui nice idea to pull down clinic trial data but the dataset has no inform on the therapi be trial though is it possibl to pull more inform about the natur of trial mlconsult nice ad can you pull how larg the differ is between and also what doe relscor mean nofoosport this is great ive start ad your answer to the contribut page a coupl of thing that would be help start order by date i like to present result chronolog on the contribut page also help if you can start break out answer to subtask a littl more for exampl for this subtask there rang of incub period for the diseas in human and how this vari across age and health status and how long individu are contagi even after recoveri itd be help to have paper and excerpt that just answer rang of incub period for the diseas in human how this vari across age and health status how long individu are contagi youll notic im start to break up even the subtask on the contribut page to make them more digest this is feedback were consist get our audienc is veri busi and want thing as scannabl as possibl anoth request it even help if you can break out subtask into sever compon as an exampl take this subtask rang of incub period for the diseas in human and how this vari across age and health status and how long individu are contagi even after recoveri itd be help to have paper and excerpt that just answer rang of incub period for the diseas in human how this vari across age and health status how long individu are contagi im start to break up even the subtask on the contribut page to make them more digest the feedback were consist get is our audienc is veri busi and want thing as scannabl as possibl i just share this feedback on nofoosportss excel notebook but want to make it more broad visibl also a nit but you have group two subtask accident preval of asymptomat shed and transmiss e g particular children and season of transmiss csheesun were typic find result easier to read when contributor are submit one notebook per task we had anoth call today and receiv anoth round of feedback first off were get good feedback that is head in a realli help direct the big issu is still that there limit signal on how strong the under paper is sinc covid is so new and most articl are in preprint journal medrxiv and biorxiv that doesnt give much signal i think this is consist with what savannareid has been say they are interest in whether we can mine other signal from the dataset i think were go to get more feedback in the next week ill provid it when i get it but an exampl that came up was pull out the number of particip in a studi the more particip the more weight should be given to a studi for eg this didnt come up on the call but i was also wonder about author previous public record might be a use signal would love to add a column that a proxi for public qualiti by the end of the week if anyon can crack the code on that davidmezzetti start a discuss thread on the topic so suggest the convers continu there anoth bit of feedback we receiv is that the health polici and medic communiti are interest in tool that might help them get to answer more quick themselv the dream tool is a questionansw engin that can reliabl answer freeform question but there interest in see other tool as well that might go beyond what thing like googl scholar and other exist search engin can do if anyon build a tool that they think meet this criteria pleas either share it here or email me ultim our goal is to be as use as possibl to health polici decis maker so you are free to search pubm if you think itll improv your result amaz youv made such quick progress look forward to tomorrow updat a recent paper suggest a potenti link between transmiss and air pollut paper suggest that explain region differ is itali between transmiss rate mlconsult thank for share paultimothymooney own look at tool be develop hes go to take a look at this if he think it promis itll definit need to be stood up as a web app feedback were get is mani of the end user are not technic enough to fork a notebook nice work mlconsult paultimothymooney as an fyi this is now a web app one of the main bit of feedback were get about the result were present is that decis maker would like to better understand the strength of the evid in a studi davidmezzetti has made a realli nice start by includ level of evid i spoke with stelio serghiou and byron wallac byron actual use nlp to assess the qualiti of clinic trial in speak to stelio and a few other im get the feedback that pull out the under studi design might be even more help than level of evid for each studi type there are measur that can help proxi the strength of the evid below is a tabl that attempt to map studi type to measur these are the thing that stelio look for when hes assess the qualiti of a paper itd be great if you can attempt to extract studi type and the correspond measur to indic evid strength this appli to those present task answer in tabl as well as those who are build searchquest answer tool level of evid studi type measur to indic evid strength i meta analysi i heterogen and tau squar heterogen number of studi ii experiment studi random random sampl size loss of follow up length of follow up iii experiment studi nonrandom random sampl size loss of follow up length of follow up iii observ studi prospect cohort sampl size loss of follow up iii observ studi retrospect cohort sampl size sampl method how was the sampl captur iii observ studi case control sampl size select bias for control when control come from a differ popul from case observ crosssect sampl size sampl method how was the sampl captur tempor did the exposur e g covid come befor the outcom e g hospit desir or after undesir iv observ case seri case studi note savannareid point out that this doesnt includ modeldriven result like those use to estim thing like reproduct rate for modeldriven result measur of evid strength are like simpl size some measur of model fit and some inform on where the under data was drawn from aravindmc nice work weve list it on the contribut page as paul said were get a lot of feedback about includ measur of the strength of evid share some of the feedback we are hear here itd be great if you could extract and includ some of that metadata in your search result arturkiulian absolut the overal object is to product someth use to the healthcar and health polici communiti if those thing help you produc more relev result then pleas do use them ajrwhit and davidmezzetti and other look to contribut the the contribut page savannareid hand code the design and sampl size for all the risk factor you can see what it look like here she also reformat the tabl can you tri and pull updat your risk factor notebook for the new data dump and can you tri and pull out design and sampl size also if you can follow this tabl format we can dump it direct onto the page we now have a more autom process for generat the tabl datestudystudi linkjournalseverefatalitydesignsampl hi all we now have a team of medic student help to curat the result that go on the contribut page theyv been review the result of notebook that focus on the transmiss incub and environment stabil task as a start point each student own curat a tabl for each of the subtask the goal for these review is to to come up with a standard format for those subtask just like savannareid has done for risk factor this will give you a target format to produc your result in give feedback to author of notebook that are review on how to make your algorithm more relevantaccurateus the goal is to continu to popul the contribut page which is gain a nice follow attract k uniqu visitor in the last week up from the week befor and when we have a meaning amount of coverag and a solid process for keep the page uptod as the new paper come in with less manual curat than we are current do were go to aim to start publish the updat literatur review with a lead medic journal so that it more visibl in the medic communiti encourag those of you who are interest in contribut notebook in the format help for the contribut page to join the slack channel with the team of medic student here is an invit link im go to attempt to pass on feedback im hear in this thread but join that slack channel give you more direct and interact way to hear from the expert who are review your work final if you want to see the curat process in action and the notebook that are be review you can check out this googl doc pleas let me know if you have a notebook you want review for a particular subtask and i can add it to the queue the queue is maintain on the master sheet tab some consist theme in the feedback so far often algorithm are pull snippet from the abstract when the key number find or result is in the full text of the articl with the summari in the abstract be too high level to be use often algorithm are pull snippet that are a refer to work from anoth paper rather than work that is core to the paper be cite this challeng is get the attent of the health polici and medic communiti who are work night and day to better understand covid we have heard feedback that an aipow literatur review is a power way to synthes new research i have start a new thread outlin how you can feed into an aipow literatur review that thread aim to be a clear and direct way you can make contribut that will have impact i am unpin this thread to direct convers there nice updat one chang that would be help is if you can break out studi studi link and journal into three separ column per this guidanc that will result in less manual curat to get the result into the format for the contribut page sasrdw thank for start this thread david ani sens for how the imh model would have perform in these competit curious where would it have place wow this is realli nice interest that the kaggl communiti keep get stronger vs ihm i guess there are three possibl explan the kaggl model are get better as more data come in more empir and less theoret than ihm the kaggl model are get better becaus peopl have more time to work on them ihm are better at forecast further out sound like you believ it david also realli curious about week is the trend continu or are we plateau vs ihm assum you left it off becaus we dont have enough data for week yet david funnili enough i was just look at his dashboard i assum theyr all output data in a standard format if hes abl to put them on the same dashboard is it easi for you to benchmark the perform of those other model as well were tri to figur out whether the top kaggl model are like ad valu beyond the standard epidemiolog model were consid launch a longer run competit and combin top perform kaggl model into an ihmestyl dashboard for interest section of this paper discuss the metric use by the epidemiolog model communiti i did some more benchmark i compar the current leader for week and with ihm and lanl model across three loss function mae rmse and rmsle im onli compar the forecast for fatal for us state in week and the kaggl leader clear win on rmsle but perform a lot less well on the other loss function cpmpml regard your comment about the kaggl comp optim differ thing the same is probabl true for ihm and lanl on mae and rmse it look like you basic get good score by get new york and mayb new jersey correct they account for of fatal and get new york right involv predict a data revis nyc revis their fatal measur to includ probabl covid death on april this revis start includ peopl who die but never receiv a covid test but had covidlik symptom this chart come from invers excel notebook that show chart for lanl ihm and two kaggl ensembl on a state by state basi cpmpml the basi for the claim that get new york right is most import come from the observ that new york account for of the total absolut error for the kagglelead and ihm model so that state is drive most of the error i havent had a chanc to take a close look at what happen with your model your number are realli impress across the board unfortun i have to return to my day job now though this was a small weekend project that i did dure my daughter nap david i bet if you ask ihm and lanl if the benchmark is fair theyd also have complaint as well optim for a differ loss function etc nonetheless i was curious to see what would happen if i benchmark across a rang of metric that the model didnt necessarili optim for i was hope it might tell me someth about the robust of result to explain whi this setup lanl onli doe forecast for us state so need to restrict to this subset to includ them and lanl and ihm model updat dont perfect correspond with our competit date nice present thank for share ive put togeth got a notebook that benchmark the perform of profession epidemiolog model and compar them with a strong kaggl model i was curious to keep track of how the kaggl model compar with the profession epidemiolog model for now im onli track one kaggl model the previous week leader to avoid make a select expost so the week model im use is one of sasrdw and david model it current do realli well although interest it make veri differ forecast from the profession epidemiolog model i have creat a benchmark panel that includ actual and the forecast from differ model at differ point in time here the notebook that generat the benchmark panel encourag other interest to play with it either add one of your model or play with some kaggl ensembl can download submiss file from public notebook mrisdal you are domin enough leaderboard i need to pick up my game mlconsult couldnt find a task that this address but includ on the contribut page becaus it come up as an open question recent ha id not known about this for other donovan weve look into this and it turn out that a bug with our process meant that we hadnt receiv the past few week of queri weve found your email and you will receiv a respons short as will other who slip through the crack apolog to you and other who have not receiv a respons as a result of this error thank for the nice wish of cours kaggl wouldnt exist without a brilliant communiti of data scientist who can solv realli challeng problem look forward to see what we can do in hi guy glad you like this dataset remind me of the rta data which was realli popular on the ip question when no rule are explicit state the kaggl term and condit prevail specif claus by accept an award you agre to grant a licens to the competit host to use ani model use or consult by you in generat your entri in ani way the competit host think fit this licens will be nonexclus unless otherwis specifi anthoni pham you do not have enough detail in the claim data to reproduc the dih proper youv like reproduc dih from claim data as accur as is possibl jason the anonym guy have withheld this inform intent to make the data set more secur sorri further to will point those who follow the netflix prize will rememb the jump from the simon funk discoveri darragh i pass your question onto hpn here the repli is there a delay between the schedul of the surgeri and when it take place yes but that is just a matter of schedul not someth forc by the govern it would also of cours depend on how urgent the surgeri is just to clarifi the result page will show the leaderboard for all competitor regardless of whether they use futur inform or not we will make an honour mention to the lead competitor who doesnt use futur inform howev their entri will be audit kaggl is current develop a leagu tabl that rank competitor when it come to this competit your posit on the leaderboard which is indiffer to the use of futur inform will be what count toward your kaggl rank inform can offer an awardhonour mention to those who dont use futur data howev the kaggl leaderboard will not seper those who use futur inform from those who dont uri you rais an interest point howev is five month long enough for somebodi rate to move enough for you to notic this given the way the competit has been setup there no way to prevent peopl from use futur data even if the winner present a model that doesnt includ futur data they may have overfit to replic the predict of a model that doe includ futur data in theori yes the problem is that there no way to be certain that the winner didnt use futur inform even when we check the win model it possibl they have use a model with futur inform to probe the test dataset this first chart how the lead score has chang on a daybyday basi the red line show the elo benchmark and the blue line show the lead score the elo benchmark was outperform within hour which is whi it alway abov the best entri interest to see some recent progress after a period of stagnat well done philipp my guess is that ani major improv from this point on will be the result of somebodi tri someth quit differ this chart show the number of daili entri higher earli but seem to have stabilis at around per day happi to put up other chart if peopl have request philipp there certain a largish gap between the top five of cours this is pure indic what realli matter is the score differ on the final leaderboard philipp great sugges weve got a stack of featur we want to implement but ill put this in our long term wish list i tri pute up a general forum for such discuss but found that it was veri light use featur in the pipelin includ fix bug or incomplet featur on the new site upgrad to kaggl infrastructur to allow us to score veri larg entri kaggl rank system an elo for kaggler base on microsoft trueskil extend social network featur includ live chat recent activ feed philipp competit analyt sugget and possibl some other data viz toolscompetit in the pipelin includ predict social network connect predict the like success of grant applic for a larg australian univers forecast travel time for freeway in melbourn australia predict prostat cancer from a high dimension dataset subject to ethic approv diagnos breast cancer from mammograph densiti imag also subject to ethic approvalani other suggest ani thought on what our prioriti ought to be jasonlt are you think along the line of karma point for particip in forum discuss or would you like the forum to be more of a qa with stackoverflow style ratingsi like the idea of guest blog post and communiti tutori after the chess competit end some might be interest in post detail of their workflowmethodcod lt the general forum has been taken down for the moment when i get a littl time i will attempt to reviv it and start encourag peopl to use it philipp it may not matter that peopl onli compet in a hand of competit becaus each competit contain quit a lot of inform unlik a singl chess game particip are compet against mani player regardless well do plenti of test with trueskil befor implement as for the point system point seem a littl abitrari i like the idea of rate that account for the strength of a competit particip i tend to agre with your point on forum particip point the stackoverflow approach seem like a nice way around the problem there are lot of direct we could take kaggl but for the moment were focus on competiiton you can email me the file if you like anthoni goldbloomkaggl com id be happi to take a look at it dielson good pick up denni is correct the date format doesnt matter what is import is that you put the correct data in the correct cell josei notic that you are now on the leaderboard it can take a few minut befor you show up anthoni edith thank for the feedback we agre with your comment and we are work on make the term more competitor friend wu wei a rout is made up of sever loop a figur of mean that per cent of the loop in the given rout are give suspect read mgomari the differ between and is count as two day overlap were account for so were not doubl count no again for privaci reason daysinhospit is calcul base on the lengthofstay variabl howev you dont have enough detail to calcul daysinhospit from lengthofstay this is veri use can you use your approach to look at word like weather temperatur and humid that is shown to have an impact on transmiss rate e g referenc here itd be interest to see what your approach turn up wow amaz you got this done so quick davidmezzetti would it be easi to add the hfactor of the last author as i understand it the lastauthor is the correspond author and the credibl of the studi rest on their shoulder that could be anoth help proxi davidmezzetti ignor the hfactor suggest ive just run it past a few peopl and the feedback were get is that it probabl not such a use metric were get feedback that some measur of sampl size would be realli valuabl in conjunct with the level of evid metric the kind of guidanc we were given was for level i either the number of studi or the total number of particip across all studi for level ii the number of particip in the random control trial etc itd be great if you could take a swing at this mlconsult realli appreci you do this it make our job of curat much easier hannesmarai i have been mean to go back through your system and see how it been perform sinc you made improv are there question that it doe particular well on if so which one also can you explain the differ between the question set origin kaggl vs extend kaggl from savanna reid thank for the thought comment first off as alway we will not make retrospect chang to how we handl past competit includ this one when issu like this come up we use it as an opportun to evalu how we might improv in the futur intern our debat focus on three issu recognit for those who complet stage one but not stage two achiev and how the competit appear on profil th out of look more impress than th out of how point are handl recognit for those who complet stage one but not stage two we need to view the stage one leaderboard as have no weight if it get a weight we incentiv overfit or hand label for stage one achiev and how the competit appear on profil if we did what julian suggest and add stage one particip to the bottom of the stage two leaderboard we undermin our rank by make it veri easi for somebodi to get an impressivelook top achiev by finish th out of with a naiv submiss how point are handl the one chang we will make in futur is the way point are handl we will add a multipli to the number of point for a twostag competit we have not settl on a formula for do this yet but commit to communic it clear in the rule of the next twostag competit these are difficult issu but we think this approach strike the best balanc between compet consider it is a mistak were sorri for it but weve decid not to correct it becaus it might not be fair to some contest if we chang the data midstream shouldnt be too importanton happen to chunk it the same mistak that caus a few chunk to have some miss data within the chunk if you were per cent sure that somebodi would spend day in hospit in y and per cent sure they would spend day in hospit than you might predict that they spend would day in hospit chrisr nice to see you compet in this sampl is random sali mali has point out that there is an error in the auc calcul for entri with tie score that is when two or more score have precis the same valu we will look at the problem over the next hour and will rescor all entri apologiesanthoni the auc calcul glitch has been fix and all entri have been rescor sali mali thank again for point this out hi edward you will appear on the leaderboard as soon as you make your first submiss hi edward tri use examplesubmiss csv avail at and replac the score column with your predict score if your still have troubl email the file to me anthoni goldbloomkaggl com and ill have a look attach is some sampl code that can be use to constuct an entri that generat a forecast base on the averag travel time on a given rout on a given day of the week at a given time mmm file didnt attach here the codephprh fopenrtadata csv r file to read fromwh fopensamplehistor csv w write the entri to this filedatedefaulttimezonesetgmt pure to prevent the interpret from rais a warningtimestamp array an array with the hump off pointsforecasthorizon array forecast horizon in lot of minut e g minut minut hour this is use for calcul the forecast time stampsforeach timestamp as ts foreach forecasthorizon as f forecasttimestamp daten h istrtotimetsf find day of week hour and minut that correspond to each of the timestamp row while data fgetcsvrh fals loop through the datafil if row write the header colcount countdata for c c colcount c fwritewh datac fwritewhn if inarray daten h i strtotimedataforecasttimestamp if the day of week hour and minut that correspond to a forecast timestamp is found then save to an array call tsarray for c c countdata c if emptydatac datac x tsarraydaten h i strtotimedatac datac rowforeach timestamp as ts foreach forecasthorizon as f fwritewh dateymd histrtotimetsf for c c colcount c fwritewh arraysumtsarraydaten h istrtotimetsfccounttsarraydaten h istrtotimetsfc write the averag for a given day of the week hour and minut to the submiss file fwritewhn fcloserhfclosewh david i believ that when loop the measur devic fail travel time are estim im work toward put togeth data on when travel time read are suspect andrew good discoveri ill pass the question onto the rta edit wouldnt it be obvious if they werent make the adjust sinc peak traffic time would chang this code doe generat a sampl entri to use it a download the php interpreterb creat a file name xxx php copi the code abov and download the data file to the same directoryc run the command php xxx php your correct the futur is use to predict the present howev i dont think the tempor leakag invalid the algorithm develop in this competit rob thank for jump in nathaniel thank for point this out definit worth investig the number of success grant and number of unsuccess grant field dont chang in the test dataset for obvious reason the journal citat also remain constant in the test dataset to prevent particip use the futur to predict the past nathaniel i have look at the problem in some detail and have spoken to the univers of melbourn they are look into it and hope to have an answer for us tomorrow befor they break for christma the univers has spent the last two day on the problem they suspect it an intern inconsist in their databas the figur are drawn from differ part of their databas well have to wait until the end of the christma break to get a final verdict deepak thank for point this out we will ask the univers about this as well unfortun we cant expect an answer until earli next year b yang first off congratul again on a fantast performanceyour frustrat is understand but we cannot enforc rule that dont exist what is common sens to some is not common sens to other as jeremi point out in the rta competit the rule say the win entri has to be a general algorithm that can be implement by the rta an algorithm that involv look up futur answer could not be implement by the rta the univers has done an investig and has found that the issu aris from an inconsist in their databas y y y etc refer to differ year we havent reveal which year to help keep the data privat eu jin youv obvious not seen this domcastro one of kaggl first suggest was to remov the registr fee for info the registr fee wasnt ever to rais money but to tri and deter peopl who werent serious from download this sensit data kaggl point out that anybodi with malevol intent would probabl still pay the modest registr fee so it effect would be to deter peopl who didnt think they had a chanc of win kaggl went on to argu that these peopl may also come from interest background and may be the one most like to appli creativ think to the problem frankthedefalco com or the women who have been treat for erectil dysfunct made anoth updat to csvs cover the new data and add a bunch of new column a nudg we still have tabl that need to be claim if you think it someth you can help with pleas see the relev list of task made anoth updat to the target csvs chang to risk factor remov tb chang bmi to overweight ad an extra two column to break out the result column and ad extra data chang to tie clean up the season tabl ad extra data fix issu found via error check sapal i just tri it from incognito and it work for me and other have been get in perhap you hit it dure a temporari outag suggest tri again updat the target csvs with addit tabl from the transmiss incub and environment stabil task that was quick this challeng is get the attent of the health polici and medic communiti who are work night and day to better understand covid in februari covid paper were publish per day in march that increas to by mid april that number is up to it is veri difficult to stay up with the literatur weve been take some of the most promis notebook and work with a larg team of volunt epidemiologist mds and medic student to turn those result into a regular updat literatur review you can see the work in progress here now that we have a clearer understand of what this need to look like were call on our communiti to more direct feed this effort by output result in a standard format all notebook should output csv file in the format list in the task descript how you can contribut read the instruct within each individu task and make a submiss we also encourag particip that have made submiss and want feedback to join this slack channel it includ the domain expert who are curat the tabl mlconsult look like youv done a realli nice job of pull out sampl column nice one other than that column im not see a close map to the tabl format mention abov am i miss someth oop my bad read too quick dirktheeng weve been chat with kylelo and team about the import of have tabl as paultimothymooney mention theyr work on approach to make the tabl machin readabl do you think you could reliabl take imag of tabl and turn them into panda datafram defer to kylelo but that may be a way to get you tabl sooner anthoni wow this is impress itd be great if you load it as a kaggl dataset just notic paultimothymooney alreadi has it on kaggl just ad a challeng for share use covid relat dataset motiv for ad that challeng is that a lot of dataset share in this thread a are realli use b potenti less relev for the forecast challeng we want to creat a specif outlet for all covid relat dataset davidbnn this is great will save a lot of user a lot of pain and make it easier to explor the impact of weather nice work nightrang nice start to join datacr featur matric is a realli nice way to make the dataset surfac in this thread more use koryto one direct you can take is tri to combin as mani of the dataset mention here into your featur matrix as you can make fit nice glad to see that larger and larger featur matric be built up in case it help to compar with past pandem sar dataset ebola dataset mer dataset ht sudalairajkumar i found these dataset from one of his tweet i suspect it pretti easi to get on a countrybycountri basi question is whether there a good global sourc btw tri to keep this thread relat clear for actual dataset were use this thread to ask question and discuss idea for dataset nice idea but a better fit for discuss in this thread pretti sure this is the weather data mani of us have been look for to look at the impact of temperatur and humid on transmiss rate koryto this is a great direct make a lot of sens to start join a lot of these dataset into a tabl i upload a dataset of doctor and nurs per capita for countri from the oecd cpmpml point out that have the number of recov case could be help just point out that avail in this dataset alreadi share by sudalairajkumar nice i would havent thought of this dataset love this chart and the titl is funni the accuraci threshold will be announc when we releas the full claim dataset on may rudychev receiv an answer from hpn on this a patient who visit a clinic outsid the network should be captur in this dataset of cours as jeremi keep reiter there is alway a disconnect between realiti and the content of a databas dih includ inpati admiss and emerg room visit as mention previous you dont have enough detail to calcul it from the claim tabl in this dataset miss paydelay either mean unknown or greater than in the may releas the anonym team will topcod paydelay so there will be fewer miss valu and will mean ralphh daysinhospit count day not night so if daysinhospit is then they have not been to the hospit at all if they were in and out of the er then daysinhospit would be cacross hpn had a granular threshold that they want to remain below some loss had to be suppress to achiev this target if there is a blank los and suplo is then this is how it was when it came out of the hpn dataset if there is a blank los and suplo is then the los has been suppress hope that help out of interest whi arent peopl rerun old approach that had previous been score on the new cross valid dataset hi alljust to let you know that we have extend the deadlin for this competit by just over a week both jeff and i will be travellng around mid novemb so wouldnt be abl to deal with the competit conclus anhoni apolog i hadnt antip that this might be an unpopular move i should have canvass opinion first if other also disapprov i will chang back the deadlin kaggl is not a dictatorship the downsid of chang back the deadlin is that it limit our abil to generat public this bother me becausea top perform deserv recognitionb public for the competit is public for kaggl and more public more member more competit andc it lessen the chanc of get fide attent a compromis might be to extend the previous deadlin by three day to wednesday novemb when jeff is offlin but i am avail thought hi philippth chessbas articl were written by jeff he has a relationship with the editor jeff be away when the competit finish mean that it unlik that chessbas will report on the end of the competit a real piti if we hope to grab fide attent it is unfortun that were both away when the competit end obvious not foreseen when it launch otherwis we would have set a differ deadlin anthoni jeff we must have post simultan you rais a good point if philipp and other are ok with the th then we should go with the compromis date this would mean that ill be avail to report preliminari result and should mean were readi to report the final result by the time you return preliminari will be unconfirm result from the raw leaderboard final result after the top ten have all agre to share their methodolog ive chang the deadlin to the th as for uri breath down your neck rememb that the public leaderboard is onli indic and that the final stand may be differ hi artemfor the intuiton behind auc have a read of the evalu page kaggl implement of auc work rough as follow sort submiss from highest to lowest goe down the sort list and for each predict plot a point on a graph that repres the cummul percentag of class a predict against the cummul percentag of class b predict join up all the point to form a curv the auc is the area under this curv ht phil brierley for this explan william no threshold is requir which is part of the beauti of auc in fact given that the algorithm work by sort particip make submiss contain ani real number higher mean more confid that the observ is of the posit class hope this respons doesnt serv to confus peopl anthoni artem ive gone through the step use your exampl data let me know if ive made ani error the kaggl algorithm basic work as followsfirst order the data predict real then calcul the total for each class in the total total initialis the cumul percentagespercentslast percentslast iter for each solutionsubmiss pair count count count count percent countstotalsperc countstotalsrectangl percentspercentslastpercentslasttriangl percentspercentslastpercentspercentslast area area rectangl trianglepercentslast percentspercentslast percentsso in your examplefirst submissionsolut paircount count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area auc also here kaggl php code to calcul aucpriv function aucsubmiss solut arraymultisortsubmiss sortnumer sortdesc solut total arraya b foreach solut as s if s totala elseif s totalb nextissam thispercenta thispercentb area counta countb index foreach submiss as k index if nextissam lastpercenta thispercenta lastpercentb thispercentb ifsolutionindex counta els countb nextissam ifindex countsolut ifsubmissionindex submissionindex nextissam mycount if nextissam thispercenta counta totala thispercentb countb totalb triangl thispercentb lastpercentb thispercenta lastpercenta rectangl thispercentb lastpercentb lastpercenta a rectangl triangl area a auc area return auc apolog uri and lt seem that ani repli is redund now also big thank to all those who particip in forum discuss you help make this a far more interest competit frank this is great i particular like the heatmap is it possibl to zoomalso itd be neat to see some anim on the m map show how travel time evolv over the cours of a dayweek dot get bigger and smaller though i suspect this might be a lot of work anthoni thank b yang the benefit of publish code is that you get sensibl suggest in return william thank for the question team are allow to merg one individu cannot be part of sever team our system ensur this anyway as long as somebodi doesnt have multipl account agre that we should make this more explicit in the futur as for find peopl who submit from multipl account we are actual in the process of implement rule that alert us when it look like this is happen in the futur for larg prize money competit we may look at verifi ident no toward the end of this competit you will be ask to nomin five entri that count toward the final result maintain this thread to describ known issu if you post an issu it help if you can link to a notebook that is prefix the the titl issu the conveni file have negat daili case and death count in some place the raw file are cumul case and death count the conveni file the diff of the raw file here a link the notebook demonstr the issu with some of my earli attempt to understand it pleas add ani request you have for this dataset in this thread if ani request get a number of upvot indic some amount of broader interest i will attempt to address the request add data in the jhu daili report file for us counti and the global file for global file this includ incidencer and casefatalityratio for us counti file it includ incidentr peopletest peoplehospit mortalityr testingr and hospitalizationr includ popul by countri in the global metadata file remov provincest from the countri level data to make it more conveni to process yes and also mean you can schedul a notebook to run on a recur basi xhlulu i use gcps cloud schedul combin with the kaggl api but a few peopl have ask about schedul function so we are consid ad it as a featur if this is interest to you itd be great if you could contribut to the discuss thread that mrisdal start earlier today fvcoppen i want to add addit data sourc over time so if you find good data sourc you think i should add pleas let me know sagnik have you seen this post i describ the issu there let me know if you have suggest for resolv it maithiltandel it look like a nice dataset in the dataset descript itd be help if you explain more about the dataset itself rather than the find for exampl where the dataset come from a link to the raw sourc perhap a link to the notebook you use to transform it look like a great dataset can you add document would love to know more about the dataset where it from what it can be use for etc sansuthi i tweet the dataset do you have a twitter account you want me to link to in a thread on that tweet mohit fyi think link you post is broken i think this is the link you meant to post work if you copi the text version click the link url take you to have an issu with the dataset that you want resolv request for addit metric that should be ad pleas add your suggest to this thread kaviml thank for the love post it realli motiv for us to receiv comment like this surajjha in repli im tri to compet with herbison to be the best among the staff 😂 joke asid thank you also for your nice word to the kaggl communiti today were share that d sculley will be take over as kaggl new leader alongsid other ecosystemfocus machin learn effort ben and i are leav kaggl and googl for our next adventur go back to our startup root kaggl recent pass it year anniversari we start with a lightheart competit aim at predict the vote matrix for the eurovis song contest in back then it would have been hard to imagin that kaggl would play a meaning role in the futur of machin learn and ai there are sever aspect of what kaggl has achiev that we are realli proud of first and most import the impact on peopl live mani have learn machin learn through kaggl of our almost mm user mm have submit to the titan get start competit and almost mm have complet exercis from kaggl cours k colleg cours have run classroom competit with k student submit to those competit and it not just those new to machin learn who use kaggl to learn advanc user get handson experi on lot of differ problem and the opportun to learn from the win solut as grandmast vladimir iglovikov like to say “i think about machin learn competit as about a gym but for a machin learn muscles” kaggl has also provid a credenti to the machin learn world we have given those who are suffici determin anoth way to break into the field alreadi back in facebook was use kaggl to find strong machin learn talent by we were well establish as a great way to land an elit ai job today wellregard ai compani like nvidia and h ai hire team of kaggl grandmast we'r also proud of the role the kaggl communiti has play in highlight what work well in practic machin learn paper per day are publish on arxiv and countless machin learn packag are develop kaggl user explor these techniqu in a competit environ and spread those that work framework like kera and xgboost took off in the kaggl communiti along with use preprocess and data augment librari like albument for comput vision mani techniqu have spread through kaggl includ unet for segment denois autoencod and adversari valid and kaggl has help prove out new applic for machin learn includ medic imag and autom essay grade and final we'r proud of the fact that while we start with machin learn competit we'v launch other servic notebook have increas the way our user can share learn and our cours have made kaggl more approach to new user and there no machin learn without data and we'r proud of our collect of over k public dataset which make kaggl one of the world largest repositori of public dataset of cours none of this would have been possibl without both the communiti and the kaggl team past and present over the year we have had the privileg of meet so mani passion and energet communiti member with so mani inspir stori and the opportun to work with a talent and motiv team we'r grate to all of you and while we are proud of what kaggl has achiev so far we'r also veri excit to see what the d and the team accomplish in the year ahead d has been oper on the cut edg of machin learn for the past year work on veri larg scale machin learn system insid googl do foundat research on topic like mlop and lead larg research team d also has a long histori with kaggl start with the semisupervis machin learn challeng he launch in which as it happen was won by some familiar face we'r excit what the combin of d ’s background and histori with kaggl will bring to the futur for those who want to get to know d better he is go to be answer the most upvot question over on this post anthoni and ben\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data_g['clean_messages'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb75d7",
   "metadata": {},
   "source": [
    "<b> Model Building </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a5f9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fff0a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds= StratifiedKFold(n_splits=5, shuffle= True, random_state=1)\n",
    "\n",
    "scoring= {\n",
    "    'acc': 'accuracy',\n",
    "    'neg_log_loss': 'neg_log_loss',\n",
    "    'f1_micro':'f1_micro'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c1949e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVect= CountVectorizer(ngram_range=(1,1), stop_words='english', lowercase=True, max_features=5000)\n",
    "model=Pipeline([('countVect', countVect),('lr', LogisticRegression(class_weight='balanced', C=0.005))])\n",
    "results=cross_validate(model, train_data['clean_posts'], train_data['type'], cv=kfolds, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7b83620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65718\n",
      "Logloss: 1.28835\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:0.5f}\".format(np.mean(results['test_acc'])))\n",
    "print(\"Logloss: {:0.5f}\". format(np.mean(-1*results['test_neg_log_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67a07b",
   "metadata": {},
   "source": [
    "<b> MODEL PREDICTIONS </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "386ca8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikas Velmurugan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data['clean_posts'], train_data['type'])\n",
    "pred=model.predict(forum_data_g['clean_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7315a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
       "        'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],\n",
       "       dtype=object),\n",
       " array([  3386,   1307,  25504,   6128,   3319,  86791,   1618,   3401,\n",
       "           339,   1017, 104765,   9946,  30518,  22577,  11581,  23803],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=np.unique(pred, return_counts=True)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58ff8610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personality</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>3386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>25504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>6128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESFJ</td>\n",
       "      <td>3319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ESFP</td>\n",
       "      <td>86791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESTP</td>\n",
       "      <td>3401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>104765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INTP</td>\n",
       "      <td>9946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ISFJ</td>\n",
       "      <td>30518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>22577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>11581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>23803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Personality   Count\n",
       "0         ENFJ    3386\n",
       "1         ENFP    1307\n",
       "2         ENTJ   25504\n",
       "3         ENTP    6128\n",
       "4         ESFJ    3319\n",
       "5         ESFP   86791\n",
       "6         ESTJ    1618\n",
       "7         ESTP    3401\n",
       "8         INFJ     339\n",
       "9         INFP    1017\n",
       "10        INTJ  104765\n",
       "11        INTP    9946\n",
       "12        ISFJ   30518\n",
       "13        ISFP   22577\n",
       "14        ISTJ   11581\n",
       "15        ISTP   23803"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_preds=list(zip(count[0], count[1]))\n",
    "pred_df=pd.DataFrame(list_of_preds, columns=['Personality','Count'])\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540af8a",
   "metadata": {},
   "source": [
    "<b> SCOPE FOR IMPROVEMENT </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9d3c4",
   "metadata": {},
   "source": [
    "* Text cleaning can be improved by removing stop words and preferring lemmentization over stemming.\n",
    "* I have just used Logistic regression. Other efficient models can be used to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
